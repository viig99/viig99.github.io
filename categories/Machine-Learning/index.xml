<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Machine Learning on /home/vigi99</title><link>https://viig99.github.io/categories/Machine-Learning/</link><description>Recent content in Machine Learning on /home/vigi99</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Fri, 27 Feb 2026 19:23:59 -0500</lastBuildDate><atom:link href="https://viig99.github.io/categories/Machine-Learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Building a Chess Bot with HSTU: From Lichess Pretraining to Value Search</title><link>https://viig99.github.io/docs/posts/imba-chess/</link><pubDate>Fri, 27 Feb 2026 00:00:00 +0000</pubDate><guid>https://viig99.github.io/docs/posts/imba-chess/</guid><description>&lt;h2 id="executive-summary"&gt;Executive Summary&lt;a class="anchor" href="#executive-summary"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This is a running journal of &lt;code&gt;imba-chess&lt;/code&gt;, a personal research project to build a compact (20–30M parameter) chess bot entirely from supervised pretraining on Lichess games, without any game-tree search engine to begin with.&lt;/p&gt;
&lt;p&gt;The journey so far:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Adapted HSTU-style causal sequence modeling from recommender systems to chess.&lt;/li&gt;
&lt;li&gt;Trained next-move prediction (policy head) on high-Elo Lichess game data.&lt;/li&gt;
&lt;li&gt;Evaluated offline metrics: top-1, hr@10, MRR.&lt;/li&gt;
&lt;li&gt;Tested against Stockfish. Results: humbling.&lt;/li&gt;
&lt;li&gt;Added a value head (WDL), wired it into move selection.&lt;/li&gt;
&lt;li&gt;Implemented depth-2 policy-pruned minimax search on top.&lt;/li&gt;
&lt;li&gt;Defined a Stockfish ladder evaluation pipeline for reproducible strength measurement.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The throughline is: &lt;strong&gt;treat each chess ply as a structured event in a sequence, the same way you would model user behavior&lt;/strong&gt;.&lt;/p&gt;</description></item><item><title>FlexAttention HSTU at 500M Events: RQ Tokens, QR Embeddings, and 1D Biases</title><link>https://viig99.github.io/docs/posts/hstu-for-yambda/</link><pubDate>Sun, 15 Feb 2026 00:00:00 +0000</pubDate><guid>https://viig99.github.io/docs/posts/hstu-for-yambda/</guid><description>&lt;h2 id="executive-summary"&gt;Executive Summary&lt;a class="anchor" href="#executive-summary"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;At Yambda scale (about 500M events and 9.4M items), models rarely break because one idea is bad. They break when many small, expensive defaults pile up.&lt;/p&gt;
&lt;p&gt;This post is the story of the choices that kept an HSTU-style recommender trainable:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Jagged, block-masked attention with &lt;a href="https://pytorch.org/docs/stable/nn.attention.flex_attention.html" target="_blank" rel="noopener" &gt;PyTorch FlexAttention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Residual quantization (RQ) token prediction instead of a giant item-ID softmax&lt;/li&gt;
&lt;li&gt;Quotient-remainder (QR) embeddings for large sparse categorical spaces&lt;/li&gt;
&lt;li&gt;On-the-fly 1D attention bias terms (time, duration, organic) instead of dense &lt;code&gt;[S, S]&lt;/code&gt; bias tensors&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/2108.12409" target="_blank" rel="noopener" &gt;ALiBi&lt;/a&gt; positional bias instead of learned position embeddings&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The throughline is simple: keep the inductive bias, cut the dense and quadratic costs.&lt;/p&gt;</description></item><item><title>Machine Learning Engineer Roadmap</title><link>https://viig99.github.io/docs/posts/ml_engineer_guidelines/</link><pubDate>Mon, 22 May 2023 00:00:00 +0000</pubDate><guid>https://viig99.github.io/docs/posts/ml_engineer_guidelines/</guid><description>&lt;h2 id="mastering-the-art-of-ai-development-a-detailed-roadmap-from-data-to-deployment"&gt;&lt;strong&gt;Mastering the Art of AI Development: A Detailed Roadmap from Data to Deployment&lt;/strong&gt;&lt;a class="anchor" href="#mastering-the-art-of-ai-development-a-detailed-roadmap-from-data-to-deployment"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="introduction"&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;a class="anchor" href="#introduction"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Developing an effective artificial intelligence (AI) model is akin to embarking on a long, complex journey. It demands expertise in areas ranging from dataset creation and feature engineering to model tuning, evaluation, and deployment. This blog post will walk you through the key stages involved in AI development, explain the importance of each, and provide a clear understanding of the skills required at different levels of expertise, namely Junior, Senior, and Staff Engineer.&lt;/p&gt;</description></item><item><title>Supervised Fine-Tuning in Large Language Models</title><link>https://viig99.github.io/docs/posts/supervised_finetuning/</link><pubDate>Mon, 22 May 2023 00:00:00 +0000</pubDate><guid>https://viig99.github.io/docs/posts/supervised_finetuning/</guid><description>&lt;h2 id="the-power-of-supervised-fine-tuning-in-large-language-models-an-in-depth-analysis"&gt;&lt;strong&gt;The Power of Supervised Fine-Tuning in Large Language Models: An In-depth Analysis&lt;/strong&gt;&lt;a class="anchor" href="#the-power-of-supervised-fine-tuning-in-large-language-models-an-in-depth-analysis"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="introduction"&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;a class="anchor" href="#introduction"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;In recent years, the development of machine learning, particularly large language models (LLMs), has revolutionized the way we approach a multitude of challenges, from query-based tasks to content generation. In this post, we will dive deep into a technique gaining traction within the AI community - supervised fine-tuning using domain-specific instruction datasets - and contrast it with the more conventional prompt tuning approach, with a focus on techniques such as retrieval augmentation.&lt;/p&gt;</description></item><item><title>The Role of Negative Mining in Machine Learning: Bridging the Gap in Model Performance</title><link>https://viig99.github.io/docs/posts/hard_negatives/</link><pubDate>Mon, 22 May 2023 00:00:00 +0000</pubDate><guid>https://viig99.github.io/docs/posts/hard_negatives/</guid><description>&lt;h2 id="introduction"&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;a class="anchor" href="#introduction"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Machine learning models are excellent tools for making predictions or classifications. However, they&amp;rsquo;re not infallible; occasionally, they may make mistakes. Some of the most enlightening mistakes are the so-called &amp;ldquo;hard negatives&amp;rdquo; — instances where the model confidently produces the incorrect output. Understanding and learning from these instances through hard negative mining can significantly improve the model&amp;rsquo;s performance.&lt;/p&gt;
&lt;h3 id="understanding-hard-negative-mining"&gt;&lt;strong&gt;Understanding Hard Negative Mining&lt;/strong&gt;&lt;a class="anchor" href="#understanding-hard-negative-mining"&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;In machine learning, &amp;ldquo;hard negatives&amp;rdquo; refer to examples that are challenging for the model to classify correctly. They are the negatives that the model most often misclassifies. Hard negative mining is a strategy for improving the performance of a model by focusing on these difficult-to-classify instances.&lt;/p&gt;</description></item><item><title>Entity Resolution using Contrastive Learning</title><link>https://viig99.github.io/docs/posts/entity_resolution/</link><pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate><guid>https://viig99.github.io/docs/posts/entity_resolution/</guid><description>&lt;h2 id="introduction-to-entity-resolution"&gt;&lt;strong&gt;Introduction to Entity Resolution&lt;/strong&gt;&lt;a class="anchor" href="#introduction-to-entity-resolution"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://paperswithcode.com/task/entity-resolution" target="_blank" rel="noopener" &gt;Entity resolution&lt;/a&gt; (also known as entity matching, record linkage, or duplicate detection) is the task of finding records that refer to the same real-world entity across different data sources (e.g., data files, books, websites, and databases).&lt;/p&gt;
&lt;p&gt;This can be a challenging task, especially when the dataset is large and the queries mention the attributes of the entities in various ways, such as with partial information, typing errors, abbreviations, or extra information. In this blog post, we&amp;rsquo;ll be discussing how to approach the Entity Resolution Problem and the solution that was implemented to solve it.&lt;/p&gt;</description></item></channel></rss>