<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Machine Learning on /home/vigi99</title><link>https://viig99.github.io/categories/Machine-Learning/</link><description>Recent content in Machine Learning on /home/vigi99</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 17 Jun 2024 10:03:05 -0400</lastBuildDate><atom:link href="https://viig99.github.io/categories/Machine-Learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Machine Learning Engineer Roadmap</title><link>https://viig99.github.io/docs/posts/ml_engineer_guidelines/</link><pubDate>Mon, 22 May 2023 00:00:00 +0000</pubDate><guid>https://viig99.github.io/docs/posts/ml_engineer_guidelines/</guid><description>&lt;h2 id="mastering-the-art-of-ai-development-a-detailed-roadmap-from-data-to-deployment">
 &lt;strong>Mastering the Art of AI Development: A Detailed Roadmap from Data to Deployment&lt;/strong>
 &lt;a class="anchor" href="#mastering-the-art-of-ai-development-a-detailed-roadmap-from-data-to-deployment">#&lt;/a>
&lt;/h2>
&lt;h3 id="introduction">
 &lt;strong>Introduction&lt;/strong>
 &lt;a class="anchor" href="#introduction">#&lt;/a>
&lt;/h3>
&lt;p>Developing an effective artificial intelligence (AI) model is akin to embarking on a long, complex journey. It demands expertise in areas ranging from dataset creation and feature engineering to model tuning, evaluation, and deployment. This blog post will walk you through the key stages involved in AI development, explain the importance of each, and provide a clear understanding of the skills required at different levels of expertise, namely Junior, Senior, and Staff Engineer.&lt;/p></description></item><item><title>Supervised Fine-Tuning in Large Language Models</title><link>https://viig99.github.io/docs/posts/supervised_finetuning/</link><pubDate>Mon, 22 May 2023 00:00:00 +0000</pubDate><guid>https://viig99.github.io/docs/posts/supervised_finetuning/</guid><description>&lt;h2 id="the-power-of-supervised-fine-tuning-in-large-language-models-an-in-depth-analysis">
 &lt;strong>The Power of Supervised Fine-Tuning in Large Language Models: An In-depth Analysis&lt;/strong>
 &lt;a class="anchor" href="#the-power-of-supervised-fine-tuning-in-large-language-models-an-in-depth-analysis">#&lt;/a>
&lt;/h2>
&lt;h3 id="introduction">
 &lt;strong>Introduction&lt;/strong>
 &lt;a class="anchor" href="#introduction">#&lt;/a>
&lt;/h3>
&lt;p>In recent years, the development of machine learning, particularly large language models (LLMs), has revolutionized the way we approach a multitude of challenges, from query-based tasks to content generation. In this post, we will dive deep into a technique gaining traction within the AI community - supervised fine-tuning using domain-specific instruction datasets - and contrast it with the more conventional prompt tuning approach, with a focus on techniques such as retrieval augmentation.&lt;/p></description></item><item><title>The Role of Negative Mining in Machine Learning: Bridging the Gap in Model Performance</title><link>https://viig99.github.io/docs/posts/hard_negatives/</link><pubDate>Mon, 22 May 2023 00:00:00 +0000</pubDate><guid>https://viig99.github.io/docs/posts/hard_negatives/</guid><description>&lt;h2 id="introduction">
 &lt;strong>Introduction&lt;/strong>
 &lt;a class="anchor" href="#introduction">#&lt;/a>
&lt;/h2>
&lt;p>Machine learning models are excellent tools for making predictions or classifications. However, they&amp;rsquo;re not infallible; occasionally, they may make mistakes. Some of the most enlightening mistakes are the so-called &amp;ldquo;hard negatives&amp;rdquo; â€” instances where the model confidently produces the incorrect output. Understanding and learning from these instances through hard negative mining can significantly improve the model&amp;rsquo;s performance.&lt;/p>
&lt;h3 id="understanding-hard-negative-mining">
 &lt;strong>Understanding Hard Negative Mining&lt;/strong>
 &lt;a class="anchor" href="#understanding-hard-negative-mining">#&lt;/a>
&lt;/h3>
&lt;p>In machine learning, &amp;ldquo;hard negatives&amp;rdquo; refer to examples that are challenging for the model to classify correctly. They are the negatives that the model most often misclassifies. Hard negative mining is a strategy for improving the performance of a model by focusing on these difficult-to-classify instances.&lt;/p></description></item><item><title>Entity Resolution using Contrastive Learning</title><link>https://viig99.github.io/docs/posts/entity_resolution/</link><pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate><guid>https://viig99.github.io/docs/posts/entity_resolution/</guid><description>&lt;h2 id="introduction-to-entity-resolution">
 &lt;strong>Introduction to Entity Resolution&lt;/strong>
 &lt;a class="anchor" href="#introduction-to-entity-resolution">#&lt;/a>
&lt;/h2>
&lt;p>&lt;a href="https://paperswithcode.com/task/entity-resolution" target="_blank" rel="noopener" >Entity resolution&lt;/a> (also known as entity matching, record linkage, or duplicate detection) is the task of finding records that refer to the same real-world entity across different data sources (e.g., data files, books, websites, and databases).&lt;/p>
&lt;p>This can be a challenging task, especially when the dataset is large and the queries mention the attributes of the entities in various ways, such as with partial information, typing errors, abbreviations, or extra information. In this blog post, we&amp;rsquo;ll be discussing how to approach the Entity Resolution Problem and the solution that was implemented to solve it.&lt;/p></description></item></channel></rss>