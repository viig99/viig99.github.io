<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Systems on /home/vigi99</title><link>https://viig99.github.io/categories/Systems/</link><description>Recent content in Systems on /home/vigi99</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Fri, 27 Feb 2026 19:31:21 -0500</lastBuildDate><atom:link href="https://viig99.github.io/categories/Systems/index.xml" rel="self" type="application/rss+xml"/><item><title>Building a Chess Bot with HSTU: From Lichess Pretraining to Value Search</title><link>https://viig99.github.io/docs/posts/imba-chess/</link><pubDate>Fri, 27 Feb 2026 00:00:00 +0000</pubDate><guid>https://viig99.github.io/docs/posts/imba-chess/</guid><description>&lt;h2 id="executive-summary"&gt;Executive Summary&lt;a class="anchor" href="#executive-summary"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This post is my running build log for &lt;code&gt;imba-chess&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Right now, the checked-in model is about &lt;strong&gt;12.6M parameters&lt;/strong&gt;.&lt;br&gt;
My original target in the notes was 20â€“30M, but I am still iterating toward that.&lt;/p&gt;
&lt;p&gt;What I have done so far:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Adapted HSTU-style causal sequence modeling from recommender systems to chess.&lt;/li&gt;
&lt;li&gt;Trained next-move prediction (policy head) on high-Elo Lichess game data.&lt;/li&gt;
&lt;li&gt;Evaluated offline metrics: top-1, hr@10, MRR.&lt;/li&gt;
&lt;li&gt;Tested against Stockfish. Results: humbling.&lt;/li&gt;
&lt;li&gt;Added a value head (WDL), wired it into move selection.&lt;/li&gt;
&lt;li&gt;Implemented depth-2 policy-pruned minimax search on top.&lt;/li&gt;
&lt;li&gt;Defined a Stockfish ladder evaluation pipeline for reproducible strength measurement.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;My core framing is: &lt;strong&gt;a chess game is a structured event sequence&lt;/strong&gt;.&lt;/p&gt;</description></item><item><title>FlexAttention HSTU at 500M Events: RQ Tokens, QR Embeddings, and 1D Biases</title><link>https://viig99.github.io/docs/posts/hstu-for-yambda/</link><pubDate>Sun, 15 Feb 2026 00:00:00 +0000</pubDate><guid>https://viig99.github.io/docs/posts/hstu-for-yambda/</guid><description>&lt;h2 id="executive-summary"&gt;Executive Summary&lt;a class="anchor" href="#executive-summary"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;At Yambda scale (about 500M events and 9.4M items), models rarely break because one idea is bad. They break when many small, expensive defaults pile up.&lt;/p&gt;
&lt;p&gt;This post is the story of the choices that kept an HSTU-style recommender trainable:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Jagged, block-masked attention with &lt;a href="https://pytorch.org/docs/stable/nn.attention.flex_attention.html" target="_blank" rel="noopener" &gt;PyTorch FlexAttention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Residual quantization (RQ) token prediction instead of a giant item-ID softmax&lt;/li&gt;
&lt;li&gt;Quotient-remainder (QR) embeddings for large sparse categorical spaces&lt;/li&gt;
&lt;li&gt;On-the-fly 1D attention bias terms (time, duration, organic) instead of dense &lt;code&gt;[S, S]&lt;/code&gt; bias tensors&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/2108.12409" target="_blank" rel="noopener" &gt;ALiBi&lt;/a&gt; positional bias instead of learned position embeddings&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The throughline is simple: keep the inductive bias, cut the dense and quadratic costs.&lt;/p&gt;</description></item></channel></rss>