[{"id":0,"href":"/docs/work-experience/","title":"Work Experience","section":"Docs","content":" Work Experience # Staff Machine Learning Engineer # Kalepa # Apr 2022 - Sep 2022 - Toronto, CA # Improved the in-house News Recommendation System using Sentence Embeddings. Identified and implemented Document Question Answering, as configurable classifiers to analyse various risks on businesses. Built and deployed Entity Resolution System using Unsupervised Contrastive Learning. improved top-20 search accuracy from 35% (on ElasticSearch) to 98% for 30m entities. reduced search latency from 1.5s to 0.3s. built a scalable system using Postgres, Onnx, Pinecone, fastAPI, and Dockerized deployment. Interviewed 35+ candidates and developed a system to identify the most suitable candidates for the role. Principal Machine Learning Engineer # Airtel X-labs # Sep 2018 - Mar 2022 - Bangalore, IN # Led the product development of Voicebot engine which powers voice-based queries on the MyAirtel app with 10m MAU, in 7 indian languages, does 500k queries/day. (Speech to text, Text to speech, training and inference pipelines.) 900hrs Hindi Speech Dataset Created using Common Voice Used wav2letter++ Streaming Convnets Distributed Training on 16 nodes GPU cluster using OpenMP, RoCE, GPUDirect High performance Bi-directional C++ Grpc Server scaled on k8s Text to Speech built using tactotron2 + vocgan\u0026rsquo;s Voicebot integeration with PBX exchange like Asterix. Presented work at Nvidia GTC Winter 2020 Researched and deployed e2e OCR pipeline serving 1.6m docs/day at 96%+ accuracy, used by Airtel for its new customer acquisition journey ICDAR Rank 6 Synthetic data creation for Documented Recognition in the Wild. EAST + Convnets as Word Localization \u0026amp; Word Recognition Backbone. Optimized C++ NMS for Zero-copy with pybind11 Dynamic parsers DSL based on clustering step. Building the workflow-orchestration engine which powers the customer support queries on mail / social media for Airtel, processes 50k emails/day, built on k8, temporal.io Reverse Engineered and ported workflows for Sprinklr from scratch. Supported 150 different workflows with ~50 activities running concurrently. Maintaining Temporal cluster on OKD, with postgres and cassandra. Hired and led a team of 9 engineers. Co-founding Engineer # AuthME ID Solutions, Acquired by Airtel # Aug 2017 - Sep 2018 - Bangalore, IN # Built OCR pipeline for reading arbitrary documents, 5 step process with word localization, word recognition, clustering, parsing and serving. Built Voice based IVR bot for Indian business by building on top of DeepSpeech and Rasa NLU. Invited for YC 2018 Winter Interview Stage in San Francisco. Machine Learning Engineer # Krowd # June 2015 - Aug 2017 - Bangalore, IN # Recommendation \u0026amp; ranking for users by clustering restaurants into latent topics space and finding top-n using a custom scoring functions built into node.js. Achieved \u0026lt;200ms latency over a set of 1 million restaurants per user. (Fast heuristic approach for cold start case similar to Netflix). Loyalty and rewards platform with second price ad bidding for banks (pilot run with Royal Bank of Scotland). Software Development Engineer # Amazon # Feb 2013 - Feb 2015 - Bangalore, IN # Built the auto correcting \u0026amp; predictive completion language keyboards for regions like germany, japan etc based on the hidden markov model. Worked on Developing \u0026amp; Deploying Amazon Instant Video on 13 different living room TV environments in 10 months to 1m+ customers. Scaling \u0026amp; building a/b testing framework to test the application across various regions. Associate Software Engineer # Kony Labs # July 2011 - Nov 2012 - Hyderabad, IN # Developing an Internal JavaScript single templating based backend/frontend framework for mobile web development (Single Page Applications). Writing native platform level code and integrating it with the existing Lua to native code using Foreign Function Interface (Objective C, Java). "},{"id":1,"href":"/docs/open-source/","title":"Open Source Contributions","section":"Docs","content":" Open Source Contributions # SymSpellCppPy # Python library for Spelling correction based on SymSpell written in C++ and exposed to python via pybind11.\nBlaze # ML inference framework for pytorch models in Asynchronous C++ which supports dynamic batching, Arrayfire, quantized model, and various optimizations written using Drogon\nSABER # Easily reproducible machine learning baseline for automatic speech recognition using semi-supervised contrastive learning.\nEpoch-Synchronous Overlap-Add (ESOLA) # Fast C++ implementation of ESOLA using KFRLib, can be used for online time-stretch augmentation during SpeechToText training.\nNewman # Initial Contributor to Newman which is a command-line collection runner for Postman.\nPostman Interceptor # Initial Contributor to Postman Interceptor a helper extension for the Postman packaged app.\n"},{"id":2,"href":"/docs/machine-learning/","title":"Machine Learning Toolkit","section":"Docs","content":" Machine Learning Toolkit # Data Engineering # Building robust unbiased datasets using feature based sampling methods. Generating Synthetic data similar to data distribution. Augmentation techniques for various modalities like vision, speech, nlp. Modelling and Feature Engineering # Knowledge of Convolution, Recurrent and Transformer based models. Feature importance techniques Contrastive Learning methods like SimCLR, BYOL, SimSiam. Model Debugging \u0026amp; Profiling. Topic models using Probablistic Graphic Model \u0026amp; with Embedding based clustering. Training # Distributed Training on clusters using OpenMPI + RoCE, Torch RPC Pytorch Lightning optimizations. Calibration # Implicit Calibration techniques like Focal Loss, Maximum Entropy Regularization, Label Smoothing, Random Dropout etc. Explicit Calibration techniques like Isotonic Regression, Patt\u0026rsquo;s scaling Optimizations # Quantization Model Pruning Distillation ML Ops Fusing techniques like ONNX, TorchDynamo, TVM etc Inference # C++ Inference using ONNX \u0026amp; Drogon Frameworks like Triton, Mosec. Scaling on k8 using OKD. Monitoring \u0026amp; Alerting using Vector.io + Prometheous + Grafana. Online Monitoring # Hard Negative mining around calibrated threshold region. Sampling and saving hard negatives Monitoring and alerts to detect Model \u0026amp; Data Drifts. "},{"id":3,"href":"/docs/posts/entity_resolution/","title":"Entity Resolution using Contrastive Learning","section":"Machine Learning Usecases","content":" Introduction to Entity Resolution # Entity resolution (also known as entity matching, record linkage, or duplicate detection) is the task of finding records that refer to the same real-world entity across different data sources (e.g., data files, books, websites, and databases).\nThis can be a challenging task, especially when the dataset is large and the queries mention the attributes of the entities in various ways, such as with partial information, typing errors, abbreviations, or extra information. In this blog post, we\u0026rsquo;ll be discussing how to approach the Entity Resolution Problem and the solution that was implemented to solve it.\nProblem Definition # Imagine you have a dataset of approximately 50 million entities, and your task is to find the right entity for a given query. The query could be a few of the entity\u0026rsquo;s attributes, and these queries could mention the attributes in various ways. This is the Entity Resolution Problem.\nThe Existing Solution # One solution to this problem is an Elastic search-based match, which uses complicated heuristics that are overfitted on a small training set. However, this solution is not scalable and the accuracy of the top-20 search retrieval decreases as the number of entities increases. At the time this problem was being addressed, the top-20 search retrieval accuracy was around 40% for the current number of entities.\nThe Implemented Solution # To solve the Entity Resolution Problem, an embedding search was implemented using a Sentence embedding model. The Deberta model was pretrained and fine-tuned for the current problem using contrastive learning. In contrastive learning, positive pairs are generated using augmentations for each attribute that best mock the queries, based on the many user queries received.\nResults # With this solution, the top-20 accuracy was around 98%. Heuristics and other business logic, along with a properly calculated confidence measure (which was hyperparameter-tuned on the validation set), were used to filter out the right entity. After the final pipeline was implemented, a top-1 accuracy of around 99.995% (precision) and 86% (recall) was achieved for high confidence matches.\nIn the end, pinecone was chosen for the embedding search and the search latency was around 100ms for the top 50 among the 50 million embeddings.\nConclusion # To conclude, the Entity Resolution Problem was successfully solved by implementing an embedding search using a Sentence embedding model and fine-tuning it with contrastive learning. This solution had a significantly higher accuracy compared to the existing Elastic search-based solution and was able to scale well as the number of entities increased.\n"}]