[{"id":0,"href":"/docs/work-experience/","title":"Work Experience","section":"Docs","content":" Work Experience # ### **Staff Machine Learning Engineer** ### [VerticalScope Inc.](https://www.verticalscope.com/) ### *Jan 2023 - Present* - *Toronto, CA* Working at Fora, a part of VerticalScope\u0026rsquo;s cloud-based digital platform that operates over 1,200 online communities connecting 110 million active users monthly, fostering their passions, and facilitating knowledge sharing. My role focuses on building healthier and better communities by empowering users with machine learning. Here are some of the tasks I have been responsible for: Empowering communities with machine learning to enhance user experience. Personalized Feed Recommendation\u0026rsquo;s, Newsletter growth, mobile notifications to all communities. Developing advanced search algorithms for accurate, tailored results. Optimizing product retrieval and facilitating new product discovery within communities. Answering user questions and providing technical support, using LLMs. Interviewing and mentoring junior engineers. ### **Staff Machine Learning Engineer** ### [Kalepa](https://kalepa.com/) ### *Apr 2022 - Sep 2022* - *Toronto, CA* Enhanced the in-house News Recommendation System using Sentence Embeddings. Identified and implemented Document Question Answering, as configurable classifiers to analysis various risks on businesses. Constructed and deployed an Entity Resolution System utilizing Unsupervised Contrastive Learning. improved top-20 search accuracy from 35% to 98% for 10^7+ entities. Cut down search latency from 1.5s to 0.3s. Crafted a scalable system employing Postgres, Onnx, Pinecone, fastAPI, with Dockerized deployment. Conducted 35+ candidate interviews and developed a system for pinpointing the most suitable candidates for respective roles. ### **Principal Machine Learning Engineer** ### [Airtel X-labs](https://careers.airtel.com/) ### *Sep 2018 - Mar 2022* - *Bangalore, IN* Led the product development of Voicebot engine which powers voice-based queries on the MyAirtel app with 10m MAU, in 7 indian languages, does 500k queries/day. (Speech to text, Text to speech, training and inference pipelines.) Voicebot integeration with PBX exchange like Asterix. Presented work at Nvidia GTC Winter 2020 Researched and deployed e2e OCR pipeline serving 1.6m docs/day at 96%+ accuracy, used by Airtel for its new customer acquisition journey ICDAR Rank 6 Building the workflow-orchestration engine which powers the customer support queries on mail / social media for Airtel, processes 50k emails/day, built on k8, temporal.io Supported 150 different workflows with ~50 activities running concurrently. Hired and led a team of 9 engineers. ### **Co-founding Engineer** ### [AuthME ID Solutions, Acquired by Airtel](https://analyticsindiamag.com/airtel-ai-startup-authme/) ### *Aug 2017 - Sep 2018* - *Bangalore, IN* Built OCR pipeline for reading arbitrary documents, 5 step process with word localization, word recognition, clustering, parsing and serving. Built Voice based IVR bot for Indian business by building on top of DeepSpeech and Rasa NLU. Invited for YC 2018 Winter Interview Stage in San Francisco. ### **Machine Learning Engineer** ### [Krowd](https://krowdit.com/) ### *June 2015 - Aug 2017* - *Bangalore, IN* Recommendation \u0026amp; ranking for users by clustering restaurants into latent topics space and recommending fresh restaurants built on node.js. \u0026lt;200ms latency over a set of 1 million restaurants per user. Loyalty and rewards platform with second price ad bidding for banks (pilot run with Royal Bank of Scotland). ### **Software Development Engineer** ### [Amazon](https://www.amazon.jobs/en/teams/prime-video) ### *Feb 2013 - Feb 2015* - *Bangalore, IN* Built the auto correcting \u0026amp; predictive completion language keyboards for regions like germany, japan etc based on the hidden markov model. Worked on Developing \u0026amp; Deploying Amazon Instant Video on 13 different living room TV environments in 10 months to 1m+ customers. Scaling \u0026amp; building a/b testing framework to test the application across various regions. ### **Associate Software Engineer** ### [Kony Labs](https://www.kony.com) ### *July 2011 - Nov 2012* - *Hyderabad, IN* Developed a JavaScript single templating based backend/frontend framework. Integrated native platform level code with existing Lua code using Foreign Function Interface. "},{"id":1,"href":"/docs/open-source/","title":"Open Source Contributions","section":"Docs","content":" Open Source Contributions # ### [SymSpellCppPy](https://github.com/viig99/SymSpellCppPy) [![star this repo](https://img.shields.io/github/stars/viig99/SymSpellCppPy?style=flat-square)](https://github.com/viig99/SymSpellCppPy) Python library for Spelling correction based on SymSpell written in C++ and exposed to python via pybind11.\n### [Comparing Contrastive losses on Vision \u0026 NLP](https://github.com/viig99/ContrastiveLearningLossComparison) [![star this repo](https://img.shields.io/github/stars/viig99/ContrastiveLearningLossComparison?style=flat-square)](https://github.com/viig99/ContrastiveLearningLossComparison) Comparing performance of different InfoNCE type losses used in contrastive learning.\n### [Windy Lunar Lander Env](https://github.com/viig99/rl-experiments) [![star this repo](https://img.shields.io/github/stars/viig99/rl-experiments?style=flat-square)](https://github.com/viig99/rl-experiments) Exploring Reinforcement Learning algorithms on customized Lunar Lander environment with dynamic realistic wind vectors by extending the gym environment.\n### [Blaze](https://github.com/SABER-labs/Drogon-torch-serve) [![star this repo](https://img.shields.io/github/stars/SABER-labs/Drogon-torch-serve?style=flat-square)](https://github.com/SABER-labs/Drogon-torch-serve) ML inference framework for pytorch models in Asynchronous C++ which supports dynamic batching, Arrayfire, quantized model, and various optimizations written using Drogon\n### [SABER](https://github.com/SABER-labs/SABERv2) [![star this repo](https://img.shields.io/github/stars/SABER-labs/SABERv2?style=flat-square)](https://github.com/SABER-labs/SABERv2) Easily reproducible machine learning baseline for automatic speech recognition using semi-supervised contrastive learning.\n### [Epoch-Synchronous Overlap-Add (ESOLA)](https://github.com/viig99/esolafast) [![star this repo](https://img.shields.io/github/stars/viig99/esolafast?style=flat-square)](https://github.com/viig99/esolafast) Fast C++ implementation of ESOLA using KFRLib, can be used for online time-stretch augmentation during SpeechToText training.\n### [Newman](https://github.com/postmanlabs/newman) [![star this repo](https://img.shields.io/github/stars/postmanlabs/newman?style=flat-square)](https://github.com/postmanlabs/newman) Initial Contributor to Newman which is a command-line collection runner for Postman.\n### [Postman Interceptor](https://github.com/postmanlabs/postman-chrome-interceptor) [![star this repo](https://img.shields.io/github/stars/postmanlabs/postman-chrome-interceptor?style=flat-square)](https://github.com/postmanlabs/postman-chrome-interceptor) Initial Contributor to Postman Interceptor a helper extension for the Postman packaged app.\n"},{"id":2,"href":"/docs/machine-learning/","title":"Machine Learning Toolkit","section":"Docs","content":" Machine Learning Toolkit: Skills and Expertise # Data Engineering # Expertise in building unbiased datasets via feature-based sampling. Proficient in generating synthetic data matching real data distribution. Skilled in augmentation techniques for vision, speech, NLP. Modelling and Feature Engineering # Comprehensive knowledge of Convolutional, Recurrent, and Transformer-based models. Experience with feature importance techniques. Proficient with Contrastive Learning methods like SimCLR, BYOL, SimSiam. Well-versed in model debugging and profiling. Experienced with topic models using Probabilistic Graphic Models and embedding-based clustering. Training # Proficient in distributed training using OpenMPI + RoCE, Torch RPC. Skilled in Pytorch Lightning optimizations. Calibration # Expertise in implicit calibration techniques like Focal Loss, Maximum Entropy Regularization, Label Smoothing, Random Dropout. Experience with explicit calibration techniques like Isotonic Regression, Platt\u0026rsquo;s scaling. Optimizations # Skilled in model optimizations such as Quantization, Pruning, Distillation. Proficient in ML Ops Fusing techniques like ONNX, TorchDynamo, TVM. Inference # Expertise in C++ inference using ONNX \u0026amp; Drogon. Experience with frameworks like Triton, Mosec. Skilled in scaling on k8 using OKD. Proficient in monitoring and alerting using Vector.io, Prometheus, Grafana. Online Monitoring # Expertise in hard negative mining around calibrated threshold region. Experience with sampling and saving hard negatives. Skilled in detecting and alerting on Model and Data Drifts. "},{"id":3,"href":"/docs/posts/hstu-for-yambda/","title":"FlexAttention HSTU at 500M Events: RQ Tokens, QR Embeddings, and 1D Biases","section":"Posts","content":" Executive Summary # This post documents the architecture choices that made an HSTU-style recommender tractable at Yambda scale (roughly 500M events and 9.4M item IDs):\nJagged, block-masked attention with PyTorch FlexAttention Residual quantization (RQ) token prediction instead of a giant item-ID softmax Quotient-remainder (QR) embeddings for large sparse categorical spaces On-the-fly 1D attention bias terms (time, duration, organic) instead of dense [S, S] bias tensors ALiBi positional bias instead of learned position embeddings The core pattern is simple: preserve useful inductive bias, but make every scaling decision memory-aware.\nSystem Constraints # The design was driven by four hard constraints:\nUser histories are jagged, not fixed-length. Item space is large enough that a full output projection is expensive. Side metadata is sparse and partially missing. Attention biasing must not materialize quadratic tensors. Those constraints forced us to optimize for throughput and memory first, while keeping model quality stable.\nArchitecture Overview # flowchart LR A[RQ Codes 8x1024] --\u003e B[Sum RQ Embeddings] C[QR Artist Embedding] --\u003e D[Event Representation] E[Event Type Embedding] --\u003e D B --\u003e D D --\u003e F[FlexAttention HSTU x N] F --\u003e G[L2 Normalize] G --\u003e H[Cascaded RQ Heads] H --\u003e I[Logits S x 8 x 1024] Key point: we do not model a direct item_id distribution at the output layer. We model codebooks.\nWhy RQ Outputs Instead of Item-ID Softmax # With ~9.4M items, a direct head looks like:\nLinear(D -\u0026gt; 9,400,000) That is expensive in memory bandwidth and optimizer state. Instead, we train an 8-level residual quantizer over item embeddings and predict discrete code indices:\n8 x Linear(D -\u0026gt; 1024) Benefits:\nSmaller output parameterization and optimizer footprint Better fit for ANN retrieval over decoded embeddings Cleaner decomposition into coarse-to-fine prediction Reference: FAISS for vector retrieval.\nAvoiding O(S^2) Bias Materialization # A common failure mode is building dense attention bias matrices for time, position, or feature-specific priors. At long sequence lengths this becomes unnecessary overhead.\nIn FlexAttention, we apply score modifiers lazily:\ndef score_mod(score, b, h, q_idx, k_idx): score += alibi_bias(h, q_idx, k_idx) score += time_bias[time_bucket(q_idx, k_idx)] score += duration_bias[duration_bucket(k_idx)] score += organic_bias[is_organic(k_idx)] return score The bias terms are 1D tables and scalar functions. Memory scales with bucket count, not pair count.\nflowchart TD A[Raw QK score] --\u003e B[Add ALiBi] B --\u003e C[Add time bucket bias] C --\u003e D[Add duration bias] D --\u003e E[Add organic bias] E --\u003e F[Final attention score] Why ALiBi Here # Learned positional embeddings are workable, but ALiBi is a better fit for this setup:\nNo position-embedding table Relative bias available in every layer Fewer constraints when pushing sequence lengths Reference: Train Short, Test Long: ALiBi.\nQR Embeddings for Large Categorical Spaces # For high-cardinality IDs (for example, artist IDs), we use quotient-remainder factorization:\nembed(id) = embed_q(id // R) + embed_r(id % R) With R = 1024, this substantially reduces table size while preserving enough representational capacity for ranking.\nThis is a practical tradeoff: a small quality hit is acceptable if it unlocks larger batches and faster iteration.\nSparse IDs and Missing Content Embeddings # Observed properties in this setup:\nItem ID space can extend to ~9.4M Only a subset has precomputed content embeddings Missingness is non-trivial (around 18% in this run) What we did:\nKeep a compact ID-to-RQ lookup for known items Route unknown/missing entries to a deterministic all-zero RQ code pattern Let the model learn a stable fallback behavior for unknown content This avoided huge dense lookup tensors and prevented nondeterministic behavior.\nJagged Batching with Block Masks # All sequence events are concatenated into one token buffer plus offsets. Attention is constrained to legal user-local ranges.\nflowchart LR A[User 1 tokens] --\u003e D[Concatenated token buffer] B[User 2 tokens] --\u003e D C[User 3 tokens] --\u003e D D --\u003e E[Block mask: no cross-user attention] This gives better accelerator utilization than naive per-user padding.\nEarly Signal # Step Main metric 32,432 0.0723 64,864 0.1431 97,296 0.1607 Absolute gain: +0.0884 Relative improvement from first logged point: about 2.2x This is still an intermediate training view; full offline ranking evaluation should include Hit@K, MRR, and NDCG over retrieved candidates. For boundary-case debugging during retrieval-stage evaluation, see The Role of Negative Mining in Machine Learning.\nTradeoffs and Failure Modes # Staff-level review usually focuses on what can go wrong. The biggest risks here are:\nQuantization bottleneck. If RQ codebooks are underfit, retrieval quality saturates early. Bucket design brittleness. Bad time/duration buckets quietly cap model quality. Missingness leakage. If unknown embeddings correlate with labels, the fallback path can become a shortcut feature. Evaluation mismatch. Improvements in training metric may not transfer to retrieval-stage KPIs. Recommended guardrails:\nRun ablations for each bias term (ALiBi/time/duration/organic) Track calibration and recall at retrieval depth Monitor unknown-code frequency by segment Keep one non-quantized baseline for regression detection For retrieval-quality diagnostics at scale, combine this with Entity Resolution using Contrastive Learning-style candidate analysis. Next Experiments # QK normalization Per-layer residual scaling Logit soft-capping Alternative optimizers (including Muon) Explicit ablations of duration and organic priors References # PyTorch FlexAttention ALiBi paper FAISS Mermaid Related Posts # The Role of Negative Mining in Machine Learning Entity Resolution using Contrastive Learning Machine Learning Engineer Roadmap Closing # The architecture scales because it refuses expensive defaults:\nNo dense attention bias matrices No giant item softmax No full-width categorical embedding tables where compression works No padding-heavy batching In short: keep the inductive bias, remove the quadratic and dense bottlenecks.\n"},{"id":4,"href":"/docs/posts/indian_salary_distribution/","title":"Distribution for SDE Salaries in India","section":"Posts","content":" Overview # I recently was having discussions with junior engineers about salary expectations for Software Development Engineers (SDEs) in India, especially how it changes with years of experience. Inspired by The Pragmatic Engineer and @deedydas’ tweet, I decided to examine a dataset of Indian salaries.\nMy goal was to parse the data, clean it, cluster salary ranges by experience, and visualize how salaries distribute across different “tiers” of companies.\nMethodology # Data Collection\nParse the raw excel sheet into a pandas dataframe using BeautifulSoup. Each record contains: Relevant Experience (years) Base Salary Variable Bonus Stock Components Data Cleaning\nFiltered out missing or non-sensical values (“NULL” or negative). Grouped records by integer years of experience. Computed totalSalary as base + (bonus + stocks) for those with 4+ years of experience. Removed outliers within each experience group by cutting off the lower 4% and upper 4% of salaries. Categorizing Companies\nFor each experience bracket, compute a tri-modal Gaussian Mixture Model (GMM) to cluster salaries into “Low”, “Medium”, and “High” categories. Plotting and Summaries\nUsing plotnine (a Python port of ggplot2), plot the faceted histogram by years of experience. Added vertical dashed lines showing mean salaries for each cluster within each experience group. Labeled each cluster with the sample size for clarity. Output # "},{"id":5,"href":"/docs/posts/ml_engineer_guidelines/","title":"Machine Learning Engineer Roadmap","section":"Posts","content":" Mastering the Art of AI Development: A Detailed Roadmap from Data to Deployment # Introduction # Developing an effective artificial intelligence (AI) model is akin to embarking on a long, complex journey. It demands expertise in areas ranging from dataset creation and feature engineering to model tuning, evaluation, and deployment. This blog post will walk you through the key stages involved in AI development, explain the importance of each, and provide a clear understanding of the skills required at different levels of expertise, namely Junior, Senior, and Staff Engineer.\n1. Dataset Creation: Building a Solid Foundation # A robust AI model requires a strong foundation, and this begins with creating an appropriate dataset. The cornerstone of a good dataset is relevant data. How do you identify what\u0026rsquo;s relevant? It\u0026rsquo;s about understanding the signal-to-noise ratio, where the \u0026lsquo;signal\u0026rsquo; is the useful information that can answer your research questions, and \u0026rsquo;noise\u0026rsquo; is the irrelevant data that may skew your results.\nA robust dataset is characterized by comprehensive and diverse features. It should also encompass labeled and unlabeled data. While labeled data serves as the ground truth for training the model, unlabeled data, despite being more challenging to work with, can unearth hidden patterns or associations.\nWe must also address potential implicit bias in our dataset. Bias can skew the model\u0026rsquo;s performance and harm its ability to make fair decisions. Careful data collection, rigorous analysis, and bias-correction techniques can help account for it.\n2. Feature Engineering: Turning Raw Data into Meaningful Information # Once we have a dataset, it\u0026rsquo;s time for feature engineering. This process involves selecting the most relevant features and transforming raw data into formats that the model can understand better.\nFeaturization techniques like one-hot encoding, binning, or polynomial features can be employed depending on the nature of your data. The distribution of the dataset also plays a key role in deciding which features to include.\nNormalization is another crucial step to ensure that extreme values or outliers don\u0026rsquo;t distort the model\u0026rsquo;s performance. This depends on the specific distribution of your data and the problem you\u0026rsquo;re trying to solve.\n3. Modeling: Choosing and Improving Your Tool # The modeling stage is where the magic happens. This is where we choose the algorithm that will learn from our data. We begin with a baseline model—a simple technique that sets the minimum performance expectation.\nBaseline models come with their own pros and cons. For example, a linear regression model may be easy to implement and interpret but may not handle complex relationships between features and outcomes well. We need to contextualize these models with our problem at hand.\nNext, we move on to more advanced models. We might opt for neural networks or ensemble methods, depending on the problem. These models need to be fine-tuned to handle bias and adapt to the specific context of the problem. This involves choosing appropriate optimization and loss functions.\n4. Evaluation Measure: Assessing Your Model # Now, we need to assess how our model performs. Depending on the problem, we could use measures like accuracy, precision, recall, or the F1 score for classification problems, or mean squared error, mean absolute error, or R-squared for regression problems.\nThese evaluation measures each have their strengths and limitations, and they assess both extrinsic and intrinsic properties of the model. For instance, accuracy might be a good measure when the classes are balanced, but it would be misleading for imbalanced datasets.\n5. Confidence Scoring and Tuning: Trusting Your Model # Confidence scoring helps us understand how certain our model is about its predictions. A well-calibrated model\u0026rsquo;s confidence aligns well with its accuracy. Both pre-training (like regularization techniques) and post-training methods (like Platt scaling) can help us calibrate our models.\nWhile it\u0026rsquo;s useful in identifying model issues, it\u0026rsquo;s important to remember that a high confidence score doesn\u0026rsquo;t always mean a correct prediction and vice versa.\n6. Inference: Deploying Your Model # Once we\u0026rsquo;re satisfied with the model\u0026rsquo;s performance, we\u0026rsquo;re ready to deploy it. This requires careful planning, from selecting the appropriate hardware to efficiently using the cores and instructions. The choice between different precision formats like int8, fp16, bf16, etc., depends on the trade-off we want to make between speed and accuracy.\nMoreover, understanding concepts like queuing theory, throughput, and latency relationships can help scale models effectively.\n7. Optimizations/Improvements Cycle: Enhancing Your Model # After deployment, our job isn\u0026rsquo;t over. We need to continuously monitor our model\u0026rsquo;s performance and make necessary improvements. This might involve tweaking features, changing the model architecture, or even collecting more data.\n8. Monitoring and Metrics: Keeping an Eye on Your Model # An effective monitoring system is crucial in maintaining the performance of our models. We need to set up alerts for key performance metrics and keep a close eye on these online metrics. Following a systematic MLOps architecture helps manage models better.\n9. User Feedback Pipeline: Learning from Your Users # Incorporating user feedback into our model improvements is vital. We need to be alert to concept drifts, where the relationships between variables change over time, and hard negatives that are consistently misclassified. Understanding the impact of both implicit and explicit feedback can help fine-tune our model further.\nThe Path to AI Mastery: Skills at Different Levels # While a Junior AI engineer should be aware of the entire development pipeline, they are expected to show proficiency (scoring 3) in dataset creation, feature engineering, modeling, evaluation, and inference, with a basic understanding (scoring 2) in other areas.\nA Senior AI engineer should exhibit minimum proficiency in all stages and demonstrate advanced knowledge (scoring 4) in key areas.\nA Staff engineer, the highest level, should have expert-level knowledge and skills (scoring 5) in multiple key areas and be able to demonstrate substantial outcomes.\nBuilding an AI model is a complex task that requires a wide array of skills. But with the right understanding, continuous learning, and constant practice, you can embark on this exciting journey with confidence. Happy modeling!\n"},{"id":6,"href":"/docs/posts/supervised_finetuning/","title":"Supervised Fine-Tuning in Large Language Models","section":"Posts","content":" The Power of Supervised Fine-Tuning in Large Language Models: An In-depth Analysis # Introduction # In recent years, the development of machine learning, particularly large language models (LLMs), has revolutionized the way we approach a multitude of challenges, from query-based tasks to content generation. In this post, we will dive deep into a technique gaining traction within the AI community - supervised fine-tuning using domain-specific instruction datasets - and contrast it with the more conventional prompt tuning approach, with a focus on techniques such as retrieval augmentation.\nWhat is Supervised Fine-Tuning? # Supervised fine-tuning involves adjusting a pre-trained LLM to improve its performance using a specific dataset that contains examples from a targeted domain. For instance, to train an LLM for medical consultation, one might use a dataset comprising medical textbooks, research papers, and patient-doctor interactions. Using instruction-answer-context pairs from social media conversations to build better contextual assistants.\nForming the Dataset # Creating an effective dataset for supervised fine-tuning is a nuanced process. The dataset must be a balanced representation of the domain you\u0026rsquo;re aiming to specialize in, so it\u0026rsquo;s vital to include diverse and contextually rich information sources. Privacy and data ethics are of paramount concern during the data collection process.\nAdvantages of Supervised Fine-Tuning # Domain Specificity: Supervised fine-tuning allows the model to be customized to a particular domain, resulting in more accurate and contextually relevant outputs. Better Generalization: A fine-tuned model can generalize better to new data within the same domain, as it has learned the specific patterns and nuances of the field. Efficient Usage of Parameters: Fine-tuning allows the vast parameter space of LLMs to be effectively utilized for domain-specific tasks, leading to parameter-efficient fine-tuning. Limitations of Supervised Fine-Tuning # Dataset Quality: The success of supervised fine-tuning largely hinges on the quality of the dataset. Poorly curated or biased datasets can lead to subpar or skewed results. Overfitting: The model can overfit to the training data, leading to less than optimal performance on unseen data. Resource-Intensive: Fine-tuning requires significant computational resources, making it more expensive than some other methods. Comparison with Prompt Tuning # Prompt tuning, by contrast, employs a more straightforward approach, guiding the LLM to generate desired responses using specifically crafted prompts. While this method is simpler and less resource-intensive, it lacks the domain specificity and generalization capabilities offered by supervised fine-tuning.\nRetrieval Augmentation # One method commonly used in prompt tuning is retrieval augmentation, where the model is trained to pull in relevant external information to enhance its responses. While this can lead to more informative replies, the quality of the output still largely depends on the relevancy and accuracy of the external data sourced, which can be a hit-or-miss.\nParameter-Efficient Fine-Tuning # Parameter-efficient fine-tuning refers to the idea of making the best use of the available parameters in a model during the fine-tuning process. With supervised fine-tuning, this can be achieved by selectively updating parameters that contribute most to the target domain, thereby improving the model\u0026rsquo;s performance while keeping computational costs in check.\nConclusion # Both supervised fine-tuning and prompt tuning have their place in the world of large language models. The choice between the two often depends on the specific requirements of the task at hand, the resources available, and the complexity of the domain. In tasks where domain-specific accuracy and robust generalization are of paramount importance, supervised fine-tuning with a well-curated instruction dataset appears to hold the edge. The resource-intensiveness and potential overfitting risks associated with it, however, call for careful implementation and ongoing evaluation. As the field evolves, the development of even more efficient and effective tuning techniques will undoubtedly continue.\n"},{"id":7,"href":"/docs/posts/hard_negatives/","title":"The Role of Negative Mining in Machine Learning: Bridging the Gap in Model Performance","section":"Posts","content":" Introduction # Machine learning models are excellent tools for making predictions or classifications. However, they\u0026rsquo;re not infallible; occasionally, they may make mistakes. Some of the most enlightening mistakes are the so-called \u0026ldquo;hard negatives\u0026rdquo; — instances where the model confidently produces the incorrect output. Understanding and learning from these instances through hard negative mining can significantly improve the model\u0026rsquo;s performance.\nUnderstanding Hard Negative Mining # In machine learning, \u0026ldquo;hard negatives\u0026rdquo; refer to examples that are challenging for the model to classify correctly. They are the negatives that the model most often misclassifies. Hard negative mining is a strategy for improving the performance of a model by focusing on these difficult-to-classify instances.\nWhy is it Important to Measure Performance in the Threshold Region? # The threshold region is where the model makes its most decisive judgments. It is in this region that we identify the hard negatives. By focusing on the threshold region, we can specifically diagnose where the model struggles and concentrate our efforts to improve those areas.\nHow to Identify Hard Negatives # Typically, hard negatives are identified by observing the model\u0026rsquo;s performance in real-world scenarios. Debugging these cases can often lead to revealing insights about the model\u0026rsquo;s shortcomings. During this process, it\u0026rsquo;s crucial to analyze the model outputs concerning the features, thereby identifying potential missing properties in the feature set that lead to incorrect predictions.\nTraining Strategies to Address Hard Negatives # Once the hard negatives have been identified and analyzed, the next step is to use this information to improve the model. This might involve:\nExpanding the training set: Incorporating more examples of hard negatives into the training set can improve the model\u0026rsquo;s ability to correctly classify these cases in the future. Fine-tuning the model: Sometimes, it may not be necessary to retrain the entire model. Instead, you could fine-tune the model on the hard negatives, enabling it to learn from its mistakes without needing to revisit all the previous training data. Revising the features: If the hard negatives are a result of inadequate or poor features, consider revising the feature set. This could involve engineering new features or improving the quality of existing ones. Conclusion # Hard negative mining is a powerful technique for improving the performance of machine learning models. By focusing on the hardest examples, we can refine our models to become more robust and accurate. The insights gained from studying these difficult cases can also help us improve our features and make our models even more effective.\n"},{"id":8,"href":"/docs/posts/entity_resolution/","title":"Entity Resolution using Contrastive Learning","section":"Posts","content":" Introduction to Entity Resolution # Entity resolution (also known as entity matching, record linkage, or duplicate detection) is the task of finding records that refer to the same real-world entity across different data sources (e.g., data files, books, websites, and databases).\nThis can be a challenging task, especially when the dataset is large and the queries mention the attributes of the entities in various ways, such as with partial information, typing errors, abbreviations, or extra information. In this blog post, we\u0026rsquo;ll be discussing how to approach the Entity Resolution Problem and the solution that was implemented to solve it.\nProblem Definition # Imagine you have a dataset of approximately 50 million entities, and your task is to find the right entity for a given query. The query could be a few of the entity\u0026rsquo;s attributes, and these queries could mention the attributes in various ways. This is the Entity Resolution Problem.\nThe Existing Solution # One solution to this problem is an Elastic search-based match, which uses complicated heuristics that are overfitted on a small training set. However, this solution is not scalable and the accuracy of the top-20 search retrieval decreases exponentially as the number of entities increases.\nAt the time this problem was being addressed, the top-20 search retrieval accuracy was around 40% for the current number of entities.\nThe Implemented Solution # To solve the Entity Resolution Problem, an embedding search was implemented using a Sentence embedding model. The Deberta model was pretrained and fine-tuned for the current problem using contrastive learning. In contrastive learning, positive pairs are generated using augmentations for each attribute that best mock the queries, based on the many user queries received.\nCustom augmentations which syntheically generate query like variations were used during training time to help the model learn generate positive similarity score for entity, query pair.\nResults # With this solution, the top-20 accuracy was around 98%. Heuristics and other business logic, along with a properly calculated confidence measure (which was hyperparameter-tuned on the validation set), were used to filter out the right entity. After the final pipeline was implemented, a top-1 accuracy of around 99.995% (precision) and 86% (recall) was achieved for high confidence matches.\nIn the end, pinecone was chosen for the embedding search and the search latency was around 100ms for the top 50 among the 50 million embeddings.\nConclusion # To conclude, the Entity Resolution Problem was successfully solved by implementing an embedding search using a Sentence embedding model and fine-tuning it with contrastive learning. This solution had a significantly higher accuracy compared to the existing Elastic search-based solution and was able to scale well as the number of entities increased.\n"}]