<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Yambda on /home/vigi99</title><link>https://viig99.github.io/tags/Yambda/</link><description>Recent content in Yambda on /home/vigi99</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 15 Feb 2026 18:41:14 -0500</lastBuildDate><atom:link href="https://viig99.github.io/tags/Yambda/index.xml" rel="self" type="application/rss+xml"/><item><title>FlexAttention HSTU at 500M Events: RQ Tokens, QR Embeddings, and 1D Biases</title><link>https://viig99.github.io/docs/posts/hstu-for-yambda/</link><pubDate>Sun, 15 Feb 2026 00:00:00 +0000</pubDate><guid>https://viig99.github.io/docs/posts/hstu-for-yambda/</guid><description>&lt;h2 id="executive-summary"&gt;Executive Summary&lt;a class="anchor" href="#executive-summary"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;At Yambda scale (about 500M events and 9.4M items), models rarely break because one idea is bad. They break when many small, expensive defaults pile up.&lt;/p&gt;
&lt;p&gt;This post is the story of the choices that kept an HSTU-style recommender trainable:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Jagged, block-masked attention with &lt;a href="https://pytorch.org/docs/stable/nn.attention.flex_attention.html" target="_blank" rel="noopener" &gt;PyTorch FlexAttention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Residual quantization (RQ) token prediction instead of a giant item-ID softmax&lt;/li&gt;
&lt;li&gt;Quotient-remainder (QR) embeddings for large sparse categorical spaces&lt;/li&gt;
&lt;li&gt;On-the-fly 1D attention bias terms (time, duration, organic) instead of dense &lt;code&gt;[S, S]&lt;/code&gt; bias tensors&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/2108.12409" target="_blank" rel="noopener" &gt;ALiBi&lt;/a&gt; positional bias instead of learned position embeddings&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The throughline is simple: keep the inductive bias, cut the dense and quadratic costs.&lt;/p&gt;</description></item></channel></rss>