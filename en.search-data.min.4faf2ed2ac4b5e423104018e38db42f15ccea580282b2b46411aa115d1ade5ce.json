[{"id":0,"href":"/docs/work-experience/","title":"Work Experience","section":"Docs","content":" Work Experience # Staff Machine Learning Engineer # Fora # Jan 2023 - Present - Toronto, CA # Working at Fora, a part of VerticalScope\u0026rsquo;s cloud-based digital platform that operates over 1,200 online communities connecting 110 million active users monthly, fostering their passions, and facilitating knowledge sharing. My role focuses on building healthier and better communities by empowering users with machine learning. Here are some of the tasks I have been responsible for: Empowering communities with machine learning to enhance user experience. Personalized Feed Recommendation\u0026rsquo;s, Newsletter growth, mobile notifications to all communities. Developing advanced search algorithms for accurate, tailored results. Optimizing product retrieval and facilitating new product discovery within communities. Answering user questions and providing technical support, using LLMs. Interviewing and mentoring junior engineers. Staff Machine Learning Engineer # Kalepa # Apr 2022 - Sep 2022 - Toronto, CA # Enhanced the in-house News Recommendation System using Sentence Embeddings. Identified and implemented Document Question Answering, as configurable classifiers to analysis various risks on businesses. Constructed and deployed an Entity Resolution System utilizing Unsupervised Contrastive Learning. improved top-20 search accuracy from 35% to 98% for 10^7+ entities. Cut down search latency from 1.5s to 0.3s. Crafted a scalable system employing Postgres, Onnx, Pinecone, fastAPI, with Dockerized deployment. Conducted 35+ candidate interviews and developed a system for pinpointing the most suitable candidates for respective roles. Principal Machine Learning Engineer # Airtel X-labs # Sep 2018 - Mar 2022 - Bangalore, IN # Led the product development of Voicebot engine which powers voice-based queries on the MyAirtel app with 10m MAU, in 7 indian languages, does 500k queries/day. (Speech to text, Text to speech, training and inference pipelines.) Voicebot integeration with PBX exchange like Asterix. Presented work at Nvidia GTC Winter 2020 Researched and deployed e2e OCR pipeline serving 1.6m docs/day at 96%+ accuracy, used by Airtel for its new customer acquisition journey ICDAR Rank 6 Building the workflow-orchestration engine which powers the customer support queries on mail / social media for Airtel, processes 50k emails/day, built on k8, temporal.io Supported 150 different workflows with ~50 activities running concurrently. Hired and led a team of 9 engineers. Co-founding Engineer # AuthME ID Solutions, Acquired by Airtel # Aug 2017 - Sep 2018 - Bangalore, IN # Built OCR pipeline for reading arbitrary documents, 5 step process with word localization, word recognition, clustering, parsing and serving. Built Voice based IVR bot for Indian business by building on top of DeepSpeech and Rasa NLU. Invited for YC 2018 Winter Interview Stage in San Francisco. Machine Learning Engineer # Krowd # June 2015 - Aug 2017 - Bangalore, IN # Recommendation \u0026amp; ranking for users by clustering restaurants into latent topics space and recommending fresh restaurants built on node.js. \u0026lt;200ms latency over a set of 1 million restaurants per user. Loyalty and rewards platform with second price ad bidding for banks (pilot run with Royal Bank of Scotland). Software Development Engineer # Amazon # Feb 2013 - Feb 2015 - Bangalore, IN # Built the auto correcting \u0026amp; predictive completion language keyboards for regions like germany, japan etc based on the hidden markov model. Worked on Developing \u0026amp; Deploying Amazon Instant Video on 13 different living room TV environments in 10 months to 1m+ customers. Scaling \u0026amp; building a/b testing framework to test the application across various regions. Associate Software Engineer # Kony Labs # July 2011 - Nov 2012 - Hyderabad, IN # Developed a JavaScript single templating based backend/frontend framework. Integrated native platform level code with existing Lua code using Foreign Function Interface. "},{"id":1,"href":"/docs/open-source/","title":"Open Source Contributions","section":"Docs","content":" Open Source Contributions # SymSpellCppPy # Python library for Spelling correction based on SymSpell written in C++ and exposed to python via pybind11.\nBlaze # ML inference framework for pytorch models in Asynchronous C++ which supports dynamic batching, Arrayfire, quantized model, and various optimizations written using Drogon\nSABER # Easily reproducible machine learning baseline for automatic speech recognition using semi-supervised contrastive learning.\nEpoch-Synchronous Overlap-Add (ESOLA) # Fast C++ implementation of ESOLA using KFRLib, can be used for online time-stretch augmentation during SpeechToText training.\nNewman # Initial Contributor to Newman which is a command-line collection runner for Postman.\nPostman Interceptor # Initial Contributor to Postman Interceptor a helper extension for the Postman packaged app.\n"},{"id":2,"href":"/docs/machine-learning/","title":"Machine Learning Toolkit","section":"Docs","content":" Machine Learning Toolkit: Skills and Expertise # Data Engineering # Expertise in building unbiased datasets via feature-based sampling. Proficient in generating synthetic data matching real data distribution. Skilled in augmentation techniques for vision, speech, NLP. Modelling and Feature Engineering # Comprehensive knowledge of Convolutional, Recurrent, and Transformer-based models. Experience with feature importance techniques. Proficient with Contrastive Learning methods like SimCLR, BYOL, SimSiam. Well-versed in model debugging and profiling. Experienced with topic models using Probabilistic Graphic Models and embedding-based clustering. Training # Proficient in distributed training using OpenMPI + RoCE, Torch RPC. Skilled in Pytorch Lightning optimizations. Calibration # Expertise in implicit calibration techniques like Focal Loss, Maximum Entropy Regularization, Label Smoothing, Random Dropout. Experience with explicit calibration techniques like Isotonic Regression, Platt\u0026rsquo;s scaling. Optimizations # Skilled in model optimizations such as Quantization, Pruning, Distillation. Proficient in ML Ops Fusing techniques like ONNX, TorchDynamo, TVM. Inference # Expertise in C++ inference using ONNX \u0026amp; Drogon. Experience with frameworks like Triton, Mosec. Skilled in scaling on k8 using OKD. Proficient in monitoring and alerting using Vector.io, Prometheus, Grafana. Online Monitoring # Expertise in hard negative mining around calibrated threshold region. Experience with sampling and saving hard negatives. Skilled in detecting and alerting on Model and Data Drifts. "},{"id":3,"href":"/docs/posts/ml_engineer_guidelines/","title":"Machine Learning Engineer Roadmap","section":"Posts","content":" Mastering the Art of AI Development: A Detailed Roadmap from Data to Deployment # Introduction # Developing an effective artificial intelligence (AI) model is akin to embarking on a long, complex journey. It demands expertise in areas ranging from dataset creation and feature engineering to model tuning, evaluation, and deployment. This blog post will walk you through the key stages involved in AI development, explain the importance of each, and provide a clear understanding of the skills required at different levels of expertise, namely Junior, Senior, and Staff Engineer.\n1. Dataset Creation: Building a Solid Foundation # A robust AI model requires a strong foundation, and this begins with creating an appropriate dataset. The cornerstone of a good dataset is relevant data. How do you identify what\u0026rsquo;s relevant? It\u0026rsquo;s about understanding the signal-to-noise ratio, where the \u0026lsquo;signal\u0026rsquo; is the useful information that can answer your research questions, and \u0026rsquo;noise\u0026rsquo; is the irrelevant data that may skew your results.\nA robust dataset is characterized by comprehensive and diverse features. It should also encompass labeled and unlabeled data. While labeled data serves as the ground truth for training the model, unlabeled data, despite being more challenging to work with, can unearth hidden patterns or associations.\nWe must also address potential implicit bias in our dataset. Bias can skew the model\u0026rsquo;s performance and harm its ability to make fair decisions. Careful data collection, rigorous analysis, and bias-correction techniques can help account for it.\n2. Feature Engineering: Turning Raw Data into Meaningful Information # Once we have a dataset, it\u0026rsquo;s time for feature engineering. This process involves selecting the most relevant features and transforming raw data into formats that the model can understand better.\nFeaturization techniques like one-hot encoding, binning, or polynomial features can be employed depending on the nature of your data. The distribution of the dataset also plays a key role in deciding which features to include.\nNormalization is another crucial step to ensure that extreme values or outliers don\u0026rsquo;t distort the model\u0026rsquo;s performance. This depends on the specific distribution of your data and the problem you\u0026rsquo;re trying to solve.\n3. Modeling: Choosing and Improving Your Tool # The modeling stage is where the magic happens. This is where we choose the algorithm that will learn from our data. We begin with a baseline model—a simple technique that sets the minimum performance expectation.\nBaseline models come with their own pros and cons. For example, a linear regression model may be easy to implement and interpret but may not handle complex relationships between features and outcomes well. We need to contextualize these models with our problem at hand.\nNext, we move on to more advanced models. We might opt for neural networks or ensemble methods, depending on the problem. These models need to be fine-tuned to handle bias and adapt to the specific context of the problem. This involves choosing appropriate optimization and loss functions.\n4. Evaluation Measure: Assessing Your Model # Now, we need to assess how our model performs. Depending on the problem, we could use measures like accuracy, precision, recall, or the F1 score for classification problems, or mean squared error, mean absolute error, or R-squared for regression problems.\nThese evaluation measures each have their strengths and limitations, and they assess both extrinsic and intrinsic properties of the model. For instance, accuracy might be a good measure when the classes are balanced, but it would be misleading for imbalanced datasets.\n5. Confidence Scoring and Tuning: Trusting Your Model # Confidence scoring helps us understand how certain our model is about its predictions. A well-calibrated model\u0026rsquo;s confidence aligns well with its accuracy. Both pre-training (like regularization techniques) and post-training methods (like Platt scaling) can help us calibrate our models.\nWhile it\u0026rsquo;s useful in identifying model issues, it\u0026rsquo;s important to remember that a high confidence score doesn\u0026rsquo;t always mean a correct prediction and vice versa.\n6. Inference: Deploying Your Model # Once we\u0026rsquo;re satisfied with the model\u0026rsquo;s performance, we\u0026rsquo;re ready to deploy it. This requires careful planning, from selecting the appropriate hardware to efficiently using the cores and instructions. The choice between different precision formats like int8, fp16, bf16, etc., depends on the trade-off we want to make between speed and accuracy.\nMoreover, understanding concepts like queuing theory, throughput, and latency relationships can help scale models effectively.\n7. Optimizations/Improvements Cycle: Enhancing Your Model # After deployment, our job isn\u0026rsquo;t over. We need to continuously monitor our model\u0026rsquo;s performance and make necessary improvements. This might involve tweaking features, changing the model architecture, or even collecting more data.\n8. Monitoring and Metrics: Keeping an Eye on Your Model # An effective monitoring system is crucial in maintaining the performance of our models. We need to set up alerts for key performance metrics and keep a close eye on these online metrics. Following a systematic MLOps architecture helps manage models better.\n9. User Feedback Pipeline: Learning from Your Users # Incorporating user feedback into our model improvements is vital. We need to be alert to concept drifts, where the relationships between variables change over time, and hard negatives that are consistently misclassified. Understanding the impact of both implicit and explicit feedback can help fine-tune our model further.\nThe Path to AI Mastery: Skills at Different Levels # While a Junior AI engineer should be aware of the entire development pipeline, they are expected to show proficiency (scoring 3) in dataset creation, feature engineering, modeling, evaluation, and inference, with a basic understanding (scoring 2) in other areas.\nA Senior AI engineer should exhibit minimum proficiency in all stages and demonstrate advanced knowledge (scoring 4) in key areas.\nA Staff engineer, the highest level, should have expert-level knowledge and skills (scoring 5) in multiple key areas and be able to demonstrate substantial outcomes.\nBuilding an AI model is a complex task that requires a wide array of skills. But with the right understanding, continuous learning, and constant practice, you can embark on this exciting journey with confidence. Happy modeling!\n"},{"id":4,"href":"/docs/posts/hard_negatives/","title":"The Role of Negative Mining in Machine Learning: Bridging the Gap in Model Performance","section":"Posts","content":" Introduction # Machine learning models are excellent tools for making predictions or classifications. However, they\u0026rsquo;re not infallible; occasionally, they may make mistakes. Some of the most enlightening mistakes are the so-called \u0026ldquo;hard negatives\u0026rdquo; — instances where the model confidently produces the incorrect output. Understanding and learning from these instances through hard negative mining can significantly improve the model\u0026rsquo;s performance.\nUnderstanding Hard Negative Mining # In machine learning, \u0026ldquo;hard negatives\u0026rdquo; refer to examples that are challenging for the model to classify correctly. They are the negatives that the model most often misclassifies. Hard negative mining is a strategy for improving the performance of a model by focusing on these difficult-to-classify instances.\nWhy is it Important to Measure Performance in the Threshold Region? # The threshold region is where the model makes its most decisive judgments. It is in this region that we identify the hard negatives. By focusing on the threshold region, we can specifically diagnose where the model struggles and concentrate our efforts to improve those areas.\nHow to Identify Hard Negatives # Typically, hard negatives are identified by observing the model\u0026rsquo;s performance in real-world scenarios. Debugging these cases can often lead to revealing insights about the model\u0026rsquo;s shortcomings. During this process, it\u0026rsquo;s crucial to analyze the model outputs concerning the features, thereby identifying potential missing properties in the feature set that lead to incorrect predictions.\nTraining Strategies to Address Hard Negatives # Once the hard negatives have been identified and analyzed, the next step is to use this information to improve the model. This might involve:\nExpanding the training set: Incorporating more examples of hard negatives into the training set can improve the model\u0026rsquo;s ability to correctly classify these cases in the future. Fine-tuning the model: Sometimes, it may not be necessary to retrain the entire model. Instead, you could fine-tune the model on the hard negatives, enabling it to learn from its mistakes without needing to revisit all the previous training data. Revising the features: If the hard negatives are a result of inadequate or poor features, consider revising the feature set. This could involve engineering new features or improving the quality of existing ones. Conclusion # Hard negative mining is a powerful technique for improving the performance of machine learning models. By focusing on the hardest examples, we can refine our models to become more robust and accurate. The insights gained from studying these difficult cases can also help us improve our features and make our models even more effective.\n"}]