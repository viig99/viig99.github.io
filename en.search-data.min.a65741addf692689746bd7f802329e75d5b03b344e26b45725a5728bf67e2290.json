[{"id":0,"href":"/docs/work-experience/","title":"Work Experience","section":"Docs","content":"Work Experience# Senior Applied ML Engineer# Shopify# Sep 2025 - Present - Toronto, CA# Working on merchant foundation models. Staff Machine Learning Engineer# VerticalScope Inc.# Jan 2023 - Jun 2025 - Toronto, CA# Worked at Fora, a part of VerticalScope\u0026rsquo;s cloud-based digital platform that operates over 1,200 online communities connecting 110 million active users monthly, fostering their passions, and facilitating knowledge sharing. My role focused on building healthier and better communities by empowering users with machine learning. Here are some of the tasks I was responsible for: Empowering communities with machine learning to enhance user experience. Personalized Feed Recommendation\u0026rsquo;s, Newsletter growth, mobile notifications to all communities. Developing advanced search algorithms for accurate, tailored results. Optimizing product retrieval and facilitating new product discovery within communities. Answering user questions and providing technical support, using LLMs. Interviewing and mentoring junior engineers. Staff Machine Learning Engineer# Kalepa# Apr 2022 - Sep 2022 - Toronto, CA# Enhanced the in-house News Recommendation System using Sentence Embeddings. Identified and implemented Document Question Answering, as configurable classifiers to analysis various risks on businesses. Constructed and deployed an Entity Resolution System utilizing Unsupervised Contrastive Learning. improved top-20 search accuracy from 35% to 98% for 10^7+ entities. Cut down search latency from 1.5s to 0.3s. Crafted a scalable system employing Postgres, Onnx, Pinecone, fastAPI, with Dockerized deployment. Conducted 35+ candidate interviews and developed a system for pinpointing the most suitable candidates for respective roles. Principal Machine Learning Engineer# Airtel X-labs# Sep 2018 - Mar 2022 - Bangalore, IN# Led the product development of Voicebot engine which powers voice-based queries on the MyAirtel app with 10m MAU, in 7 indian languages, does 500k queries/day. (Speech to text, Text to speech, training and inference pipelines.) Voicebot integeration with PBX exchange like Asterix. Presented work at Nvidia GTC Winter 2020 Researched and deployed e2e OCR pipeline serving 1.6m docs/day at 96%+ accuracy, used by Airtel for its new customer acquisition journey ICDAR Rank 6 Building the workflow-orchestration engine which powers the customer support queries on mail / social media for Airtel, processes 50k emails/day, built on k8, temporal.io Supported 150 different workflows with ~50 activities running concurrently. Hired and led a team of 9 engineers. Co-founding Engineer# AuthME ID Solutions, Acquired by Airtel# Aug 2017 - Sep 2018 - Bangalore, IN# Built OCR pipeline for reading arbitrary documents, 5 step process with word localization, word recognition, clustering, parsing and serving. Built Voice based IVR bot for Indian business by building on top of DeepSpeech and Rasa NLU. Invited for YC 2018 Winter Interview Stage in San Francisco. Machine Learning Engineer# Krowd# June 2015 - Aug 2017 - Bangalore, IN# Recommendation \u0026amp; ranking for users by clustering restaurants into latent topics space and recommending fresh restaurants built on node.js. \u0026lt;200ms latency over a set of 1 million restaurants per user. Loyalty and rewards platform with second price ad bidding for banks (pilot run with Royal Bank of Scotland). Software Development Engineer# Amazon# Feb 2013 - Feb 2015 - Bangalore, IN# Built the auto correcting \u0026amp; predictive completion language keyboards for regions like germany, japan etc based on the hidden markov model. Worked on Developing \u0026amp; Deploying Amazon Instant Video on 13 different living room TV environments in 10 months to 1m+ customers. Scaling \u0026amp; building a/b testing framework to test the application across various regions. Associate Software Engineer# Kony Labs# July 2011 - Nov 2012 - Hyderabad, IN# Developed a JavaScript single templating based backend/frontend framework. Integrated native platform level code with existing Lua code using Foreign Function Interface. "},{"id":1,"href":"/docs/open-source/","title":"Open Source Contributions","section":"Docs","content":"Open Source Contributions# SymSpellCppPy# Python library for Spelling correction based on SymSpell written in C++ and exposed to python via pybind11.\nComparing Contrastive losses on Vision \u0026amp; NLP# Comparing performance of different InfoNCE type losses used in contrastive learning.\nWindy Lunar Lander Env# Exploring Reinforcement Learning algorithms on customized Lunar Lander environment with dynamic realistic wind vectors by extending the gym environment.\nBlaze# ML inference framework for pytorch models in Asynchronous C++ which supports dynamic batching, Arrayfire, quantized model, and various optimizations written using Drogon\nSABER# Easily reproducible machine learning baseline for automatic speech recognition using semi-supervised contrastive learning.\nEpoch-Synchronous Overlap-Add (ESOLA)# Fast C++ implementation of ESOLA using KFRLib, can be used for online time-stretch augmentation during SpeechToText training.\nNewman# Initial Contributor to Newman which is a command-line collection runner for Postman.\nPostman Interceptor# Initial Contributor to Postman Interceptor a helper extension for the Postman packaged app.\n"},{"id":2,"href":"/docs/machine-learning/","title":"Machine Learning Toolkit","section":"Docs","content":"Machine Learning Toolkit: Skills and Expertise# Data Engineering# Expertise in building unbiased datasets via feature-based sampling. Proficient in generating synthetic data matching real data distribution. Skilled in augmentation techniques for vision, speech, NLP. Modelling and Feature Engineering# Comprehensive knowledge of Convolutional, Recurrent, and Transformer-based models. Experience with feature importance techniques. Proficient with Contrastive Learning methods like SimCLR, BYOL, SimSiam. Well-versed in model debugging and profiling. Experienced with topic models using Probabilistic Graphic Models and embedding-based clustering. Training# Proficient in distributed training using OpenMPI + RoCE, Torch RPC. Skilled in Pytorch Lightning optimizations. Calibration# Expertise in implicit calibration techniques like Focal Loss, Maximum Entropy Regularization, Label Smoothing, Random Dropout. Experience with explicit calibration techniques like Isotonic Regression, Platt\u0026rsquo;s scaling. Optimizations# Skilled in model optimizations such as Quantization, Pruning, Distillation. Proficient in ML Ops Fusing techniques like ONNX, TorchDynamo, TVM. Inference# Expertise in C++ inference using ONNX \u0026amp; Drogon. Experience with frameworks like Triton, Mosec. Skilled in scaling on k8 using OKD. Proficient in monitoring and alerting using Vector.io, Prometheus, Grafana. Online Monitoring# Expertise in hard negative mining around calibrated threshold region. Experience with sampling and saving hard negatives. Skilled in detecting and alerting on Model and Data Drifts. "},{"id":3,"href":"/docs/posts/imba-chess/","title":"Building a Chess Bot with HSTU: From Lichess Pretraining to Value Search","section":"Posts","content":"Executive Summary# This is a running journal of imba-chess, a personal research project to build a compact (20–30M parameter) chess bot entirely from supervised pretraining on Lichess games, without any game-tree search engine to begin with.\nThe journey so far:\nAdapted HSTU-style causal sequence modeling from recommender systems to chess. Trained next-move prediction (policy head) on high-Elo Lichess game data. Evaluated offline metrics: top-1, hr@10, MRR. Tested against Stockfish. Results: humbling. Added a value head (WDL), wired it into move selection. Implemented depth-2 policy-pruned minimax search on top. Defined a Stockfish ladder evaluation pipeline for reproducible strength measurement. The throughline is: treat each chess ply as a structured event in a sequence, the same way you would model user behavior.\nWhy Chess and Why HSTU# Most chess neural nets (Leela, AlphaZero) are trained purely by self-play, using MCTS to generate experience. That is powerful but expensive. A cheaper first step is imitation: train on human games, learn the distribution of plausible moves, and then layer search on top.\nHSTU (Hierarchical Sequential Transduction Unit) was designed for large-scale sequential recommendation — jagged user histories, variable event types, structured side features. Chess plylines have almost the same structure: a variable-length sequence of structured events (board states + moves), with a known target (next move) at each step.\nThe idea was: if HSTU can predict next song plays, it can predict next chess moves.\nData Pipeline# Source# All data comes from Lichess open game database, accessed via the Lichess/standard-chess-games Hugging Face dataset. It is hive-partitioned by year/month, which makes temporal splits clean.\nFilter: average Elo of (WhiteElo + BlackElo) / 2 \u0026gt;= 2000. High-Elo games have lower noise and better move quality.\nTemporal Splits# Splits are chronological, not random. This prevents future-move leakage and gives a more realistic out-of-sample test.\nSplit Window train 2018-01 through 2025-07 val 2025-08 (single month) test 2025-09 (single month) No model is selected using the test split.\nPGN Parsing and Board State# Each game is replayed with python-chess. At each ply, the board is converted to a structured token representation — not a FEN string, not a text sequence.\nBoard state per ply:\nField Values Notes piece_ids [64], 0–12 0=empty, 1–6=white, 7–12=black turn_id 0/1 side to move castle_id 0–15 KQkq bitmask ep_file_id 0–8 en-passant file + 1, 0=none halfmove_bucket_id ≥0 bucketed clock fullmove_bucket_id ≥0 bucketed move number Targets are UCI move IDs from a static vocabulary of all legal UCI moves (from→to + promotions).\nIncremental Board Updates# Rebuilding piece_ids from board.piece_map() every ply takes ~14–16 µs per ply. The optimization: update only the changed squares.\nA chess move touches at most 4 squares (castling: king + rook). So we maintain a bytearray(64) and apply incremental updates:\nNormal move: 2 squares Capture: 2 squares En passant: 3 squares Castling: 4 squares Promotion: 2 squaresThis drops board encoding from ~16 µs to ~2–5 µs per ply. At Lichess dataset scale (hundreds of millions of plies) this is a meaningful saving.\nJagged Batching# Multiple games are packed into a single flat token buffer, with seq_offsets marking boundaries. This is the same trick from FlexAttention HSTU at 500M Events: no cross-game attention, no padding waste.\n[BOS | ply1 ply2 ... plyN | BOS | ply1 ... plyM | ...] game 1 game 2Batch shape: [total_tokens] for most fields, [total_tokens, 64] for piece_ids. No attention mask needed when seq_offsets are passed directly to FlexAttention.\nModel Architecture# Overview# flowchart LR A[piece_ids 64 tokens] --\u0026gt; B[E_piece + E_square] B --\u0026gt; C[Mean pool → board_emb] D[prev_move_id] --\u0026gt; E[E_move] F[turn/castle/ep/clk] --\u0026gt; G[E_meta] C --\u0026gt; H[Concat + Project → event_t] E --\u0026gt; H G --\u0026gt; H H --\u0026gt; I[HSTU Backbone causal N layers] I --\u0026gt; J[Policy Head → move logits] I --\u0026gt; K[Value Head → loss/draw/win] Embedding Layers# Each ply is converted to a single event vector by embedding structured fields and concatenating them:\nevent_t = LN(W · concat(board_emb, move_emb_{t-1}, meta_emb))Where:\nboard_emb = mean over (E_piece(piece_ids) + E_square(index)) for all 64 squares move_emb_{t-1} = embedding of the previous move (or START token at ply 1) meta_emb = embeddings of turn, castling rights, en-passant file, clock buckets HSTU Backbone# The backbone is a causal transformer with relative positional bias indexed by ply number. It runs over the event sequence e_1 … e_T and produces hidden states per ply.\nTarget model size: 20–30M parameters. This fits in a single RTX 4090 training run.\nTraining Phase 1: Supervised Policy Pretraining# Objective# Cross-entropy on next legal UCI move:\nloss = CE(logits_masked, target_move_id)BOS positions are excluded via ignore_index = -100.\nElo-Weighted Loss# Not all moves are equally informative. Moves by 2000 Elo players carry less signal than moves by 2600 Elo players. We apply per-token Elo weighting:\nnorm_i = clamp((elo_i - min_elo) / (max_elo - min_elo), 0, 1) w_i = 1 + strength × (norm_i ^ alpha) loss = Σ(w_i × ce_i) / Σ(w_i)Weight normalization keeps gradient scale stable when weighting is on.\nLabel smoothing is also applied to account for move non-uniqueness: strong positions often have multiple valid moves.\nTraining Infrastructure# Optimizer: StableAdamW with OneCycleLR scheduler Precision: bfloat16 mixed precision Checkpointing: best by hr@10 on full val, plus periodic last checkpoints Logging: TensorBoard + periodic fast val/test checks Evaluation Metrics# Offline Metrics (Phase 1)# Evaluated on held-out val/test splits every N steps:\nMetric What it measures loss_ce Cross-entropy on target move ppl Perplexity (exp of loss_ce) top1_acc Argmax move matches human move top3_acc / top5_acc Move in top-3/5 hr@10 Hit rate at 10 (top-10 accuracy) mrr Mean reciprocal rank of ground-truth move Model selection uses hr@10 from full val as the primary signal.\nSlice Metrics# Global averages hide regressions. We also report by:\nGame phase: opening (ply 1–20), middlegame (ply 21–60), endgame (ply 61+) Elo bucket: 2000–2199, 2200–2399, 2400+ Engine Evaluation (Phase 2)# After offline metrics stabilize, the model plays against Stockfish:\nAlternating colors, fixed time controls Ladder evaluation across Elo settings: 1600, 1800, 2000, 2200, 2400, 2600, 2800 Reports: wins/draws/losses, score rate, color split, Elo estimate with confidence intervals Phase 2: Adding a Value Head# Why a Value Head# A policy head alone picks moves based on how likely a human would play them. It does not reason about outcomes. The value head adds a separate prediction: given the current board position, what is the probability of winning, drawing, or losing?\nWithout value at inference time, the model cannot distinguish between:\n\u0026ldquo;This move is popular in human games\u0026rdquo; (policy says yes) \u0026ldquo;This move leads to a winning position\u0026rdquo; (requires value) WDL Classification# The value head is a 3-class classifier from the side-to-move perspective:\nvalue_logits = Linear(d, 3) # [loss, draw, win]Labels are derived from the per-game result (game_result_white ∈ {+1, 0, -1}) and per-token turn_id (to flip perspective for black).\nA scalar value is extracted as:\nV(s) = p(win) - p(loss) ∈ [-1, 1]Progress Weighting# Value labels derived from final game results are noisy — early positions have a weak causal link to who ultimately wins. We downweight early plies and emphasize later ones:\nprogress_weight = (ply_index / total_plies) ^ alphaWith alpha = 1.5, the first few plies contribute little; the endgame contributes most.\nCombined Loss# total_loss = policy_loss + λ × value_lossWe start with λ = 0.15 to protect policy quality while the value head bootstraps. If top1/hr@10 drops sharply, reduce λ further.\nflowchart TD A[HSTU hidden state] --\u0026gt; B[Policy Head] A --\u0026gt; C[Value Head] B --\u0026gt; D[CE loss on next move] C --\u0026gt; E[CE loss on WDL outcome] D --\u0026gt; F[total_loss = policy_loss + 0.15 × value_loss] E --\u0026gt; F Training Schedule# Warm start (optional): freeze backbone for 1k–3k steps, train only heads. Joint training: unfreeze all, keep value_loss_weight low initially. Monitor: if policy metrics drop, reduce value weight. Phase 3: Using Value at Inference# Adding a value head to training only modestly improves playing strength. The real gain comes from using the value during move selection.\nMode 1: Greedy (Baseline)# Pick the highest-logit legal move. Fast, deterministic, no value used.\nMode 2: Sampled Decoding# Sample from top-k / top-p legal moves with temperature. Adds variety, occasionally finds surprising moves, but can also pick blunders.\nMode 3: Value Rerank (1-Ply Lookahead)# Take top-K policy candidates, evaluate each resulting position with the value head, pick the best:\nscore(move) = log π(move | s) - λ × V(next_state)The minus sign: after we move, it is the opponent\u0026rsquo;s turn at next_state, so high opponent value is bad for us.\nflowchart LR A[Current state s] --\u0026gt; B[Policy: top-K legal moves] B --\u0026gt; C[Apply each move → s\u0026#39;] C --\u0026gt; D[Value head on each s\u0026#39;] D --\u0026gt; E[Score = log π - λ V_opp] E --\u0026gt; F[Pick best scoring move] Default settings: K = 8, λ = 0.35.\nMode 4: Depth-2 Policy-Pruned Minimax# Go one level deeper: after our move, simulate the opponent\u0026rsquo;s best response, then pick our move that leads to the best position after that response. This is one-step minimax.\nQ(a) = min_{b ∈ top-K} V(apply(apply(s, a), b)) a* = argmax_a Q(a)The branching factor is controlled by keeping only top-K policy candidates at each ply: K1 candidate moves for us, K2 opponent responses each.\nflowchart TD A[Root state s] --\u0026gt; B[Our top-K1 moves] B --\u0026gt; C[For each candidate a → s\u0026#39;] C --\u0026gt; D[Opponent top-K2 moves] D --\u0026gt; E[For each response b → s\u0026#39;\u0026#39;] E --\u0026gt; F[Value V at s\u0026#39;\u0026#39;] F --\u0026gt; G[Opponent picks b that minimizes V for us] G --\u0026gt; H[We pick a with best worst-case V] Why this matters more than RL early on: most amateur-level losses come from hanging pieces, missing forks, and stepping into mate threats. Depth-2 catches a large fraction of these because it explicitly asks \u0026ldquo;what is my opponent\u0026rsquo;s best immediate reply?\u0026rdquo;\nBatch optimization: instead of calling the transformer node-by-node, batch all K1 × K2 grandchild states into a single forward pass. This can be 10–100× faster on GPU.\nAblation Matrix# To measure what actually moves the needle on Stockfish win rate, we run a controlled comparison:\nConfiguration Description Policy-only + greedy Baseline Policy+value training, greedy decode Does value training help representations? Policy+value training, value-rerank Does 1-ply value improve play? Policy+value training, depth-2 search Does minimax help further? All comparisons use the same Stockfish time controls and opening protocols.\nCurrent Limitations and Known Issues# No legal-move masking in the prediction head yet. Full-vocab classification. The model can in principle output illegal move IDs (tracked separately as legal_top1). Training is single-process (no DDP launcher yet). value_rerank is one-ply only; value_search_d2 is depth-2 and substantially slower than greedy. Value labels are noisy for early plies — progress weighting helps but does not fully solve this. Value head may learn player-strength bias (higher Elo games have more draws): tracked by Elo-diff slices during eval. Planned Next Steps# Self-play RL (Phase 4)\nAfter pretraining stabilizes and value metrics look calibrated, the next stage is RL fine-tuning via self-play:\nEnvironment: gym-chess with parallel rollouts (~1000 workers via pufferlib) Algorithm: PPO or KL-regularized PPO (GRPO-style) Reward: +1 win, 0 draw, −1 loss; optional shaping from engine eval delta League: self-play against current + past checkpoints; optional Stockfish/Leela matches Beam Search\nAnother direction: instead of depth-2 minimax, run beam search over likely continuations. Policy priors guide the beam; value head scores leaf nodes. More compute, potentially better tactical vision.\nScaling\nCurrent target is 20–30M parameters on a single RTX 4090. Interesting questions:\nDoes Elo scale smoothly with model size? Does value search help more at smaller model sizes (where policy alone is weaker)? References# Lichess Open Database python-chess Searchless Chess (DeepMind) grpo_chess PyTorch FlexAttention Mermaid Related Posts# FlexAttention HSTU at 500M Events Machine Learning Engineer Roadmap Closing# The bet here is that structured event modeling — the same pattern that works for sequential recommendation — transfers cleanly to chess. Board state is richer than a user\u0026rsquo;s listening history, but the sequence modeling problem is the same: predict what comes next from what came before.\nThe value head and minimax search are the bridge from imitation to reasoning. Imitation learns the prior; search uses that prior to avoid mistakes.\nWhether that\u0026rsquo;s enough to reach a respectable Elo without full RL self-play is the open question.\n"},{"id":4,"href":"/docs/posts/hstu-for-yambda/","title":"FlexAttention HSTU at 500M Events: RQ Tokens, QR Embeddings, and 1D Biases","section":"Posts","content":"Executive Summary# At Yambda scale (about 500M events and 9.4M items), models rarely break because one idea is bad. They break when many small, expensive defaults pile up.\nThis post is the story of the choices that kept an HSTU-style recommender trainable:\nJagged, block-masked attention with PyTorch FlexAttention Residual quantization (RQ) token prediction instead of a giant item-ID softmax Quotient-remainder (QR) embeddings for large sparse categorical spaces On-the-fly 1D attention bias terms (time, duration, organic) instead of dense [S, S] bias tensors ALiBi positional bias instead of learned position embeddings The throughline is simple: keep the inductive bias, cut the dense and quadratic costs.\nWhy Yambda Forces Architectural Discipline# Four constraints shaped every design decision:\nUser histories are jagged, not fixed-length. Item space is large enough that a full output projection is expensive. Side metadata is sparse and partially missing. Attention biasing must not materialize quadratic tensors. At this scale, architecture is mostly cost control. So we optimized for memory and throughput first, then validated that quality still held.\nArchitecture Overview# flowchart LR A[RQ Codes 8x1024] --\u0026gt; B[Sum RQ Embeddings] C[QR Artist Embedding] --\u0026gt; D[Event Representation] E[Event Type Embedding] --\u0026gt; D B --\u0026gt; D D --\u0026gt; F[FlexAttention HSTU x N] F --\u0026gt; G[L2 Normalize] G --\u0026gt; H[Cascaded RQ Heads] H --\u0026gt; I[Logits S x 8 x 1024] The pivotal move is at the output: we do not model a direct item_id distribution. We model codebooks.\nDecision 1: Replace Item-ID Softmax with RQ Outputs# With ~9.4M items, a direct head looks like:\nLinear(D -\u0026gt; 9,400,000)That is expensive in parameters, optimizer state, and memory bandwidth. Instead, we train an 8-level residual quantizer over item embeddings and predict discrete code indices:\n8 x Linear(D -\u0026gt; 1024)What this changes in practice:\nSmaller output parameterization and optimizer footprint Better fit for ANN retrieval over decoded embeddings Cleaner decomposition into coarse-to-fine prediction Reference: FAISS for vector retrieval.\nDecision 2: Keep Attention Priors, Drop O(S^2) Bias Tensors# A common failure mode is precomputing dense bias matrices for time, position, and feature priors. At longer sequence lengths, that burns memory for very little return.\nIn FlexAttention, we apply score modifiers lazily:\ndef score_mod(score, b, h, q_idx, k_idx): score += alibi_bias(h, q_idx, k_idx) score += time_bias[time_bucket(q_idx, k_idx)] score += duration_bias[duration_bucket(k_idx)] score += organic_bias[is_organic(k_idx)] return scoreThe bias terms are 1D tables and scalar functions. Memory scales with bucket count, not pair count.\nflowchart TD A[Raw QK score] --\u0026gt; B[Add ALiBi] B --\u0026gt; C[Add time bucket bias] C --\u0026gt; D[Add duration bias] D --\u0026gt; E[Add organic bias] E --\u0026gt; F[Final attention score] Decision 3: Use ALiBi for Position Bias# Learned positional embeddings work, but ALiBi fits this setup better:\nNo position-embedding table Relative bias available in every layer Fewer constraints when pushing sequence lengths Reference: Train Short, Test Long: ALiBi.\nDecision 4: Compress Large Categorical Spaces with QR Embeddings# For high-cardinality IDs (for example, artist IDs), we use quotient-remainder factorization:\nembed(id) = embed_q(id // R) + embed_r(id % R)With R = 1024, this substantially reduces table size while preserving enough representational capacity for ranking.\nThis tradeoff is intentional: a modest representation loss is acceptable if it unlocks larger batches and faster iteration.\nHandling Sparse IDs and Missing Content Embeddings# What we observed:\nItem ID space can extend to ~9.4M Only a subset has precomputed content embeddings Missingness is non-trivial (around 18% in this run) What we did:\nKeep a compact ID-to-RQ lookup for known items Route unknown/missing entries to a deterministic all-zero RQ code pattern Let the model learn a stable fallback behavior for unknown content This avoids huge dense lookup tensors and keeps behavior deterministic.\nJagged Batching with Block Masks# All sequence events are concatenated into one token buffer plus offsets. Attention is constrained to legal user-local ranges.\nflowchart LR A[User 1 tokens] --\u0026gt; D[Concatenated token buffer] B[User 2 tokens] --\u0026gt; D C[User 3 tokens] --\u0026gt; D D --\u0026gt; E[Block mask: no cross-user attention] This improves accelerator utilization versus naive per-user padding while preserving exact user boundaries.\nEarly Signal# Step Main metric 32,432 0.0723 64,864 0.1431 97,296 0.1607 Absolute gain: +0.0884 Relative improvement from first logged point: about 2.2x This is still an intermediate training signal. Final offline validation should report Hit@K, MRR, and NDCG on retrieved candidates. For boundary-case debugging during retrieval-stage evaluation, see The Role of Negative Mining in Machine Learning.\nWhat Can Break# The main failure modes are straightforward:\nQuantization bottleneck. If RQ codebooks are underfit, retrieval quality saturates early. Bucket design brittleness. Bad time/duration buckets quietly cap model quality. Missingness leakage. If unknown embeddings correlate with labels, the fallback path can become a shortcut feature. Evaluation mismatch. Improvements in training metric may not transfer to retrieval-stage KPIs. Recommended guardrails:\nRun ablations for each bias term (ALiBi/time/duration/organic) Track calibration and recall at retrieval depth Monitor unknown-code frequency by segment Keep one non-quantized baseline for regression detection For retrieval-quality diagnostics at scale, combine this with Entity Resolution using Contrastive Learning-style candidate analysis. Next Experiments# QK normalization Per-layer residual scaling Logit soft-capping Alternative optimizers (including Muon) Explicit ablations of duration and organic priors References# PyTorch FlexAttention ALiBi paper FAISS Mermaid Related Posts# The Role of Negative Mining in Machine Learning Entity Resolution using Contrastive Learning Machine Learning Engineer Roadmap Closing# At Yambda scale, disciplined defaults win:\nno dense attention bias matrices, no giant item softmax, no full-width categorical tables when compression is enough, no padding-heavy batching. That is the full pattern: preserve signal, strip avoidable cost.\n"},{"id":5,"href":"/docs/posts/indian_salary_distribution/","title":"Distribution for SDE Salaries in India","section":"Posts","content":"Overview# I recently was having discussions with junior engineers about salary expectations for Software Development Engineers (SDEs) in India, especially how it changes with years of experience. Inspired by The Pragmatic Engineer and @deedydas’ tweet, I decided to examine a dataset of Indian salaries.\nMy goal was to parse the data, clean it, cluster salary ranges by experience, and visualize how salaries distribute across different “tiers” of companies.\nMethodology# Data Collection\nParse the raw excel sheet into a pandas dataframe using BeautifulSoup. Each record contains: Relevant Experience (years) Base Salary Variable Bonus Stock Components Data Cleaning\nFiltered out missing or non-sensical values (“NULL” or negative). Grouped records by integer years of experience. Computed totalSalary as base + (bonus + stocks) for those with 4+ years of experience. Removed outliers within each experience group by cutting off the lower 4% and upper 4% of salaries. Categorizing Companies\nFor each experience bracket, compute a tri-modal Gaussian Mixture Model (GMM) to cluster salaries into “Low”, “Medium”, and “High” categories. Plotting and Summaries\nUsing plotnine (a Python port of ggplot2), plot the faceted histogram by years of experience. Added vertical dashed lines showing mean salaries for each cluster within each experience group. Labeled each cluster with the sample size for clarity. Output# "},{"id":6,"href":"/docs/posts/ml_engineer_guidelines/","title":"Machine Learning Engineer Roadmap","section":"Posts","content":"Mastering the Art of AI Development: A Detailed Roadmap from Data to Deployment# Introduction# Developing an effective artificial intelligence (AI) model is akin to embarking on a long, complex journey. It demands expertise in areas ranging from dataset creation and feature engineering to model tuning, evaluation, and deployment. This blog post will walk you through the key stages involved in AI development, explain the importance of each, and provide a clear understanding of the skills required at different levels of expertise, namely Junior, Senior, and Staff Engineer.\n1. Dataset Creation: Building a Solid Foundation# A robust AI model requires a strong foundation, and this begins with creating an appropriate dataset. The cornerstone of a good dataset is relevant data. How do you identify what\u0026rsquo;s relevant? It\u0026rsquo;s about understanding the signal-to-noise ratio, where the \u0026lsquo;signal\u0026rsquo; is the useful information that can answer your research questions, and \u0026rsquo;noise\u0026rsquo; is the irrelevant data that may skew your results.\nA robust dataset is characterized by comprehensive and diverse features. It should also encompass labeled and unlabeled data. While labeled data serves as the ground truth for training the model, unlabeled data, despite being more challenging to work with, can unearth hidden patterns or associations.\nWe must also address potential implicit bias in our dataset. Bias can skew the model\u0026rsquo;s performance and harm its ability to make fair decisions. Careful data collection, rigorous analysis, and bias-correction techniques can help account for it.\n2. Feature Engineering: Turning Raw Data into Meaningful Information# Once we have a dataset, it\u0026rsquo;s time for feature engineering. This process involves selecting the most relevant features and transforming raw data into formats that the model can understand better.\nFeaturization techniques like one-hot encoding, binning, or polynomial features can be employed depending on the nature of your data. The distribution of the dataset also plays a key role in deciding which features to include.\nNormalization is another crucial step to ensure that extreme values or outliers don\u0026rsquo;t distort the model\u0026rsquo;s performance. This depends on the specific distribution of your data and the problem you\u0026rsquo;re trying to solve.\n3. Modeling: Choosing and Improving Your Tool# The modeling stage is where the magic happens. This is where we choose the algorithm that will learn from our data. We begin with a baseline model—a simple technique that sets the minimum performance expectation.\nBaseline models come with their own pros and cons. For example, a linear regression model may be easy to implement and interpret but may not handle complex relationships between features and outcomes well. We need to contextualize these models with our problem at hand.\nNext, we move on to more advanced models. We might opt for neural networks or ensemble methods, depending on the problem. These models need to be fine-tuned to handle bias and adapt to the specific context of the problem. This involves choosing appropriate optimization and loss functions.\n4. Evaluation Measure: Assessing Your Model# Now, we need to assess how our model performs. Depending on the problem, we could use measures like accuracy, precision, recall, or the F1 score for classification problems, or mean squared error, mean absolute error, or R-squared for regression problems.\nThese evaluation measures each have their strengths and limitations, and they assess both extrinsic and intrinsic properties of the model. For instance, accuracy might be a good measure when the classes are balanced, but it would be misleading for imbalanced datasets.\n5. Confidence Scoring and Tuning: Trusting Your Model# Confidence scoring helps us understand how certain our model is about its predictions. A well-calibrated model\u0026rsquo;s confidence aligns well with its accuracy. Both pre-training (like regularization techniques) and post-training methods (like Platt scaling) can help us calibrate our models.\nWhile it\u0026rsquo;s useful in identifying model issues, it\u0026rsquo;s important to remember that a high confidence score doesn\u0026rsquo;t always mean a correct prediction and vice versa.\n6. Inference: Deploying Your Model# Once we\u0026rsquo;re satisfied with the model\u0026rsquo;s performance, we\u0026rsquo;re ready to deploy it. This requires careful planning, from selecting the appropriate hardware to efficiently using the cores and instructions. The choice between different precision formats like int8, fp16, bf16, etc., depends on the trade-off we want to make between speed and accuracy.\nMoreover, understanding concepts like queuing theory, throughput, and latency relationships can help scale models effectively.\n7. Optimizations/Improvements Cycle: Enhancing Your Model# After deployment, our job isn\u0026rsquo;t over. We need to continuously monitor our model\u0026rsquo;s performance and make necessary improvements. This might involve tweaking features, changing the model architecture, or even collecting more data.\n8. Monitoring and Metrics: Keeping an Eye on Your Model# An effective monitoring system is crucial in maintaining the performance of our models. We need to set up alerts for key performance metrics and keep a close eye on these online metrics. Following a systematic MLOps architecture helps manage models better.\n9. User Feedback Pipeline: Learning from Your Users# Incorporating user feedback into our model improvements is vital. We need to be alert to concept drifts, where the relationships between variables change over time, and hard negatives that are consistently misclassified. Understanding the impact of both implicit and explicit feedback can help fine-tune our model further.\nThe Path to AI Mastery: Skills at Different Levels# While a Junior AI engineer should be aware of the entire development pipeline, they are expected to show proficiency (scoring 3) in dataset creation, feature engineering, modeling, evaluation, and inference, with a basic understanding (scoring 2) in other areas.\nA Senior AI engineer should exhibit minimum proficiency in all stages and demonstrate advanced knowledge (scoring 4) in key areas.\nA Staff engineer, the highest level, should have expert-level knowledge and skills (scoring 5) in multiple key areas and be able to demonstrate substantial outcomes.\nBuilding an AI model is a complex task that requires a wide array of skills. But with the right understanding, continuous learning, and constant practice, you can embark on this exciting journey with confidence. Happy modeling!\n"},{"id":7,"href":"/docs/posts/supervised_finetuning/","title":"Supervised Fine-Tuning in Large Language Models","section":"Posts","content":"The Power of Supervised Fine-Tuning in Large Language Models: An In-depth Analysis# Introduction# In recent years, the development of machine learning, particularly large language models (LLMs), has revolutionized the way we approach a multitude of challenges, from query-based tasks to content generation. In this post, we will dive deep into a technique gaining traction within the AI community - supervised fine-tuning using domain-specific instruction datasets - and contrast it with the more conventional prompt tuning approach, with a focus on techniques such as retrieval augmentation.\nWhat is Supervised Fine-Tuning?# Supervised fine-tuning involves adjusting a pre-trained LLM to improve its performance using a specific dataset that contains examples from a targeted domain. For instance, to train an LLM for medical consultation, one might use a dataset comprising medical textbooks, research papers, and patient-doctor interactions. Using instruction-answer-context pairs from social media conversations to build better contextual assistants.\nForming the Dataset# Creating an effective dataset for supervised fine-tuning is a nuanced process. The dataset must be a balanced representation of the domain you\u0026rsquo;re aiming to specialize in, so it\u0026rsquo;s vital to include diverse and contextually rich information sources. Privacy and data ethics are of paramount concern during the data collection process.\nAdvantages of Supervised Fine-Tuning# Domain Specificity: Supervised fine-tuning allows the model to be customized to a particular domain, resulting in more accurate and contextually relevant outputs. Better Generalization: A fine-tuned model can generalize better to new data within the same domain, as it has learned the specific patterns and nuances of the field. Efficient Usage of Parameters: Fine-tuning allows the vast parameter space of LLMs to be effectively utilized for domain-specific tasks, leading to parameter-efficient fine-tuning. Limitations of Supervised Fine-Tuning# Dataset Quality: The success of supervised fine-tuning largely hinges on the quality of the dataset. Poorly curated or biased datasets can lead to subpar or skewed results. Overfitting: The model can overfit to the training data, leading to less than optimal performance on unseen data. Resource-Intensive: Fine-tuning requires significant computational resources, making it more expensive than some other methods. Comparison with Prompt Tuning# Prompt tuning, by contrast, employs a more straightforward approach, guiding the LLM to generate desired responses using specifically crafted prompts. While this method is simpler and less resource-intensive, it lacks the domain specificity and generalization capabilities offered by supervised fine-tuning.\nRetrieval Augmentation# One method commonly used in prompt tuning is retrieval augmentation, where the model is trained to pull in relevant external information to enhance its responses. While this can lead to more informative replies, the quality of the output still largely depends on the relevancy and accuracy of the external data sourced, which can be a hit-or-miss.\nParameter-Efficient Fine-Tuning# Parameter-efficient fine-tuning refers to the idea of making the best use of the available parameters in a model during the fine-tuning process. With supervised fine-tuning, this can be achieved by selectively updating parameters that contribute most to the target domain, thereby improving the model\u0026rsquo;s performance while keeping computational costs in check.\nConclusion# Both supervised fine-tuning and prompt tuning have their place in the world of large language models. The choice between the two often depends on the specific requirements of the task at hand, the resources available, and the complexity of the domain. In tasks where domain-specific accuracy and robust generalization are of paramount importance, supervised fine-tuning with a well-curated instruction dataset appears to hold the edge. The resource-intensiveness and potential overfitting risks associated with it, however, call for careful implementation and ongoing evaluation. As the field evolves, the development of even more efficient and effective tuning techniques will undoubtedly continue.\n"},{"id":8,"href":"/docs/posts/hard_negatives/","title":"The Role of Negative Mining in Machine Learning: Bridging the Gap in Model Performance","section":"Posts","content":"Introduction# Machine learning models are excellent tools for making predictions or classifications. However, they\u0026rsquo;re not infallible; occasionally, they may make mistakes. Some of the most enlightening mistakes are the so-called \u0026ldquo;hard negatives\u0026rdquo; — instances where the model confidently produces the incorrect output. Understanding and learning from these instances through hard negative mining can significantly improve the model\u0026rsquo;s performance.\nUnderstanding Hard Negative Mining# In machine learning, \u0026ldquo;hard negatives\u0026rdquo; refer to examples that are challenging for the model to classify correctly. They are the negatives that the model most often misclassifies. Hard negative mining is a strategy for improving the performance of a model by focusing on these difficult-to-classify instances.\nWhy is it Important to Measure Performance in the Threshold Region?# The threshold region is where the model makes its most decisive judgments. It is in this region that we identify the hard negatives. By focusing on the threshold region, we can specifically diagnose where the model struggles and concentrate our efforts to improve those areas.\nHow to Identify Hard Negatives# Typically, hard negatives are identified by observing the model\u0026rsquo;s performance in real-world scenarios. Debugging these cases can often lead to revealing insights about the model\u0026rsquo;s shortcomings. During this process, it\u0026rsquo;s crucial to analyze the model outputs concerning the features, thereby identifying potential missing properties in the feature set that lead to incorrect predictions.\nTraining Strategies to Address Hard Negatives# Once the hard negatives have been identified and analyzed, the next step is to use this information to improve the model. This might involve:\nExpanding the training set: Incorporating more examples of hard negatives into the training set can improve the model\u0026rsquo;s ability to correctly classify these cases in the future. Fine-tuning the model: Sometimes, it may not be necessary to retrain the entire model. Instead, you could fine-tune the model on the hard negatives, enabling it to learn from its mistakes without needing to revisit all the previous training data. Revising the features: If the hard negatives are a result of inadequate or poor features, consider revising the feature set. This could involve engineering new features or improving the quality of existing ones. Conclusion# Hard negative mining is a powerful technique for improving the performance of machine learning models. By focusing on the hardest examples, we can refine our models to become more robust and accurate. The insights gained from studying these difficult cases can also help us improve our features and make our models even more effective.\n"},{"id":9,"href":"/docs/posts/entity_resolution/","title":"Entity Resolution using Contrastive Learning","section":"Posts","content":"Introduction to Entity Resolution# Entity resolution (also known as entity matching, record linkage, or duplicate detection) is the task of finding records that refer to the same real-world entity across different data sources (e.g., data files, books, websites, and databases).\nThis can be a challenging task, especially when the dataset is large and the queries mention the attributes of the entities in various ways, such as with partial information, typing errors, abbreviations, or extra information. In this blog post, we\u0026rsquo;ll be discussing how to approach the Entity Resolution Problem and the solution that was implemented to solve it.\nProblem Definition# Imagine you have a dataset of approximately 50 million entities, and your task is to find the right entity for a given query. The query could be a few of the entity\u0026rsquo;s attributes, and these queries could mention the attributes in various ways. This is the Entity Resolution Problem.\nThe Existing Solution# One solution to this problem is an Elastic search-based match, which uses complicated heuristics that are overfitted on a small training set. However, this solution is not scalable and the accuracy of the top-20 search retrieval decreases exponentially as the number of entities increases.\nAt the time this problem was being addressed, the top-20 search retrieval accuracy was around 40% for the current number of entities.\nThe Implemented Solution# To solve the Entity Resolution Problem, an embedding search was implemented using a Sentence embedding model. The Deberta model was pretrained and fine-tuned for the current problem using contrastive learning. In contrastive learning, positive pairs are generated using augmentations for each attribute that best mock the queries, based on the many user queries received.\nCustom augmentations which syntheically generate query like variations were used during training time to help the model learn generate positive similarity score for entity, query pair.\nResults# With this solution, the top-20 accuracy was around 98%. Heuristics and other business logic, along with a properly calculated confidence measure (which was hyperparameter-tuned on the validation set), were used to filter out the right entity. After the final pipeline was implemented, a top-1 accuracy of around 99.995% (precision) and 86% (recall) was achieved for high confidence matches.\nIn the end, pinecone was chosen for the embedding search and the search latency was around 100ms for the top 50 among the 50 million embeddings.\nConclusion# To conclude, the Entity Resolution Problem was successfully solved by implementing an embedding search using a Sentence embedding model and fine-tuning it with contrastive learning. This solution had a significantly higher accuracy compared to the existing Elastic search-based solution and was able to scale well as the number of entities increased.\n"}]