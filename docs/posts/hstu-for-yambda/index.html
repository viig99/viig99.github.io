<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="How we scaled jagged sequential recommendation on Yambda-scale data with FlexAttention, residual quantization outputs, quotient-remainder embeddings, and on-the-fly 1D attention biases."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://viig99.github.io/docs/posts/hstu-for-yambda/"><meta property="og:site_name" content="/home/vigi99"><meta property="og:title" content="FlexAttention HSTU at 500M Events: RQ Tokens, QR Embeddings, and 1D Biases"><meta property="og:description" content="How we scaled jagged sequential recommendation on Yambda-scale data with FlexAttention, residual quantization outputs, quotient-remainder embeddings, and on-the-fly 1D attention biases."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:published_time" content="2026-02-15T00:00:00+00:00"><meta property="article:modified_time" content="2026-02-15T18:41:14-05:00"><meta property="article:tag" content="Recommender Systems"><meta property="article:tag" content="HSTU"><meta property="article:tag" content="FlexAttention"><meta property="article:tag" content="Residual Quantization"><meta property="article:tag" content="Quotient Remainder Embedding"><meta property="article:tag" content="Sequential Recommendation"><meta itemprop=name content="FlexAttention HSTU at 500M Events: RQ Tokens, QR Embeddings, and 1D Biases"><meta itemprop=description content="How we scaled jagged sequential recommendation on Yambda-scale data with FlexAttention, residual quantization outputs, quotient-remainder embeddings, and on-the-fly 1D attention biases."><meta itemprop=datePublished content="2026-02-15T00:00:00+00:00"><meta itemprop=dateModified content="2026-02-15T18:41:14-05:00"><meta itemprop=wordCount content="897"><meta itemprop=keywords content="Recommender Systems,HSTU,FlexAttention,Residual Quantization,Quotient Remainder Embedding,Sequential Recommendation,Yambda,Large-Scale ML"><title>FlexAttention HSTU at 500M Events: RQ Tokens, QR Embeddings, and 1D Biases | /home/vigi99</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://viig99.github.io/docs/posts/hstu-for-yambda/><link rel=stylesheet href=/book.min.cc2c524ed250aac81b23d1f4af87344917b325208841feca0968fe450f570575.css integrity="sha256-zCxSTtJQqsgbI9H0r4c0SRezJSCIQf7KCWj+RQ9XBXU=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.e875f5448736d4963f367e5905e7353ced203bcfb1c0fe15c59391052b684305.js integrity="sha256-6HX1RIc21JY/Nn5ZBec1PO0gO8+xwP4VxZORBStoQwU=" crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-55K4J76G9F"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-55K4J76G9F")}</script></head><body dir=ltr class="book-kind-page book-type-docs"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>/home/vigi99</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><a href=/docs/work-experience/>Work Experience</a><ul></ul></li><li><a href=/docs/open-source/>Open Source Contributions</a><ul></ul></li><li><a href=/docs/machine-learning/>Machine Learning Toolkit</a><ul></ul></li><li class=book-section-flat><a>Posts</a><ul><li><a href=/docs/posts/imba-chess/>Building a Chess Bot with HSTU: From Lichess Pretraining to Value Search</a></li><li><a href=/docs/posts/hstu-for-yambda/ class=active>FlexAttention HSTU at 500M Events: RQ Tokens, QR Embeddings, and 1D Biases</a></li><li><a href=/docs/posts/indian_salary_distribution/>Distribution for SDE Salaries in India</a></li><li><a href=/docs/posts/ml_engineer_guidelines/>Machine Learning Engineer Roadmap</a></li><li><a href=/docs/posts/supervised_finetuning/>Supervised Fine-Tuning in Large Language Models</a></li><li><a href=/docs/posts/hard_negatives/>The Role of Negative Mining in Machine Learning: Bridging the Gap in Model Performance</a></li><li><a href=/docs/posts/entity_resolution/>Entity Resolution using Contrastive Learning</a></li></ul></li></ul><ul><li><a href="mailto:accio.arjun@gmail.com?subject=Job%20Opportunity" target=_blank rel=noopener>Contact Me<span style=line-height:1em;vertical-align:middle><svg viewBox="0 0 512 512" style="height:1em;width:1em"><path fill="#ea4335" opacity="1" d="M0 128C0 92.65 28.65 64 64 64H448c35.3.0 64 28.65 64 64V384c0 35.3-28.7 64-64 64H64c-35.35.0-64-28.7-64-64V128zm48 0v22.1L220.5 291.7c20.6 17 50.4 17 71 0L464 150.1v-23C464 119.2 456.8 111.1 448 111.1H64C55.16 111.1 48 119.2 48 127.1V128zm0 84.2V384C48 392.8 55.16 4e2 64 4e2H448C456.8 4e2 464 392.8 464 384V212.2L322 328.8c-38.4 31.5-93.6 31.5-132.9.0L48 212.2z"/></svg></span></a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=/icons/menu.svg class=book-icon alt=Menu></label><h3>FlexAttention HSTU at 500M Events: RQ Tokens, QR Embeddings, and 1D Biases</h3><label for=toc-control><img src=/icons/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class=hidden><nav id=TableOfContents><ul><li><ul><li><a href=#executive-summary>Executive Summary</a></li><li><a href=#why-yambda-forces-architectural-discipline>Why Yambda Forces Architectural Discipline</a></li><li><a href=#architecture-overview>Architecture Overview</a></li><li><a href=#decision-1-replace-item-id-softmax-with-rq-outputs>Decision 1: Replace Item-ID Softmax with RQ Outputs</a></li><li><a href=#decision-2-keep-attention-priors-drop-os2-bias-tensors>Decision 2: Keep Attention Priors, Drop O(S^2) Bias Tensors</a></li><li><a href=#decision-3-use-alibi-for-position-bias>Decision 3: Use ALiBi for Position Bias</a></li><li><a href=#decision-4-compress-large-categorical-spaces-with-qr-embeddings>Decision 4: Compress Large Categorical Spaces with QR Embeddings</a></li><li><a href=#handling-sparse-ids-and-missing-content-embeddings>Handling Sparse IDs and Missing Content Embeddings</a></li><li><a href=#jagged-batching-with-block-masks>Jagged Batching with Block Masks</a></li><li><a href=#early-signal>Early Signal</a></li><li><a href=#what-can-break>What Can Break</a></li><li><a href=#next-experiments>Next Experiments</a></li><li><a href=#references>References</a></li><li><a href=#related-posts>Related Posts</a></li><li><a href=#closing>Closing</a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h2 id=executive-summary>Executive Summary<a class=anchor href=#executive-summary>#</a></h2><p>At Yambda scale (about 500M events and 9.4M items), models rarely break because one idea is bad. They break when many small, expensive defaults pile up.</p><p>This post is the story of the choices that kept an HSTU-style recommender trainable:</p><ul><li>Jagged, block-masked attention with <a href=https://pytorch.org/docs/stable/nn.attention.flex_attention.html target=_blank rel=noopener>PyTorch FlexAttention</a></li><li>Residual quantization (RQ) token prediction instead of a giant item-ID softmax</li><li>Quotient-remainder (QR) embeddings for large sparse categorical spaces</li><li>On-the-fly 1D attention bias terms (time, duration, organic) instead of dense <code>[S, S]</code> bias tensors</li><li><a href=https://arxiv.org/abs/2108.12409 target=_blank rel=noopener>ALiBi</a> positional bias instead of learned position embeddings</li></ul><p>The throughline is simple: keep the inductive bias, cut the dense and quadratic costs.</p><h2 id=why-yambda-forces-architectural-discipline>Why Yambda Forces Architectural Discipline<a class=anchor href=#why-yambda-forces-architectural-discipline>#</a></h2><p>Four constraints shaped every design decision:</p><ol><li>User histories are jagged, not fixed-length.</li><li>Item space is large enough that a full output projection is expensive.</li><li>Side metadata is sparse and partially missing.</li><li>Attention biasing must not materialize quadratic tensors.</li></ol><p>At this scale, architecture is mostly cost control. So we optimized for memory and throughput first, then validated that quality still held.</p><h2 id=architecture-overview>Architecture Overview<a class=anchor href=#architecture-overview>#</a></h2><pre class=mermaid>
flowchart LR
    A[RQ Codes 8x1024] --&gt; B[Sum RQ Embeddings]
    C[QR Artist Embedding] --&gt; D[Event Representation]
    E[Event Type Embedding] --&gt; D
    B --&gt; D
    D --&gt; F[FlexAttention HSTU x N]
    F --&gt; G[L2 Normalize]
    G --&gt; H[Cascaded RQ Heads]
    H --&gt; I[Logits S x 8 x 1024]
</pre><script src=/mermaid.min.js onload='mermaid.initialize({flowchart:{useMaxWidth:!0},theme:"default"})'></script><p>The pivotal move is at the output: we do not model a direct <code>item_id</code> distribution. We model codebooks.</p><h2 id=decision-1-replace-item-id-softmax-with-rq-outputs>Decision 1: Replace Item-ID Softmax with RQ Outputs<a class=anchor href=#decision-1-replace-item-id-softmax-with-rq-outputs>#</a></h2><p>With ~9.4M items, a direct head looks like:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-text data-lang=text><span style=display:flex><span>Linear(D -&gt; 9,400,000)</span></span></code></pre></div><p>That is expensive in parameters, optimizer state, and memory bandwidth. Instead, we train an 8-level residual quantizer over item embeddings and predict discrete code indices:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-text data-lang=text><span style=display:flex><span>8 x Linear(D -&gt; 1024)</span></span></code></pre></div><p>What this changes in practice:</p><ul><li>Smaller output parameterization and optimizer footprint</li><li>Better fit for ANN retrieval over decoded embeddings</li><li>Cleaner decomposition into coarse-to-fine prediction</li></ul><p>Reference: <a href=https://github.com/facebookresearch/faiss target=_blank rel=noopener>FAISS</a> for vector retrieval.</p><h2 id=decision-2-keep-attention-priors-drop-os2-bias-tensors>Decision 2: Keep Attention Priors, Drop O(S^2) Bias Tensors<a class=anchor href=#decision-2-keep-attention-priors-drop-os2-bias-tensors>#</a></h2><p>A common failure mode is precomputing dense bias matrices for time, position, and feature priors. At longer sequence lengths, that burns memory for very little return.</p><p>In FlexAttention, we apply score modifiers lazily:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>score_mod</span>(score, b, h, q_idx, k_idx):
</span></span><span style=display:flex><span>    score <span style=color:#f92672>+=</span> alibi_bias(h, q_idx, k_idx)
</span></span><span style=display:flex><span>    score <span style=color:#f92672>+=</span> time_bias[time_bucket(q_idx, k_idx)]
</span></span><span style=display:flex><span>    score <span style=color:#f92672>+=</span> duration_bias[duration_bucket(k_idx)]
</span></span><span style=display:flex><span>    score <span style=color:#f92672>+=</span> organic_bias[is_organic(k_idx)]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> score</span></span></code></pre></div><p>The bias terms are 1D tables and scalar functions. Memory scales with bucket count, not pair count.</p><pre class=mermaid>
flowchart TD
    A[Raw QK score] --&gt; B[Add ALiBi]
    B --&gt; C[Add time bucket bias]
    C --&gt; D[Add duration bias]
    D --&gt; E[Add organic bias]
    E --&gt; F[Final attention score]
</pre><h2 id=decision-3-use-alibi-for-position-bias>Decision 3: Use ALiBi for Position Bias<a class=anchor href=#decision-3-use-alibi-for-position-bias>#</a></h2><p>Learned positional embeddings work, but ALiBi fits this setup better:</p><ul><li>No position-embedding table</li><li>Relative bias available in every layer</li><li>Fewer constraints when pushing sequence lengths</li></ul><p>Reference: <a href=https://arxiv.org/abs/2108.12409 target=_blank rel=noopener>Train Short, Test Long: ALiBi</a>.</p><h2 id=decision-4-compress-large-categorical-spaces-with-qr-embeddings>Decision 4: Compress Large Categorical Spaces with QR Embeddings<a class=anchor href=#decision-4-compress-large-categorical-spaces-with-qr-embeddings>#</a></h2><p>For high-cardinality IDs (for example, artist IDs), we use quotient-remainder factorization:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-text data-lang=text><span style=display:flex><span>embed(id) = embed_q(id // R) + embed_r(id % R)</span></span></code></pre></div><p>With <code>R = 1024</code>, this substantially reduces table size while preserving enough representational capacity for ranking.</p><p>This tradeoff is intentional: a modest representation loss is acceptable if it unlocks larger batches and faster iteration.</p><h2 id=handling-sparse-ids-and-missing-content-embeddings>Handling Sparse IDs and Missing Content Embeddings<a class=anchor href=#handling-sparse-ids-and-missing-content-embeddings>#</a></h2><p>What we observed:</p><ul><li>Item ID space can extend to ~9.4M</li><li>Only a subset has precomputed content embeddings</li><li>Missingness is non-trivial (around 18% in this run)</li></ul><p>What we did:</p><ul><li>Keep a compact ID-to-RQ lookup for known items</li><li>Route unknown/missing entries to a deterministic all-zero RQ code pattern</li><li>Let the model learn a stable fallback behavior for unknown content</li></ul><p>This avoids huge dense lookup tensors and keeps behavior deterministic.</p><h2 id=jagged-batching-with-block-masks>Jagged Batching with Block Masks<a class=anchor href=#jagged-batching-with-block-masks>#</a></h2><p>All sequence events are concatenated into one token buffer plus offsets. Attention is constrained to legal user-local ranges.</p><pre class=mermaid>
flowchart LR
    A[User 1 tokens] --&gt; D[Concatenated token buffer]
    B[User 2 tokens] --&gt; D
    C[User 3 tokens] --&gt; D
    D --&gt; E[Block mask: no cross-user attention]
</pre><p>This improves accelerator utilization versus naive per-user padding while preserving exact user boundaries.</p><h2 id=early-signal>Early Signal<a class=anchor href=#early-signal>#</a></h2><table><thead><tr><th>Step</th><th>Main metric</th></tr></thead><tbody><tr><td>32,432</td><td>0.0723</td></tr><tr><td>64,864</td><td>0.1431</td></tr><tr><td>97,296</td><td><strong>0.1607</strong></td></tr></tbody></table><ul><li>Absolute gain: <code>+0.0884</code></li><li>Relative improvement from first logged point: about <code>2.2x</code></li></ul><p>This is still an intermediate training signal. Final offline validation should report <code>Hit@K</code>, <code>MRR</code>, and <code>NDCG</code> on retrieved candidates.
For boundary-case debugging during retrieval-stage evaluation, see <a href=./hard_negatives.md>The Role of Negative Mining in Machine Learning</a>.</p><h2 id=what-can-break>What Can Break<a class=anchor href=#what-can-break>#</a></h2><p>The main failure modes are straightforward:</p><ol><li>Quantization bottleneck. If RQ codebooks are underfit, retrieval quality saturates early.</li><li>Bucket design brittleness. Bad time/duration buckets quietly cap model quality.</li><li>Missingness leakage. If unknown embeddings correlate with labels, the fallback path can become a shortcut feature.</li><li>Evaluation mismatch. Improvements in training metric may not transfer to retrieval-stage KPIs.</li></ol><p>Recommended guardrails:</p><ul><li>Run ablations for each bias term (ALiBi/time/duration/organic)</li><li>Track calibration and recall at retrieval depth</li><li>Monitor unknown-code frequency by segment</li><li>Keep one non-quantized baseline for regression detection</li><li>For retrieval-quality diagnostics at scale, combine this with <a href=./entity_resolution.md>Entity Resolution using Contrastive Learning</a>-style candidate analysis.</li></ul><h2 id=next-experiments>Next Experiments<a class=anchor href=#next-experiments>#</a></h2><ul><li>QK normalization</li><li>Per-layer residual scaling</li><li>Logit soft-capping</li><li>Alternative optimizers (including <a href=https://github.com/KellerJordan/Muon target=_blank rel=noopener>Muon</a>)</li><li>Explicit ablations of duration and organic priors</li></ul><h2 id=references>References<a class=anchor href=#references>#</a></h2><ul><li><a href=https://pytorch.org/docs/stable/nn.attention.flex_attention.html target=_blank rel=noopener>PyTorch FlexAttention</a></li><li><a href=https://arxiv.org/abs/2108.12409 target=_blank rel=noopener>ALiBi paper</a></li><li><a href=https://github.com/facebookresearch/faiss target=_blank rel=noopener>FAISS</a></li><li><a href=https://mermaid.js.org/ target=_blank rel=noopener>Mermaid</a></li></ul><h2 id=related-posts>Related Posts<a class=anchor href=#related-posts>#</a></h2><ul><li><a href=./hard_negatives.md>The Role of Negative Mining in Machine Learning</a></li><li><a href=./entity_resolution.md>Entity Resolution using Contrastive Learning</a></li><li><a href=./ml_engineer_guidelines.md>Machine Learning Engineer Roadmap</a></li></ul><h2 id=closing>Closing<a class=anchor href=#closing>#</a></h2><p>At Yambda scale, disciplined defaults win:</p><ul><li>no dense attention bias matrices,</li><li>no giant item softmax,</li><li>no full-width categorical tables when compression is enough,</li><li>no padding-heavy batching.</li></ul><p>That is the full pattern: preserve signal, strip avoidable cost.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=/docs/posts/imba-chess/ class="flex align-center"><img src=/icons/backward.svg class=book-icon alt=Backward>
<span>Building a Chess Bot with HSTU: From Lichess Pretraining to Value Search</span>
</a></span><span><a href=/docs/posts/indian_salary_distribution/ class="flex align-center"><span>Distribution for SDE Salaries in India</span>
<img src=/icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><ul><li><a href=#executive-summary>Executive Summary</a></li><li><a href=#why-yambda-forces-architectural-discipline>Why Yambda Forces Architectural Discipline</a></li><li><a href=#architecture-overview>Architecture Overview</a></li><li><a href=#decision-1-replace-item-id-softmax-with-rq-outputs>Decision 1: Replace Item-ID Softmax with RQ Outputs</a></li><li><a href=#decision-2-keep-attention-priors-drop-os2-bias-tensors>Decision 2: Keep Attention Priors, Drop O(S^2) Bias Tensors</a></li><li><a href=#decision-3-use-alibi-for-position-bias>Decision 3: Use ALiBi for Position Bias</a></li><li><a href=#decision-4-compress-large-categorical-spaces-with-qr-embeddings>Decision 4: Compress Large Categorical Spaces with QR Embeddings</a></li><li><a href=#handling-sparse-ids-and-missing-content-embeddings>Handling Sparse IDs and Missing Content Embeddings</a></li><li><a href=#jagged-batching-with-block-masks>Jagged Batching with Block Masks</a></li><li><a href=#early-signal>Early Signal</a></li><li><a href=#what-can-break>What Can Break</a></li><li><a href=#next-experiments>Next Experiments</a></li><li><a href=#references>References</a></li><li><a href=#related-posts>Related Posts</a></li><li><a href=#closing>Closing</a></li></ul></li></ul></nav></div></aside></main></body></html>