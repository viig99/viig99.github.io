<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="How we scaled jagged sequential recommendation on Yambda-scale data with FlexAttention, residual quantization outputs, quotient-remainder embeddings, and on-the-fly 1D attention biases."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://viig99.github.io/docs/posts/hstu-for-yambda/"><meta property="og:site_name" content="/home/vigi99"><meta property="og:title" content="FlexAttention HSTU at 500M Events: RQ Tokens, QR Embeddings, and 1D Biases"><meta property="og:description" content="How we scaled jagged sequential recommendation on Yambda-scale data with FlexAttention, residual quantization outputs, quotient-remainder embeddings, and on-the-fly 1D attention biases."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:published_time" content="2026-02-15T00:00:00+00:00"><meta property="article:modified_time" content="2026-02-15T18:13:08-05:00"><meta property="article:tag" content="Recommender Systems"><meta property="article:tag" content="HSTU"><meta property="article:tag" content="FlexAttention"><meta property="article:tag" content="Residual Quantization"><meta property="article:tag" content="Quotient Remainder Embedding"><meta property="article:tag" content="Sequential Recommendation"><title>FlexAttention HSTU at 500M Events: RQ Tokens, QR Embeddings, and 1D Biases | /home/vigi99</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://viig99.github.io/docs/posts/hstu-for-yambda/><link rel=stylesheet href=/book.min.3257a7ce7c061eb5dfc72257c356229b1470dc77ce9b982ac220c02542f9f04f.css integrity="sha256-MlenznwGHrXfxyJXw1YimxRw3HfOm5gqwiDAJUL58E8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.de230d434394154b6d69b376844da512b268a591a3919d462acacbd5488906cd.js integrity="sha256-3iMNQ0OUFUttabN2hE2lErJopZGjkZ1GKsrL1UiJBs0=" crossorigin=anonymous></script><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-55K4J76G9F"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-55K4J76G9F")}</script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>/home/vigi99</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><a href=/docs/work-experience/>Work Experience</a><ul></ul></li><li><a href=/docs/open-source/>Open Source Contributions</a><ul></ul></li><li><a href=/docs/machine-learning/>Machine Learning Toolkit</a><ul></ul></li><li class=book-section-flat><span>Posts</span><ul><li><a href=/docs/posts/hstu-for-yambda/ class=active>FlexAttention HSTU at 500M Events: RQ Tokens, QR Embeddings, and 1D Biases</a></li><li><a href=/docs/posts/indian_salary_distribution/>Distribution for SDE Salaries in India</a></li><li><a href=/docs/posts/ml_engineer_guidelines/>Machine Learning Engineer Roadmap</a></li><li><a href=/docs/posts/supervised_finetuning/>Supervised Fine-Tuning in Large Language Models</a></li><li><a href=/docs/posts/hard_negatives/>The Role of Negative Mining in Machine Learning: Bridging the Gap in Model Performance</a></li><li><a href=/docs/posts/entity_resolution/>Entity Resolution using Contrastive Learning</a></li></ul></li></ul><ul><li><a href="mailto:accio.arjun@gmail.com?subject=Job%20Opportunity" target=_blank rel=noopener>Contact Me
<span style=line-height:1em;vertical-align:middle><svg viewBox="0 0 512 512" style="height:1em;width:1em"><path fill="#ea4335" opacity="1" d="M0 128C0 92.65 28.65 64 64 64H448c35.3.0 64 28.65 64 64V384c0 35.3-28.7 64-64 64H64c-35.35.0-64-28.7-64-64V128zm48 0v22.1L220.5 291.7c20.6 17 50.4 17 71 0L464 150.1v-23C464 119.2 456.8 111.1 448 111.1H64C55.16 111.1 48 119.2 48 127.1V128zm0 84.2V384C48 392.8 55.16 4e2 64 4e2H448C456.8 4e2 464 392.8 464 384V212.2L322 328.8c-38.4 31.5-93.6 31.5-132.9.0L48 212.2z"/></svg></span></a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>FlexAttention HSTU at 500M Events: RQ Tokens, QR Embeddings, and 1D Biases</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><ul><li><a href=#executive-summary>Executive Summary</a></li><li><a href=#system-constraints>System Constraints</a></li><li><a href=#architecture-overview>Architecture Overview</a></li><li><a href=#why-rq-outputs-instead-of-item-id-softmax>Why RQ Outputs Instead of Item-ID Softmax</a></li><li><a href=#avoiding-os2-bias-materialization>Avoiding O(S^2) Bias Materialization</a></li><li><a href=#why-alibi-here>Why ALiBi Here</a></li><li><a href=#qr-embeddings-for-large-categorical-spaces>QR Embeddings for Large Categorical Spaces</a></li><li><a href=#sparse-ids-and-missing-content-embeddings>Sparse IDs and Missing Content Embeddings</a></li><li><a href=#jagged-batching-with-block-masks>Jagged Batching with Block Masks</a></li><li><a href=#early-signal>Early Signal</a></li><li><a href=#tradeoffs-and-failure-modes>Tradeoffs and Failure Modes</a></li><li><a href=#next-experiments>Next Experiments</a></li><li><a href=#references>References</a></li><li><a href=#related-posts>Related Posts</a></li><li><a href=#closing>Closing</a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h2 id=executive-summary>Executive Summary
<a class=anchor href=#executive-summary>#</a></h2><p>This post documents the architecture choices that made an HSTU-style recommender tractable at Yambda scale (roughly 500M events and 9.4M item IDs):</p><ul><li>Jagged, block-masked attention with <a href=https://pytorch.org/docs/stable/nn.attention.flex_attention.html target=_blank rel=noopener>PyTorch FlexAttention</a></li><li>Residual quantization (RQ) token prediction instead of a giant item-ID softmax</li><li>Quotient-remainder (QR) embeddings for large sparse categorical spaces</li><li>On-the-fly 1D attention bias terms (time, duration, organic) instead of dense <code>[S, S]</code> bias tensors</li><li><a href=https://arxiv.org/abs/2108.12409 target=_blank rel=noopener>ALiBi</a> positional bias instead of learned position embeddings</li></ul><p>The core pattern is simple: preserve useful inductive bias, but make every scaling decision memory-aware.</p><h2 id=system-constraints>System Constraints
<a class=anchor href=#system-constraints>#</a></h2><p>The design was driven by four hard constraints:</p><ol><li>User histories are jagged, not fixed-length.</li><li>Item space is large enough that a full output projection is expensive.</li><li>Side metadata is sparse and partially missing.</li><li>Attention biasing must not materialize quadratic tensors.</li></ol><p>Those constraints forced us to optimize for throughput and memory first, while keeping model quality stable.</p><h2 id=architecture-overview>Architecture Overview
<a class=anchor href=#architecture-overview>#</a></h2><script src=/mermaid.min.js></script><script>mermaid.initialize({flowchart:{useMaxWidth:!0},theme:"default"})</script><pre class=mermaid>
flowchart LR
    A[RQ Codes 8x1024] --> B[Sum RQ Embeddings]
    C[QR Artist Embedding] --> D[Event Representation]
    E[Event Type Embedding] --> D
    B --> D
    D --> F[FlexAttention HSTU x N]
    F --> G[L2 Normalize]
    G --> H[Cascaded RQ Heads]
    H --> I[Logits S x 8 x 1024]
</pre><p>Key point: we do not model a direct <code>item_id</code> distribution at the output layer. We model codebooks.</p><h2 id=why-rq-outputs-instead-of-item-id-softmax>Why RQ Outputs Instead of Item-ID Softmax
<a class=anchor href=#why-rq-outputs-instead-of-item-id-softmax>#</a></h2><p>With ~9.4M items, a direct head looks like:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-text data-lang=text><span style=display:flex><span>Linear(D -&gt; 9,400,000)
</span></span></code></pre></div><p>That is expensive in memory bandwidth and optimizer state. Instead, we train an 8-level residual quantizer over item embeddings and predict discrete code indices:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-text data-lang=text><span style=display:flex><span>8 x Linear(D -&gt; 1024)
</span></span></code></pre></div><p>Benefits:</p><ul><li>Smaller output parameterization and optimizer footprint</li><li>Better fit for ANN retrieval over decoded embeddings</li><li>Cleaner decomposition into coarse-to-fine prediction</li></ul><p>Reference: <a href=https://github.com/facebookresearch/faiss target=_blank rel=noopener>FAISS</a> for vector retrieval.</p><h2 id=avoiding-os2-bias-materialization>Avoiding O(S^2) Bias Materialization
<a class=anchor href=#avoiding-os2-bias-materialization>#</a></h2><p>A common failure mode is building dense attention bias matrices for time, position, or feature-specific priors. At long sequence lengths this becomes unnecessary overhead.</p><p>In FlexAttention, we apply score modifiers lazily:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>score_mod</span>(score, b, h, q_idx, k_idx):
</span></span><span style=display:flex><span>    score <span style=color:#f92672>+=</span> alibi_bias(h, q_idx, k_idx)
</span></span><span style=display:flex><span>    score <span style=color:#f92672>+=</span> time_bias[time_bucket(q_idx, k_idx)]
</span></span><span style=display:flex><span>    score <span style=color:#f92672>+=</span> duration_bias[duration_bucket(k_idx)]
</span></span><span style=display:flex><span>    score <span style=color:#f92672>+=</span> organic_bias[is_organic(k_idx)]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> score
</span></span></code></pre></div><p>The bias terms are 1D tables and scalar functions. Memory scales with bucket count, not pair count.</p><pre class=mermaid>
flowchart TD
    A[Raw QK score] --> B[Add ALiBi]
    B --> C[Add time bucket bias]
    C --> D[Add duration bias]
    D --> E[Add organic bias]
    E --> F[Final attention score]
</pre><h2 id=why-alibi-here>Why ALiBi Here
<a class=anchor href=#why-alibi-here>#</a></h2><p>Learned positional embeddings are workable, but ALiBi is a better fit for this setup:</p><ul><li>No position-embedding table</li><li>Relative bias available in every layer</li><li>Fewer constraints when pushing sequence lengths</li></ul><p>Reference: <a href=https://arxiv.org/abs/2108.12409 target=_blank rel=noopener>Train Short, Test Long: ALiBi</a>.</p><h2 id=qr-embeddings-for-large-categorical-spaces>QR Embeddings for Large Categorical Spaces
<a class=anchor href=#qr-embeddings-for-large-categorical-spaces>#</a></h2><p>For high-cardinality IDs (for example, artist IDs), we use quotient-remainder factorization:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-text data-lang=text><span style=display:flex><span>embed(id) = embed_q(id // R) + embed_r(id % R)
</span></span></code></pre></div><p>With <code>R = 1024</code>, this substantially reduces table size while preserving enough representational capacity for ranking.</p><p>This is a practical tradeoff: a small quality hit is acceptable if it unlocks larger batches and faster iteration.</p><h2 id=sparse-ids-and-missing-content-embeddings>Sparse IDs and Missing Content Embeddings
<a class=anchor href=#sparse-ids-and-missing-content-embeddings>#</a></h2><p>Observed properties in this setup:</p><ul><li>Item ID space can extend to ~9.4M</li><li>Only a subset has precomputed content embeddings</li><li>Missingness is non-trivial (around 18% in this run)</li></ul><p>What we did:</p><ul><li>Keep a compact ID-to-RQ lookup for known items</li><li>Route unknown/missing entries to a deterministic all-zero RQ code pattern</li><li>Let the model learn a stable fallback behavior for unknown content</li></ul><p>This avoided huge dense lookup tensors and prevented nondeterministic behavior.</p><h2 id=jagged-batching-with-block-masks>Jagged Batching with Block Masks
<a class=anchor href=#jagged-batching-with-block-masks>#</a></h2><p>All sequence events are concatenated into one token buffer plus offsets. Attention is constrained to legal user-local ranges.</p><pre class=mermaid>
flowchart LR
    A[User 1 tokens] --> D[Concatenated token buffer]
    B[User 2 tokens] --> D
    C[User 3 tokens] --> D
    D --> E[Block mask: no cross-user attention]
</pre><p>This gives better accelerator utilization than naive per-user padding.</p><h2 id=early-signal>Early Signal
<a class=anchor href=#early-signal>#</a></h2><table><thead><tr><th>Step</th><th>Main metric</th></tr></thead><tbody><tr><td>32,432</td><td>0.0723</td></tr><tr><td>64,864</td><td>0.1431</td></tr><tr><td>97,296</td><td><strong>0.1607</strong></td></tr></tbody></table><ul><li>Absolute gain: <code>+0.0884</code></li><li>Relative improvement from first logged point: about <code>2.2x</code></li></ul><p>This is still an intermediate training view; full offline ranking evaluation should include <code>Hit@K</code>, <code>MRR</code>, and <code>NDCG</code> over retrieved candidates.
For boundary-case debugging during retrieval-stage evaluation, see <a href=./hard_negatives.md>The Role of Negative Mining in Machine Learning</a>.</p><h2 id=tradeoffs-and-failure-modes>Tradeoffs and Failure Modes
<a class=anchor href=#tradeoffs-and-failure-modes>#</a></h2><p>Staff-level review usually focuses on what can go wrong. The biggest risks here are:</p><ol><li>Quantization bottleneck. If RQ codebooks are underfit, retrieval quality saturates early.</li><li>Bucket design brittleness. Bad time/duration buckets quietly cap model quality.</li><li>Missingness leakage. If unknown embeddings correlate with labels, the fallback path can become a shortcut feature.</li><li>Evaluation mismatch. Improvements in training metric may not transfer to retrieval-stage KPIs.</li></ol><p>Recommended guardrails:</p><ul><li>Run ablations for each bias term (ALiBi/time/duration/organic)</li><li>Track calibration and recall at retrieval depth</li><li>Monitor unknown-code frequency by segment</li><li>Keep one non-quantized baseline for regression detection</li><li>For retrieval-quality diagnostics at scale, combine this with <a href=./entity_resolution.md>Entity Resolution using Contrastive Learning</a>-style candidate analysis.</li></ul><h2 id=next-experiments>Next Experiments
<a class=anchor href=#next-experiments>#</a></h2><ul><li>QK normalization</li><li>Per-layer residual scaling</li><li>Logit soft-capping</li><li>Alternative optimizers (including <a href=https://github.com/KellerJordan/Muon target=_blank rel=noopener>Muon</a>)</li><li>Explicit ablations of duration and organic priors</li></ul><h2 id=references>References
<a class=anchor href=#references>#</a></h2><ul><li><a href=https://pytorch.org/docs/stable/nn.attention.flex_attention.html target=_blank rel=noopener>PyTorch FlexAttention</a></li><li><a href=https://arxiv.org/abs/2108.12409 target=_blank rel=noopener>ALiBi paper</a></li><li><a href=https://github.com/facebookresearch/faiss target=_blank rel=noopener>FAISS</a></li><li><a href=https://mermaid.js.org/ target=_blank rel=noopener>Mermaid</a></li></ul><h2 id=related-posts>Related Posts
<a class=anchor href=#related-posts>#</a></h2><ul><li><a href=./hard_negatives.md>The Role of Negative Mining in Machine Learning</a></li><li><a href=./entity_resolution.md>Entity Resolution using Contrastive Learning</a></li><li><a href=./ml_engineer_guidelines.md>Machine Learning Engineer Roadmap</a></li></ul><h2 id=closing>Closing
<a class=anchor href=#closing>#</a></h2><p>The architecture scales because it refuses expensive defaults:</p><ul><li>No dense attention bias matrices</li><li>No giant item softmax</li><li>No full-width categorical embedding tables where compression works</li><li>No padding-heavy batching</li></ul><p>In short: keep the inductive bias, remove the quadratic and dense bottlenecks.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><ul><li><a href=#executive-summary>Executive Summary</a></li><li><a href=#system-constraints>System Constraints</a></li><li><a href=#architecture-overview>Architecture Overview</a></li><li><a href=#why-rq-outputs-instead-of-item-id-softmax>Why RQ Outputs Instead of Item-ID Softmax</a></li><li><a href=#avoiding-os2-bias-materialization>Avoiding O(S^2) Bias Materialization</a></li><li><a href=#why-alibi-here>Why ALiBi Here</a></li><li><a href=#qr-embeddings-for-large-categorical-spaces>QR Embeddings for Large Categorical Spaces</a></li><li><a href=#sparse-ids-and-missing-content-embeddings>Sparse IDs and Missing Content Embeddings</a></li><li><a href=#jagged-batching-with-block-masks>Jagged Batching with Block Masks</a></li><li><a href=#early-signal>Early Signal</a></li><li><a href=#tradeoffs-and-failure-modes>Tradeoffs and Failure Modes</a></li><li><a href=#next-experiments>Next Experiments</a></li><li><a href=#references>References</a></li><li><a href=#related-posts>Related Posts</a></li><li><a href=#closing>Closing</a></li></ul></li></ul></nav></div></aside></main></body></html>