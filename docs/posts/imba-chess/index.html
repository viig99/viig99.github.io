<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="A learning journal on training a compact chess policy model using HSTU-style next-token prediction on Lichess games, adding a value head for WDL prediction, and using policy-pruned minimax search to improve playing strength."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://viig99.github.io/docs/posts/imba-chess/"><meta property="og:site_name" content="/home/vigi99"><meta property="og:title" content="Building a Chess Bot with HSTU: From Lichess Pretraining to Value Search"><meta property="og:description" content="A learning journal on training a compact chess policy model using HSTU-style next-token prediction on Lichess games, adding a value head for WDL prediction, and using policy-pruned minimax search to improve playing strength."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:published_time" content="2026-02-27T00:00:00+00:00"><meta property="article:modified_time" content="2026-02-27T19:23:59-05:00"><meta property="article:tag" content="Chess"><meta property="article:tag" content="HSTU"><meta property="article:tag" content="Reinforcement Learning"><meta property="article:tag" content="Next-Token Prediction"><meta property="article:tag" content="Value Head"><meta property="article:tag" content="Minimax Search"><meta itemprop=name content="Building a Chess Bot with HSTU: From Lichess Pretraining to Value Search"><meta itemprop=description content="A learning journal on training a compact chess policy model using HSTU-style next-token prediction on Lichess games, adding a value head for WDL prediction, and using policy-pruned minimax search to improve playing strength."><meta itemprop=datePublished content="2026-02-27T00:00:00+00:00"><meta itemprop=dateModified content="2026-02-27T19:23:59-05:00"><meta itemprop=wordCount content="2002"><meta itemprop=keywords content="Chess,HSTU,Reinforcement Learning,Next-Token Prediction,Value Head,Minimax Search,Lichess,Stockfish,Sequential Modeling,Large-Scale ML"><title>Building a Chess Bot with HSTU: From Lichess Pretraining to Value Search | /home/vigi99</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://viig99.github.io/docs/posts/imba-chess/><link rel=stylesheet href=/book.min.cc2c524ed250aac81b23d1f4af87344917b325208841feca0968fe450f570575.css integrity="sha256-zCxSTtJQqsgbI9H0r4c0SRezJSCIQf7KCWj+RQ9XBXU=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.f332a99f8c35bee04d8b600db74e6d0fbb5d77f5aa420a75226e47af9c29210d.js integrity="sha256-8zKpn4w1vuBNi2ANt05tD7tdd/WqQgp1Im5Hr5wpIQ0=" crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-55K4J76G9F"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-55K4J76G9F")}</script></head><body dir=ltr class="book-kind-page book-type-docs"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>/home/vigi99</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><a href=/docs/work-experience/>Work Experience</a><ul></ul></li><li><a href=/docs/open-source/>Open Source Contributions</a><ul></ul></li><li><a href=/docs/machine-learning/>Machine Learning Toolkit</a><ul></ul></li><li class=book-section-flat><a>Posts</a><ul><li><a href=/docs/posts/imba-chess/ class=active>Building a Chess Bot with HSTU: From Lichess Pretraining to Value Search</a></li><li><a href=/docs/posts/hstu-for-yambda/>FlexAttention HSTU at 500M Events: RQ Tokens, QR Embeddings, and 1D Biases</a></li><li><a href=/docs/posts/indian_salary_distribution/>Distribution for SDE Salaries in India</a></li><li><a href=/docs/posts/ml_engineer_guidelines/>Machine Learning Engineer Roadmap</a></li><li><a href=/docs/posts/supervised_finetuning/>Supervised Fine-Tuning in Large Language Models</a></li><li><a href=/docs/posts/hard_negatives/>The Role of Negative Mining in Machine Learning: Bridging the Gap in Model Performance</a></li><li><a href=/docs/posts/entity_resolution/>Entity Resolution using Contrastive Learning</a></li></ul></li></ul><ul><li><a href="mailto:accio.arjun@gmail.com?subject=Job%20Opportunity" target=_blank rel=noopener>Contact Me<span style=line-height:1em;vertical-align:middle><svg viewBox="0 0 512 512" style="height:1em;width:1em"><path fill="#ea4335" opacity="1" d="M0 128C0 92.65 28.65 64 64 64H448c35.3.0 64 28.65 64 64V384c0 35.3-28.7 64-64 64H64c-35.35.0-64-28.7-64-64V128zm48 0v22.1L220.5 291.7c20.6 17 50.4 17 71 0L464 150.1v-23C464 119.2 456.8 111.1 448 111.1H64C55.16 111.1 48 119.2 48 127.1V128zm0 84.2V384C48 392.8 55.16 4e2 64 4e2H448C456.8 4e2 464 392.8 464 384V212.2L322 328.8c-38.4 31.5-93.6 31.5-132.9.0L48 212.2z"/></svg></span></a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=/icons/menu.svg class=book-icon alt=Menu></label><h3>Building a Chess Bot with HSTU: From Lichess Pretraining to Value Search</h3><label for=toc-control><img src=/icons/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class=hidden><nav id=TableOfContents><ul><li><ul><li><a href=#executive-summary>Executive Summary</a></li><li><a href=#why-chess-and-why-hstu>Why Chess and Why HSTU</a></li><li><a href=#data-pipeline>Data Pipeline</a><ul><li><a href=#source>Source</a></li><li><a href=#temporal-splits>Temporal Splits</a></li><li><a href=#pgn-parsing-and-board-state>PGN Parsing and Board State</a></li><li><a href=#incremental-board-updates>Incremental Board Updates</a></li><li><a href=#jagged-batching>Jagged Batching</a></li></ul></li><li><a href=#model-architecture>Model Architecture</a><ul><li><a href=#overview>Overview</a></li><li><a href=#embedding-layers>Embedding Layers</a></li><li><a href=#hstu-backbone>HSTU Backbone</a></li></ul></li><li><a href=#training-phase-1-supervised-policy-pretraining>Training Phase 1: Supervised Policy Pretraining</a><ul><li><a href=#objective>Objective</a></li><li><a href=#elo-weighted-loss>Elo-Weighted Loss</a></li><li><a href=#training-infrastructure>Training Infrastructure</a></li></ul></li><li><a href=#evaluation-metrics>Evaluation Metrics</a><ul><li><a href=#offline-metrics-phase-1>Offline Metrics (Phase 1)</a></li><li><a href=#slice-metrics>Slice Metrics</a></li><li><a href=#engine-evaluation-phase-2>Engine Evaluation (Phase 2)</a></li></ul></li><li><a href=#phase-2-adding-a-value-head>Phase 2: Adding a Value Head</a><ul><li><a href=#why-a-value-head>Why a Value Head</a></li><li><a href=#wdl-classification>WDL Classification</a></li><li><a href=#progress-weighting>Progress Weighting</a></li><li><a href=#combined-loss>Combined Loss</a></li><li><a href=#training-schedule>Training Schedule</a></li></ul></li><li><a href=#phase-3-using-value-at-inference>Phase 3: Using Value at Inference</a><ul><li><a href=#mode-1-greedy-baseline>Mode 1: Greedy (Baseline)</a></li><li><a href=#mode-2-sampled-decoding>Mode 2: Sampled Decoding</a></li><li><a href=#mode-3-value-rerank-1-ply-lookahead>Mode 3: Value Rerank (1-Ply Lookahead)</a></li><li><a href=#mode-4-depth-2-policy-pruned-minimax>Mode 4: Depth-2 Policy-Pruned Minimax</a></li></ul></li><li><a href=#ablation-matrix>Ablation Matrix</a></li><li><a href=#current-limitations-and-known-issues>Current Limitations and Known Issues</a></li><li><a href=#planned-next-steps>Planned Next Steps</a></li><li><a href=#references>References</a></li><li><a href=#related-posts>Related Posts</a></li><li><a href=#closing>Closing</a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h2 id=executive-summary>Executive Summary<a class=anchor href=#executive-summary>#</a></h2><p>This is a running journal of <code>imba-chess</code>, a personal research project to build a compact (20–30M parameter) chess bot entirely from supervised pretraining on Lichess games, without any game-tree search engine to begin with.</p><p>The journey so far:</p><ul><li>Adapted HSTU-style causal sequence modeling from recommender systems to chess.</li><li>Trained next-move prediction (policy head) on high-Elo Lichess game data.</li><li>Evaluated offline metrics: top-1, hr@10, MRR.</li><li>Tested against Stockfish. Results: humbling.</li><li>Added a value head (WDL), wired it into move selection.</li><li>Implemented depth-2 policy-pruned minimax search on top.</li><li>Defined a Stockfish ladder evaluation pipeline for reproducible strength measurement.</li></ul><p>The throughline is: <strong>treat each chess ply as a structured event in a sequence, the same way you would model user behavior</strong>.</p><hr><h2 id=why-chess-and-why-hstu>Why Chess and Why HSTU<a class=anchor href=#why-chess-and-why-hstu>#</a></h2><p>Most chess neural nets (Leela, AlphaZero) are trained purely by self-play, using MCTS to generate experience. That is powerful but expensive. A cheaper first step is imitation: train on human games, learn the distribution of plausible moves, and then layer search on top.</p><p>HSTU (Hierarchical Sequential Transduction Unit) was designed for large-scale sequential recommendation — jagged user histories, variable event types, structured side features. Chess plylines have almost the same structure: a variable-length sequence of structured events (board states + moves), with a known target (next move) at each step.</p><p>The idea was: <strong>if HSTU can predict next song plays, it can predict next chess moves</strong>.</p><hr><h2 id=data-pipeline>Data Pipeline<a class=anchor href=#data-pipeline>#</a></h2><h3 id=source>Source<a class=anchor href=#source>#</a></h3><p>All data comes from <a href=https://database.lichess.org/ target=_blank rel=noopener>Lichess open game database</a>, accessed via the <code>Lichess/standard-chess-games</code> Hugging Face dataset. It is hive-partitioned by <code>year/month</code>, which makes temporal splits clean.</p><p>Filter: average Elo of <code>(WhiteElo + BlackElo) / 2 >= 2000</code>. High-Elo games have lower noise and better move quality.</p><h3 id=temporal-splits>Temporal Splits<a class=anchor href=#temporal-splits>#</a></h3><p>Splits are chronological, not random. This prevents future-move leakage and gives a more realistic out-of-sample test.</p><table><thead><tr><th>Split</th><th>Window</th></tr></thead><tbody><tr><td>train</td><td>2018-01 through 2025-07</td></tr><tr><td>val</td><td>2025-08 (single month)</td></tr><tr><td>test</td><td>2025-09 (single month)</td></tr></tbody></table><p>No model is selected using the test split.</p><h3 id=pgn-parsing-and-board-state>PGN Parsing and Board State<a class=anchor href=#pgn-parsing-and-board-state>#</a></h3><p>Each game is replayed with <code>python-chess</code>. At each ply, the board is converted to a structured token representation — not a FEN string, not a text sequence.</p><p><strong>Board state per ply:</strong></p><table><thead><tr><th>Field</th><th>Values</th><th>Notes</th></tr></thead><tbody><tr><td><code>piece_ids</code></td><td><code>[64]</code>, 0–12</td><td>0=empty, 1–6=white, 7–12=black</td></tr><tr><td><code>turn_id</code></td><td>0/1</td><td>side to move</td></tr><tr><td><code>castle_id</code></td><td>0–15</td><td>KQkq bitmask</td></tr><tr><td><code>ep_file_id</code></td><td>0–8</td><td>en-passant file + 1, 0=none</td></tr><tr><td><code>halfmove_bucket_id</code></td><td>≥0</td><td>bucketed clock</td></tr><tr><td><code>fullmove_bucket_id</code></td><td>≥0</td><td>bucketed move number</td></tr></tbody></table><p>Targets are UCI move IDs from a static vocabulary of all legal UCI moves (from→to + promotions).</p><h3 id=incremental-board-updates>Incremental Board Updates<a class=anchor href=#incremental-board-updates>#</a></h3><p>Rebuilding <code>piece_ids</code> from <code>board.piece_map()</code> every ply takes ~14–16 µs per ply. The optimization: update only the changed squares.</p><p>A chess move touches at most 4 squares (castling: king + rook). So we maintain a <code>bytearray(64)</code> and apply incremental updates:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-text data-lang=text><span style=display:flex><span>Normal move:    2 squares
</span></span><span style=display:flex><span>Capture:        2 squares
</span></span><span style=display:flex><span>En passant:     3 squares
</span></span><span style=display:flex><span>Castling:       4 squares
</span></span><span style=display:flex><span>Promotion:      2 squares</span></span></code></pre></div><p>This drops board encoding from ~16 µs to ~2–5 µs per ply. At Lichess dataset scale (hundreds of millions of plies) this is a meaningful saving.</p><h3 id=jagged-batching>Jagged Batching<a class=anchor href=#jagged-batching>#</a></h3><p>Multiple games are packed into a single flat token buffer, with <code>seq_offsets</code> marking boundaries. This is the same trick from <a href=./hstu-for-yambda.md>FlexAttention HSTU at 500M Events</a>: no cross-game attention, no padding waste.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-text data-lang=text><span style=display:flex><span>[BOS | ply1 ply2 ... plyN | BOS | ply1 ... plyM | ...]
</span></span><span style=display:flex><span>       game 1                    game 2</span></span></code></pre></div><p>Batch shape: <code>[total_tokens]</code> for most fields, <code>[total_tokens, 64]</code> for <code>piece_ids</code>. No attention mask needed when <code>seq_offsets</code> are passed directly to FlexAttention.</p><hr><h2 id=model-architecture>Model Architecture<a class=anchor href=#model-architecture>#</a></h2><h3 id=overview>Overview<a class=anchor href=#overview>#</a></h3><pre class=mermaid>
flowchart LR
    A[piece_ids 64 tokens] --&gt; B[E_piece + E_square]
    B --&gt; C[Mean pool → board_emb]
    D[prev_move_id] --&gt; E[E_move]
    F[turn/castle/ep/clk] --&gt; G[E_meta]
    C --&gt; H[Concat + Project → event_t]
    E --&gt; H
    G --&gt; H
    H --&gt; I[HSTU Backbone causal N layers]
    I --&gt; J[Policy Head → move logits]
    I --&gt; K[Value Head → loss/draw/win]
</pre><script src=/mermaid.min.js onload='mermaid.initialize({flowchart:{useMaxWidth:!0},theme:"default"})'></script><h3 id=embedding-layers>Embedding Layers<a class=anchor href=#embedding-layers>#</a></h3><p>Each ply is converted to a single event vector by embedding structured fields and concatenating them:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-text data-lang=text><span style=display:flex><span>event_t = LN(W · concat(board_emb, move_emb_{t-1}, meta_emb))</span></span></code></pre></div><p>Where:</p><ul><li><code>board_emb</code> = mean over (E_piece(piece_ids) + E_square(index)) for all 64 squares</li><li><code>move_emb_{t-1}</code> = embedding of the previous move (or START token at ply 1)</li><li><code>meta_emb</code> = embeddings of turn, castling rights, en-passant file, clock buckets</li></ul><h3 id=hstu-backbone>HSTU Backbone<a class=anchor href=#hstu-backbone>#</a></h3><p>The backbone is a causal transformer with relative positional bias indexed by ply number. It runs over the event sequence <code>e_1 … e_T</code> and produces hidden states per ply.</p><p>Target model size: 20–30M parameters. This fits in a single RTX 4090 training run.</p><hr><h2 id=training-phase-1-supervised-policy-pretraining>Training Phase 1: Supervised Policy Pretraining<a class=anchor href=#training-phase-1-supervised-policy-pretraining>#</a></h2><h3 id=objective>Objective<a class=anchor href=#objective>#</a></h3><p>Cross-entropy on next legal UCI move:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-text data-lang=text><span style=display:flex><span>loss = CE(logits_masked, target_move_id)</span></span></code></pre></div><p>BOS positions are excluded via <code>ignore_index = -100</code>.</p><h3 id=elo-weighted-loss>Elo-Weighted Loss<a class=anchor href=#elo-weighted-loss>#</a></h3><p>Not all moves are equally informative. Moves by 2000 Elo players carry less signal than moves by 2600 Elo players. We apply per-token Elo weighting:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-text data-lang=text><span style=display:flex><span>norm_i = clamp((elo_i - min_elo) / (max_elo - min_elo), 0, 1)
</span></span><span style=display:flex><span>w_i    = 1 + strength × (norm_i ^ alpha)
</span></span><span style=display:flex><span>loss   = Σ(w_i × ce_i) / Σ(w_i)</span></span></code></pre></div><p>Weight normalization keeps gradient scale stable when weighting is on.</p><p>Label smoothing is also applied to account for move non-uniqueness: strong positions often have multiple valid moves.</p><h3 id=training-infrastructure>Training Infrastructure<a class=anchor href=#training-infrastructure>#</a></h3><ul><li>Optimizer: StableAdamW with OneCycleLR scheduler</li><li>Precision: bfloat16 mixed precision</li><li>Checkpointing: best by <code>hr@10</code> on full val, plus periodic last checkpoints</li><li>Logging: TensorBoard + periodic fast val/test checks</li></ul><hr><h2 id=evaluation-metrics>Evaluation Metrics<a class=anchor href=#evaluation-metrics>#</a></h2><h3 id=offline-metrics-phase-1>Offline Metrics (Phase 1)<a class=anchor href=#offline-metrics-phase-1>#</a></h3><p>Evaluated on held-out val/test splits every N steps:</p><table><thead><tr><th>Metric</th><th>What it measures</th></tr></thead><tbody><tr><td><code>loss_ce</code></td><td>Cross-entropy on target move</td></tr><tr><td><code>ppl</code></td><td>Perplexity (exp of loss_ce)</td></tr><tr><td><code>top1_acc</code></td><td>Argmax move matches human move</td></tr><tr><td><code>top3_acc</code> / <code>top5_acc</code></td><td>Move in top-3/5</td></tr><tr><td><code>hr@10</code></td><td>Hit rate at 10 (top-10 accuracy)</td></tr><tr><td><code>mrr</code></td><td>Mean reciprocal rank of ground-truth move</td></tr></tbody></table><p>Model selection uses <code>hr@10</code> from full val as the primary signal.</p><h3 id=slice-metrics>Slice Metrics<a class=anchor href=#slice-metrics>#</a></h3><p>Global averages hide regressions. We also report by:</p><ul><li><strong>Game phase</strong>: opening (ply 1–20), middlegame (ply 21–60), endgame (ply 61+)</li><li><strong>Elo bucket</strong>: 2000–2199, 2200–2399, 2400+</li></ul><h3 id=engine-evaluation-phase-2>Engine Evaluation (Phase 2)<a class=anchor href=#engine-evaluation-phase-2>#</a></h3><p>After offline metrics stabilize, the model plays against Stockfish:</p><ul><li>Alternating colors, fixed time controls</li><li>Ladder evaluation across Elo settings: 1600, 1800, 2000, 2200, 2400, 2600, 2800</li><li>Reports: wins/draws/losses, score rate, color split, Elo estimate with confidence intervals</li></ul><hr><h2 id=phase-2-adding-a-value-head>Phase 2: Adding a Value Head<a class=anchor href=#phase-2-adding-a-value-head>#</a></h2><h3 id=why-a-value-head>Why a Value Head<a class=anchor href=#why-a-value-head>#</a></h3><p>A policy head alone picks moves based on how likely a human would play them. It does not reason about <strong>outcomes</strong>. The value head adds a separate prediction: given the current board position, what is the probability of winning, drawing, or losing?</p><p>Without value at inference time, the model cannot distinguish between:</p><ul><li>&ldquo;This move is popular in human games&rdquo; (policy says yes)</li><li>&ldquo;This move leads to a winning position&rdquo; (requires value)</li></ul><h3 id=wdl-classification>WDL Classification<a class=anchor href=#wdl-classification>#</a></h3><p>The value head is a 3-class classifier from the side-to-move perspective:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-text data-lang=text><span style=display:flex><span>value_logits = Linear(d, 3)  # [loss, draw, win]</span></span></code></pre></div><p>Labels are derived from the per-game result (<code>game_result_white ∈ {+1, 0, -1}</code>) and per-token <code>turn_id</code> (to flip perspective for black).</p><p>A scalar value is extracted as:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-text data-lang=text><span style=display:flex><span>V(s) = p(win) - p(loss) ∈ [-1, 1]</span></span></code></pre></div><h3 id=progress-weighting>Progress Weighting<a class=anchor href=#progress-weighting>#</a></h3><p>Value labels derived from final game results are noisy — early positions have a weak causal link to who ultimately wins. We downweight early plies and emphasize later ones:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-text data-lang=text><span style=display:flex><span>progress_weight = (ply_index / total_plies) ^ alpha</span></span></code></pre></div><p>With <code>alpha = 1.5</code>, the first few plies contribute little; the endgame contributes most.</p><h3 id=combined-loss>Combined Loss<a class=anchor href=#combined-loss>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-text data-lang=text><span style=display:flex><span>total_loss = policy_loss + λ × value_loss</span></span></code></pre></div><p>We start with <code>λ = 0.15</code> to protect policy quality while the value head bootstraps. If <code>top1/hr@10</code> drops sharply, reduce <code>λ</code> further.</p><pre class=mermaid>
flowchart TD
    A[HSTU hidden state] --&gt; B[Policy Head]
    A --&gt; C[Value Head]
    B --&gt; D[CE loss on next move]
    C --&gt; E[CE loss on WDL outcome]
    D --&gt; F[total_loss = policy_loss + 0.15 × value_loss]
    E --&gt; F
</pre><h3 id=training-schedule>Training Schedule<a class=anchor href=#training-schedule>#</a></h3><ol><li><strong>Warm start</strong> (optional): freeze backbone for 1k–3k steps, train only heads.</li><li><strong>Joint training</strong>: unfreeze all, keep <code>value_loss_weight</code> low initially.</li><li><strong>Monitor</strong>: if policy metrics drop, reduce value weight.</li></ol><hr><h2 id=phase-3-using-value-at-inference>Phase 3: Using Value at Inference<a class=anchor href=#phase-3-using-value-at-inference>#</a></h2><p>Adding a value head to training only modestly improves playing strength. The real gain comes from using the value during <strong>move selection</strong>.</p><h3 id=mode-1-greedy-baseline>Mode 1: Greedy (Baseline)<a class=anchor href=#mode-1-greedy-baseline>#</a></h3><p>Pick the highest-logit legal move. Fast, deterministic, no value used.</p><h3 id=mode-2-sampled-decoding>Mode 2: Sampled Decoding<a class=anchor href=#mode-2-sampled-decoding>#</a></h3><p>Sample from top-k / top-p legal moves with temperature. Adds variety, occasionally finds surprising moves, but can also pick blunders.</p><h3 id=mode-3-value-rerank-1-ply-lookahead>Mode 3: Value Rerank (1-Ply Lookahead)<a class=anchor href=#mode-3-value-rerank-1-ply-lookahead>#</a></h3><p>Take top-K policy candidates, evaluate each resulting position with the value head, pick the best:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-text data-lang=text><span style=display:flex><span>score(move) = log π(move | s) - λ × V(next_state)</span></span></code></pre></div><p>The minus sign: after we move, it is the opponent&rsquo;s turn at <code>next_state</code>, so high opponent value is bad for us.</p><pre class=mermaid>
flowchart LR
    A[Current state s] --&gt; B[Policy: top-K legal moves]
    B --&gt; C[Apply each move → s&#39;]
    C --&gt; D[Value head on each s&#39;]
    D --&gt; E[Score = log π - λ V_opp]
    E --&gt; F[Pick best scoring move]
</pre><p>Default settings: <code>K = 8</code>, <code>λ = 0.35</code>.</p><h3 id=mode-4-depth-2-policy-pruned-minimax>Mode 4: Depth-2 Policy-Pruned Minimax<a class=anchor href=#mode-4-depth-2-policy-pruned-minimax>#</a></h3><p>Go one level deeper: after our move, simulate the opponent&rsquo;s best response, then pick our move that leads to the best position after that response. This is one-step minimax.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-text data-lang=text><span style=display:flex><span>Q(a) = min_{b ∈ top-K} V(apply(apply(s, a), b))
</span></span><span style=display:flex><span>a*   = argmax_a Q(a)</span></span></code></pre></div><p>The branching factor is controlled by keeping only top-K policy candidates at each ply: K1 candidate moves for us, K2 opponent responses each.</p><pre class=mermaid>
flowchart TD
    A[Root state s] --&gt; B[Our top-K1 moves]
    B --&gt; C[For each candidate a → s&#39;]
    C --&gt; D[Opponent top-K2 moves]
    D --&gt; E[For each response b → s&#39;&#39;]
    E --&gt; F[Value V at s&#39;&#39;]
    F --&gt; G[Opponent picks b that minimizes V for us]
    G --&gt; H[We pick a with best worst-case V]
</pre><p>Why this matters more than RL early on: most amateur-level losses come from hanging pieces, missing forks, and stepping into mate threats. Depth-2 catches a large fraction of these because it explicitly asks &ldquo;what is my opponent&rsquo;s best immediate reply?&rdquo;</p><p><strong>Batch optimization</strong>: instead of calling the transformer node-by-node, batch all K1 × K2 grandchild states into a single forward pass. This can be 10–100× faster on GPU.</p><hr><h2 id=ablation-matrix>Ablation Matrix<a class=anchor href=#ablation-matrix>#</a></h2><p>To measure what actually moves the needle on Stockfish win rate, we run a controlled comparison:</p><table><thead><tr><th>Configuration</th><th>Description</th></tr></thead><tbody><tr><td>Policy-only + greedy</td><td>Baseline</td></tr><tr><td>Policy+value training, greedy decode</td><td>Does value training help representations?</td></tr><tr><td>Policy+value training, value-rerank</td><td>Does 1-ply value improve play?</td></tr><tr><td>Policy+value training, depth-2 search</td><td>Does minimax help further?</td></tr></tbody></table><p>All comparisons use the same Stockfish time controls and opening protocols.</p><hr><h2 id=current-limitations-and-known-issues>Current Limitations and Known Issues<a class=anchor href=#current-limitations-and-known-issues>#</a></h2><ul><li>No legal-move masking in the prediction head yet. Full-vocab classification. The model can in principle output illegal move IDs (tracked separately as <code>legal_top1</code>).</li><li>Training is single-process (no DDP launcher yet).</li><li><code>value_rerank</code> is one-ply only; <code>value_search_d2</code> is depth-2 and substantially slower than greedy.</li><li>Value labels are noisy for early plies — progress weighting helps but does not fully solve this.</li><li>Value head may learn player-strength bias (higher Elo games have more draws): tracked by Elo-diff slices during eval.</li></ul><hr><h2 id=planned-next-steps>Planned Next Steps<a class=anchor href=#planned-next-steps>#</a></h2><p><strong>Self-play RL (Phase 4)</strong></p><p>After pretraining stabilizes and value metrics look calibrated, the next stage is RL fine-tuning via self-play:</p><ul><li>Environment: <code>gym-chess</code> with parallel rollouts (~1000 workers via pufferlib)</li><li>Algorithm: PPO or KL-regularized PPO (GRPO-style)</li><li>Reward: +1 win, 0 draw, −1 loss; optional shaping from engine eval delta</li><li>League: self-play against current + past checkpoints; optional Stockfish/Leela matches</li></ul><p><strong>Beam Search</strong></p><p>Another direction: instead of depth-2 minimax, run beam search over likely continuations. Policy priors guide the beam; value head scores leaf nodes. More compute, potentially better tactical vision.</p><p><strong>Scaling</strong></p><p>Current target is 20–30M parameters on a single RTX 4090. Interesting questions:</p><ul><li>Does Elo scale smoothly with model size?</li><li>Does value search help more at smaller model sizes (where policy alone is weaker)?</li></ul><hr><h2 id=references>References<a class=anchor href=#references>#</a></h2><ul><li><a href=https://database.lichess.org/ target=_blank rel=noopener>Lichess Open Database</a></li><li><a href=https://python-chess.readthedocs.io/ target=_blank rel=noopener>python-chess</a></li><li><a href=https://github.com/google-deepmind/searchless_chess target=_blank rel=noopener>Searchless Chess (DeepMind)</a></li><li><a href=https://github.com/noamdwc/grpo_chess target=_blank rel=noopener>grpo_chess</a></li><li><a href=https://pytorch.org/docs/stable/nn.attention.flex_attention.html target=_blank rel=noopener>PyTorch FlexAttention</a></li><li><a href=https://mermaid.js.org/ target=_blank rel=noopener>Mermaid</a></li></ul><h2 id=related-posts>Related Posts<a class=anchor href=#related-posts>#</a></h2><ul><li><a href=./hstu-for-yambda.md>FlexAttention HSTU at 500M Events</a></li><li><a href=./ml_engineer_guidelines.md>Machine Learning Engineer Roadmap</a></li></ul><h2 id=closing>Closing<a class=anchor href=#closing>#</a></h2><p>The bet here is that structured event modeling — the same pattern that works for sequential recommendation — transfers cleanly to chess. Board state is richer than a user&rsquo;s listening history, but the sequence modeling problem is the same: predict what comes next from what came before.</p><p>The value head and minimax search are the bridge from imitation to reasoning. Imitation learns the prior; search uses that prior to avoid mistakes.</p><p>Whether that&rsquo;s enough to reach a respectable Elo without full RL self-play is the open question.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=/docs/machine-learning/ class="flex align-center"><img src=/icons/backward.svg class=book-icon alt=Backward>
<span>Machine Learning Toolkit</span>
</a></span><span><a href=/docs/posts/hstu-for-yambda/ class="flex align-center"><span>FlexAttention HSTU at 500M Events: RQ Tokens, QR Embeddings, and 1D Biases</span>
<img src=/icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><ul><li><a href=#executive-summary>Executive Summary</a></li><li><a href=#why-chess-and-why-hstu>Why Chess and Why HSTU</a></li><li><a href=#data-pipeline>Data Pipeline</a><ul><li><a href=#source>Source</a></li><li><a href=#temporal-splits>Temporal Splits</a></li><li><a href=#pgn-parsing-and-board-state>PGN Parsing and Board State</a></li><li><a href=#incremental-board-updates>Incremental Board Updates</a></li><li><a href=#jagged-batching>Jagged Batching</a></li></ul></li><li><a href=#model-architecture>Model Architecture</a><ul><li><a href=#overview>Overview</a></li><li><a href=#embedding-layers>Embedding Layers</a></li><li><a href=#hstu-backbone>HSTU Backbone</a></li></ul></li><li><a href=#training-phase-1-supervised-policy-pretraining>Training Phase 1: Supervised Policy Pretraining</a><ul><li><a href=#objective>Objective</a></li><li><a href=#elo-weighted-loss>Elo-Weighted Loss</a></li><li><a href=#training-infrastructure>Training Infrastructure</a></li></ul></li><li><a href=#evaluation-metrics>Evaluation Metrics</a><ul><li><a href=#offline-metrics-phase-1>Offline Metrics (Phase 1)</a></li><li><a href=#slice-metrics>Slice Metrics</a></li><li><a href=#engine-evaluation-phase-2>Engine Evaluation (Phase 2)</a></li></ul></li><li><a href=#phase-2-adding-a-value-head>Phase 2: Adding a Value Head</a><ul><li><a href=#why-a-value-head>Why a Value Head</a></li><li><a href=#wdl-classification>WDL Classification</a></li><li><a href=#progress-weighting>Progress Weighting</a></li><li><a href=#combined-loss>Combined Loss</a></li><li><a href=#training-schedule>Training Schedule</a></li></ul></li><li><a href=#phase-3-using-value-at-inference>Phase 3: Using Value at Inference</a><ul><li><a href=#mode-1-greedy-baseline>Mode 1: Greedy (Baseline)</a></li><li><a href=#mode-2-sampled-decoding>Mode 2: Sampled Decoding</a></li><li><a href=#mode-3-value-rerank-1-ply-lookahead>Mode 3: Value Rerank (1-Ply Lookahead)</a></li><li><a href=#mode-4-depth-2-policy-pruned-minimax>Mode 4: Depth-2 Policy-Pruned Minimax</a></li></ul></li><li><a href=#ablation-matrix>Ablation Matrix</a></li><li><a href=#current-limitations-and-known-issues>Current Limitations and Known Issues</a></li><li><a href=#planned-next-steps>Planned Next Steps</a></li><li><a href=#references>References</a></li><li><a href=#related-posts>Related Posts</a></li><li><a href=#closing>Closing</a></li></ul></li></ul></nav></div></aside></main></body></html>