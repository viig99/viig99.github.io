<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="A learning journal on training a compact chess policy model using HSTU-style next-token prediction on Lichess games, adding a value head for WDL prediction, and using policy-pruned minimax search to improve playing strength."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://viig99.github.io/docs/posts/imba-chess/"><meta property="og:site_name" content="/home/vigi99"><meta property="og:title" content="Building a Chess Bot with HSTU: From Lichess Pretraining to Value Search"><meta property="og:description" content="A learning journal on training a compact chess policy model using HSTU-style next-token prediction on Lichess games, adding a value head for WDL prediction, and using policy-pruned minimax search to improve playing strength."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:published_time" content="2026-02-27T00:00:00+00:00"><meta property="article:modified_time" content="2026-02-27T19:39:09-05:00"><meta property="article:tag" content="Chess"><meta property="article:tag" content="HSTU"><meta property="article:tag" content="Reinforcement Learning"><meta property="article:tag" content="Next-Token Prediction"><meta property="article:tag" content="Value Head"><meta property="article:tag" content="Minimax Search"><meta itemprop=name content="Building a Chess Bot with HSTU: From Lichess Pretraining to Value Search"><meta itemprop=description content="A learning journal on training a compact chess policy model using HSTU-style next-token prediction on Lichess games, adding a value head for WDL prediction, and using policy-pruned minimax search to improve playing strength."><meta itemprop=datePublished content="2026-02-27T00:00:00+00:00"><meta itemprop=dateModified content="2026-02-27T19:39:09-05:00"><meta itemprop=wordCount content="2392"><meta itemprop=keywords content="Chess,HSTU,Reinforcement Learning,Next-Token Prediction,Value Head,Minimax Search,Lichess,Stockfish,Sequential Modeling,Large-Scale ML"><title>Building a Chess Bot with HSTU: From Lichess Pretraining to Value Search | /home/vigi99</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://viig99.github.io/docs/posts/imba-chess/><link rel=stylesheet href=/book.min.cc2c524ed250aac81b23d1f4af87344917b325208841feca0968fe450f570575.css integrity="sha256-zCxSTtJQqsgbI9H0r4c0SRezJSCIQf7KCWj+RQ9XBXU=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.3ae5093989fed4e2dd6804327b7cd82648ba85f61f2f958aface833b6d21b2d0.js integrity="sha256-OuUJOYn+1OLdaAQye3zYJki6hfYfL5WK+s6DO20hstA=" crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-55K4J76G9F"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-55K4J76G9F")}</script></head><body dir=ltr class="book-kind-page book-type-docs"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>/home/vigi99</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><a href=/docs/work-experience/>Work Experience</a><ul></ul></li><li><a href=/docs/open-source/>Open Source Contributions</a><ul></ul></li><li><a href=/docs/machine-learning/>Machine Learning Toolkit</a><ul></ul></li><li class=book-section-flat><a>Posts</a><ul><li><a href=/docs/posts/imba-chess/ class=active>Building a Chess Bot with HSTU: From Lichess Pretraining to Value Search</a></li><li><a href=/docs/posts/hstu-for-yambda/>FlexAttention HSTU at 500M Events: RQ Tokens, QR Embeddings, and 1D Biases</a></li><li><a href=/docs/posts/indian_salary_distribution/>Distribution for SDE Salaries in India</a></li><li><a href=/docs/posts/ml_engineer_guidelines/>Machine Learning Engineer Roadmap</a></li><li><a href=/docs/posts/supervised_finetuning/>Supervised Fine-Tuning in Large Language Models</a></li><li><a href=/docs/posts/hard_negatives/>The Role of Negative Mining in Machine Learning: Bridging the Gap in Model Performance</a></li><li><a href=/docs/posts/entity_resolution/>Entity Resolution using Contrastive Learning</a></li></ul></li></ul><ul><li><a href="mailto:accio.arjun@gmail.com?subject=Job%20Opportunity" target=_blank rel=noopener>Contact Me<span style=line-height:1em;vertical-align:middle><svg viewBox="0 0 512 512" style="height:1em;width:1em"><path fill="#ea4335" opacity="1" d="M0 128C0 92.65 28.65 64 64 64H448c35.3.0 64 28.65 64 64V384c0 35.3-28.7 64-64 64H64c-35.35.0-64-28.7-64-64V128zm48 0v22.1L220.5 291.7c20.6 17 50.4 17 71 0L464 150.1v-23C464 119.2 456.8 111.1 448 111.1H64C55.16 111.1 48 119.2 48 127.1V128zm0 84.2V384C48 392.8 55.16 4e2 64 4e2H448C456.8 4e2 464 392.8 464 384V212.2L322 328.8c-38.4 31.5-93.6 31.5-132.9.0L48 212.2z"/></svg></span></a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=/icons/menu.svg class=book-icon alt=Menu></label><h3>Building a Chess Bot with HSTU: From Lichess Pretraining to Value Search</h3><label for=toc-control><img src=/icons/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class=hidden><nav id=TableOfContents><ul><li><ul><li><a href=#build-log-snapshot>Build Log Snapshot</a></li><li><a href=#what-triggered-this-project>What Triggered This Project</a></li><li><a href=#why-chess-and-why-hstu>Why Chess and Why HSTU</a></li><li><a href=#original-project-brief-early-notes>Original Project Brief (Early Notes)</a></li><li><a href=#data-pipeline>Data Pipeline</a><ul><li><a href=#source>Source</a></li><li><a href=#temporal-splits>Temporal Splits</a></li><li><a href=#historical-run-profile-from-earlier-training-notes>Historical Run Profile (From Earlier Training Notes)</a></li><li><a href=#pgn-parsing-and-board-state>PGN Parsing and Board State</a></li><li><a href=#incremental-board-updates-benchmarked-not-yet-main-path>Incremental Board Updates (Benchmarked, Not Yet Main Path)</a></li><li><a href=#jagged-batching>Jagged Batching</a></li></ul></li><li><a href=#model-architecture>Model Architecture</a><ul><li><a href=#overview>Overview</a></li><li><a href=#embedding-layers>Embedding Layers</a></li><li><a href=#hstu-backbone>HSTU Backbone</a></li></ul></li><li><a href=#training-phase-1-supervised-policy-pretraining>Training Phase 1: Supervised Policy Pretraining</a><ul><li><a href=#objective>Objective</a></li><li><a href=#elo-weighted-loss>Elo-Weighted Loss</a></li><li><a href=#training-infrastructure>Training Infrastructure</a></li></ul></li><li><a href=#evaluation-metrics>Evaluation Metrics</a><ul><li><a href=#offline-metrics-phase-1>Offline Metrics (Phase 1)</a></li><li><a href=#slice-metrics-planned-expansion>Slice Metrics (Planned Expansion)</a></li><li><a href=#engine-evaluation-phase-2>Engine Evaluation (Phase 2)</a></li><li><a href=#current-snapshot-from-stored-artifacts>Current Snapshot from Stored Artifacts</a></li><li><a href=#early-milestone-notes-before-full-valuesearch-integration>Early Milestone Notes (Before Full Value/Search Integration)</a></li></ul></li><li><a href=#phase-2-adding-a-value-head>Phase 2: Adding a Value Head</a><ul><li><a href=#why-a-value-head>Why a Value Head</a></li><li><a href=#wdl-classification>WDL Classification</a></li><li><a href=#progress-weighting>Progress Weighting</a></li><li><a href=#combined-loss>Combined Loss</a></li><li><a href=#training-schedule>Training Schedule</a></li></ul></li><li><a href=#phase-3-using-value-at-inference>Phase 3: Using Value at Inference</a><ul><li><a href=#mode-1-greedy-baseline>Mode 1: Greedy (Baseline)</a></li><li><a href=#mode-2-sampled-decoding>Mode 2: Sampled Decoding</a></li><li><a href=#mode-3-value-rerank-1-ply-lookahead>Mode 3: Value Rerank (1-Ply Lookahead)</a></li><li><a href=#mode-4-depth-2-policy-pruned-minimax>Mode 4: Depth-2 Policy-Pruned Minimax</a></li></ul></li><li><a href=#what-went-wrong-and-why>What Went Wrong (And Why)</a></li><li><a href=#ablation-matrix>Ablation Matrix</a></li><li><a href=#current-limitations-and-known-issues>Current Limitations and Known Issues</a></li><li><a href=#planned-next-steps>Planned Next Steps</a></li><li><a href=#references>References</a></li><li><a href=#related-posts>Related Posts</a></li><li><a href=#closing>Closing</a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h2 id=build-log-snapshot>Build Log Snapshot<a class=anchor href=#build-log-snapshot>#</a></h2><p>This post is my running build log for <code>imba-chess</code>.</p><p>Right now, the checked-in model is about <strong>12.6M parameters</strong>.<br>My original target in the notes was 20–30M, but I am still iterating toward that.</p><p>The project is intentionally built to run on constrained hardware.<br>Most of this training/dev loop has been on my <strong>8GB RTX 3070 laptop</strong>.</p><p>What I have done so far:</p><ul><li>Adapted HSTU-style causal sequence modeling from recommender systems to chess.</li><li>Trained next-move prediction (policy head) on high-Elo Lichess game data.</li><li>Evaluated offline metrics: top-1, hr@10, MRR.</li><li>Tested against Stockfish. Results: humbling.</li><li>Added a value head (WDL), wired it into move selection.</li><li>Implemented depth-2 policy-pruned minimax search on top.</li><li>Defined a Stockfish ladder evaluation pipeline for reproducible strength measurement.</li></ul><p>My core framing is: <strong>a chess game is a structured event sequence</strong>.</p><hr><h2 id=what-triggered-this-project>What Triggered This Project<a class=anchor href=#what-triggered-this-project>#</a></h2><p>Before this, I finished an STU + FlexAttention rewrite and tested it on:</p><ul><li>Zvuk-200M</li><li>Yambda-500M</li></ul><p>That work beat strong SASRec baselines and showed clean scaling.<br>After a point, additional recsys gains felt marginal, so I wanted a harder sequential reasoning problem.</p><p>Chess was the natural next step:</p><ul><li>two-player sequential decisions</li><li>long horizon credit assignment</li><li>explicit win/draw/loss outcomes</li><li>cheap large-scale supervised data from Lichess</li></ul><p>So this became my side learning project: can the same sequence toolkit transfer from recommendation to strategy?</p><hr><h2 id=why-chess-and-why-hstu>Why Chess and Why HSTU<a class=anchor href=#why-chess-and-why-hstu>#</a></h2><p>Most strong chess systems lean heavily on self-play + search. That works, but it is expensive.<br>I started with a cheaper path: imitate strong human games first, then add value/search.</p><p>HSTU was built for jagged structured sequences in recsys.<br>Chess has the same shape: variable-length event sequences, structured state, known next-action target.</p><p>Simple bet: <strong>if it can predict next-item well, it can predict next-move well</strong>.</p><hr><h2 id=original-project-brief-early-notes>Original Project Brief (Early Notes)<a class=anchor href=#original-project-brief-early-notes>#</a></h2><p>My initial brief was:</p><ul><li>Use the full Lichess stream (~2.3TB scale over many years).</li><li>Encode board state + move history + metadata.</li><li>Add player embeddings (QR), Elo buckets, time-control features, and clock signals.</li><li>Pretrain on next legal UCI move prediction.</li><li>Move into RL post-training (PPO/GRPO), self-play via pufferlib, and/or engine matches.</li><li>Evaluate against Stockfish/Leela by win-rate and Elo trends.</li><li>Keep the whole system practical on a single 4090 in roughly 20–30M parameters.</li></ul><p>That direction still holds, but the implementation has changed based on actual results.</p><hr><h2 id=data-pipeline>Data Pipeline<a class=anchor href=#data-pipeline>#</a></h2><h3 id=source>Source<a class=anchor href=#source>#</a></h3><p>I use the <a href=https://database.lichess.org/ target=_blank rel=noopener>Lichess open game database</a> via <code>Lichess/standard-chess-games</code> on Hugging Face.
The hive <code>year/month</code> partitioning makes temporal splits straightforward.</p><p>Main filter: average Elo <code>(WhiteElo + BlackElo) / 2 >= 2000</code>.</p><h3 id=temporal-splits>Temporal Splits<a class=anchor href=#temporal-splits>#</a></h3><p>I use chronological splits (not random) to avoid future leakage.</p><table><thead><tr><th>Split</th><th>Window</th></tr></thead><tbody><tr><td>train</td><td>2021-01 through 2025-06</td></tr><tr><td>val</td><td>2025-07 (single month)</td></tr><tr><td>test</td><td>2025-08 through 2025-09</td></tr></tbody></table><p>No model is selected using the test split.</p><h3 id=historical-run-profile-from-earlier-training-notes>Historical Run Profile (From Earlier Training Notes)<a class=anchor href=#historical-run-profile-from-earlier-training-notes>#</a></h3><p>One earlier run (from notes) used:</p><ul><li>Train window: <code>2018-01 -> 2025-06</code></li><li>Train filter: average Elo >= 2000</li><li>Test filter: average Elo >= 2400 on <code>2025-08 -> 2025-09</code></li></ul><p>From those notes:</p><ul><li>estimated high-Elo train pool around 422M games</li><li>observed ingest/training throughput around ~1.5M games/hour in that run</li><li>average game length observed around 70-80 moves</li></ul><p>These are run-log observations, not the current default config.</p><h3 id=pgn-parsing-and-board-state>PGN Parsing and Board State<a class=anchor href=#pgn-parsing-and-board-state>#</a></h3><p>Each game is replayed with <code>python-chess</code>. At each ply, the board is converted to a structured token representation — not a FEN string, not a text sequence.</p><p><strong>Board state per ply:</strong></p><table><thead><tr><th>Field</th><th>Values</th><th>Notes</th></tr></thead><tbody><tr><td><code>piece_ids</code></td><td><code>[64]</code>, 0–12</td><td>0=empty, 1–6=white, 7–12=black</td></tr><tr><td><code>turn_id</code></td><td>0/1</td><td>side to move</td></tr><tr><td><code>castle_id</code></td><td>0–15</td><td>KQkq bitmask</td></tr><tr><td><code>ep_file_id</code></td><td>0–8</td><td>en-passant file + 1, 0=none</td></tr><tr><td><code>halfmove_bucket_id</code></td><td>≥0</td><td>bucketed clock</td></tr><tr><td><code>fullmove_bucket_id</code></td><td>≥0</td><td>bucketed move number</td></tr></tbody></table><p>Targets are UCI move IDs from a static vocabulary of all legal UCI moves (from→to + promotions).</p><h3 id=incremental-board-updates-benchmarked-not-yet-main-path>Incremental Board Updates (Benchmarked, Not Yet Main Path)<a class=anchor href=#incremental-board-updates-benchmarked-not-yet-main-path>#</a></h3><p>I benchmarked the <code>board.piece_map()</code> rebuild path and explored incremental <code>bytearray(64)</code> updates.</p><p>In theory, a chess move touches at most 4 squares (castling: king + rook):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-text data-lang=text><span style=display:flex><span>Normal move:    2 squares
</span></span><span style=display:flex><span>Capture:        2 squares
</span></span><span style=display:flex><span>En passant:     3 squares
</span></span><span style=display:flex><span>Castling:       4 squares
</span></span><span style=display:flex><span>Promotion:      2 squares</span></span></code></pre></div><p>The estimated improvement in notes was ~14–16 µs/ply down to ~2–5 µs/ply.<br>But to be clear: the <strong>current main encoder still rebuilds from <code>board.piece_map()</code></strong>. Incremental updates are planned, not merged.</p><h3 id=jagged-batching>Jagged Batching<a class=anchor href=#jagged-batching>#</a></h3><p>Multiple games are packed into a single flat token buffer, with <code>seq_offsets</code> marking boundaries. This is the same trick from <a href=./hstu-for-yambda.md>FlexAttention HSTU at 500M Events</a>: no cross-game attention, no padding waste.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-text data-lang=text><span style=display:flex><span>[BOS | ply1 ply2 ... plyN | BOS | ply1 ... plyM | ...]
</span></span><span style=display:flex><span>       game 1                    game 2</span></span></code></pre></div><p>Batch shape: <code>[total_tokens]</code> for most fields, <code>[total_tokens, 64]</code> for <code>piece_ids</code>.<br>No per-token padding mask is materialized; a block mask is derived from <code>seq_offsets</code> at runtime.</p><hr><h2 id=model-architecture>Model Architecture<a class=anchor href=#model-architecture>#</a></h2><h3 id=overview>Overview<a class=anchor href=#overview>#</a></h3><pre class=mermaid>
flowchart LR
    A[piece_ids 64 tokens] --&gt; B[E_piece + E_square]
    B --&gt; C[Mean pool → board_emb]
    D[prev_move_id] --&gt; E[E_move]
    F[turn/castle/ep/clk] --&gt; G[E_meta]
    C --&gt; H[Additive event composition]
    E --&gt; H
    G --&gt; H
    H --&gt; I[HSTU Backbone causal layers]
    I --&gt; J[Policy Head → move logits]
    I --&gt; K[Value Head → loss/draw/win]
</pre><script src=/mermaid.min.js onload='mermaid.initialize({flowchart:{useMaxWidth:!0},theme:"default"})'></script><h3 id=embedding-layers>Embedding Layers<a class=anchor href=#embedding-layers>#</a></h3><p>Each ply is converted to a single event vector by embedding structured fields and <strong>summing</strong> them (current implementation):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-text data-lang=text><span style=display:flex><span>event_t = board_emb
</span></span><span style=display:flex><span>        + seq_token_emb
</span></span><span style=display:flex><span>        + turn_emb
</span></span><span style=display:flex><span>        + castle_emb
</span></span><span style=display:flex><span>        + ep_emb
</span></span><span style=display:flex><span>        + halfmove_emb
</span></span><span style=display:flex><span>        + fullmove_emb
</span></span><span style=display:flex><span>        + prev_move_emb</span></span></code></pre></div><p>Where:</p><ul><li><code>board_emb</code> = mean over (E_piece(piece_ids) + E_square(index)) for all 64 squares</li><li><code>move_emb_{t-1}</code> = embedding of the previous move (or START token at ply 1)</li><li><code>meta_emb</code> = embeddings of turn, castling rights, en-passant file, clock buckets</li></ul><h3 id=hstu-backbone>HSTU Backbone<a class=anchor href=#hstu-backbone>#</a></h3><p>The backbone is causal with relative position bias over plies.
Current footprint is ~12.6M params (with value head), and it trains fine on a single RTX 4090.</p><hr><h2 id=training-phase-1-supervised-policy-pretraining>Training Phase 1: Supervised Policy Pretraining<a class=anchor href=#training-phase-1-supervised-policy-pretraining>#</a></h2><h3 id=objective>Objective<a class=anchor href=#objective>#</a></h3><p>Cross-entropy on next UCI move over the full static move vocabulary:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-text data-lang=text><span style=display:flex><span>loss = CE(logits, target_move_id)</span></span></code></pre></div><p>BOS positions are excluded via <code>ignore_index = -100</code>.</p><h3 id=elo-weighted-loss>Elo-Weighted Loss<a class=anchor href=#elo-weighted-loss>#</a></h3><p>Not all supervision is equally useful, so I weight by played-by Elo:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-text data-lang=text><span style=display:flex><span>norm_i = clamp((elo_i - min_elo) / (max_elo - min_elo), 0, 1)
</span></span><span style=display:flex><span>w_i    = 1 + strength × (norm_i ^ alpha)
</span></span><span style=display:flex><span>loss   = Σ(w_i × ce_i) / Σ(w_i)</span></span></code></pre></div><p>Weight normalization keeps gradients stable.<br>I also use label smoothing because strong positions often have multiple reasonable moves.</p><h3 id=training-infrastructure>Training Infrastructure<a class=anchor href=#training-infrastructure>#</a></h3><ul><li>Optimizer: StableAdamW with OneCycleLR scheduler</li><li>Precision: bfloat16 mixed precision</li><li>Checkpointing: best by <code>hr@10</code> on full val, plus periodic last checkpoints</li><li>Logging: TensorBoard + periodic fast val/test checks</li></ul><hr><h2 id=evaluation-metrics>Evaluation Metrics<a class=anchor href=#evaluation-metrics>#</a></h2><h3 id=offline-metrics-phase-1>Offline Metrics (Phase 1)<a class=anchor href=#offline-metrics-phase-1>#</a></h3><p>Evaluated on held-out val/test splits every N steps:</p><table><thead><tr><th>Metric</th><th>What it measures</th></tr></thead><tbody><tr><td><code>loss_ce</code></td><td>Cross-entropy on target move</td></tr><tr><td><code>ppl</code></td><td>Perplexity (exp of loss_ce)</td></tr><tr><td><code>top1_acc</code></td><td>Argmax move matches human move</td></tr><tr><td><code>top3_acc</code> / <code>top5_acc</code></td><td>Move in top-3/5</td></tr><tr><td><code>hr@10</code></td><td>Hit rate at 10 (top-10 accuracy)</td></tr><tr><td><code>mrr</code></td><td>Mean reciprocal rank of ground-truth move</td></tr></tbody></table><p>Model selection uses <code>hr@10</code> from full val as the primary signal.</p><h3 id=slice-metrics-planned-expansion>Slice Metrics (Planned Expansion)<a class=anchor href=#slice-metrics-planned-expansion>#</a></h3><p>Global averages hide regressions.<br>Phase/Elo slice reporting is in the eval spec, but current evaluator outputs are mostly global (<code>loss_ce</code>, <code>ppl</code>, <code>top-k</code>, <code>mrr</code>).</p><h3 id=engine-evaluation-phase-2>Engine Evaluation (Phase 2)<a class=anchor href=#engine-evaluation-phase-2>#</a></h3><p>After offline metrics stabilize, the model plays against Stockfish:</p><ul><li>Alternating colors, fixed time controls</li><li>Current default ladder config: <code>1320, 1600, 1800, 2000</code> (plus optional full-strength segment)</li><li>Reports currently include wins/draws/losses, score rate, color split, legal-move coverage, and run config</li><li>Elo estimate + confidence intervals are part of the evaluation plan, but not yet emitted by the script</li></ul><h3 id=current-snapshot-from-stored-artifacts>Current Snapshot from Stored Artifacts<a class=anchor href=#current-snapshot-from-stored-artifacts>#</a></h3><p>From current <code>artifacts/checkpoints/tb</code> and <code>artifacts/eval</code>:</p><table><thead><tr><th>Area</th><th>Result</th></tr></thead><tbody><tr><td>Offline full-val <code>hr@10</code></td><td>improved to <strong>0.9208</strong></td></tr><tr><td>Offline full-val <code>top1_acc</code></td><td><strong>0.4341</strong></td></tr><tr><td>Offline full-val <code>mrr</code></td><td><strong>0.6029</strong></td></tr><tr><td>Stockfish ladder @1320 (sample policy)</td><td><code>2/22/76</code>, score <code>0.13</code></td></tr><tr><td>Stockfish ladder @1600 (sample policy)</td><td><code>0/5/95</code>, score <code>0.025</code></td></tr><tr><td>Stockfish ladder @1800 (sample policy)</td><td><code>0/10/90</code>, score <code>0.05</code></td></tr><tr><td>Stockfish ladder @2000 (sample policy)</td><td><code>1/7/92</code>, score <code>0.045</code></td></tr><tr><td>Stockfish full strength (sample policy)</td><td><code>0/0/100</code>, score <code>0.00</code></td></tr></tbody></table><h3 id=early-milestone-notes-before-full-valuesearch-integration>Early Milestone Notes (Before Full Value/Search Integration)<a class=anchor href=#early-milestone-notes-before-full-valuesearch-integration>#</a></h3><p>From an earlier checkpointing phase:</p><ul><li><code>hr@10 = 0.830764</code> was reached while training was still early.</li><li>Later policy-only runs crossed <code>hr@10 > 0.9</code> and <code>top1 ~ 0.45</code>.</li><li>Despite that, initial engine strength was poor: policy-only behavior could still underperform badly versus low-Elo Stockfish settings.</li></ul><p>This was the key project inflection point for me: good imitation metrics were necessary, but not enough for winning play.</p><hr><h2 id=phase-2-adding-a-value-head>Phase 2: Adding a Value Head<a class=anchor href=#phase-2-adding-a-value-head>#</a></h2><h3 id=why-a-value-head>Why a Value Head<a class=anchor href=#why-a-value-head>#</a></h3><p>A policy head mainly learns &ldquo;what move humans pick&rdquo;.<br>It does not directly optimize for outcome quality.<br>The value head adds explicit WDL outcome prediction from the position.</p><p>Without value at inference time, the model cannot cleanly separate:</p><ul><li>&ldquo;This move is popular in human games&rdquo; (policy says yes)</li><li>&ldquo;This move leads to a winning position&rdquo; (requires value)</li></ul><h3 id=wdl-classification>WDL Classification<a class=anchor href=#wdl-classification>#</a></h3><p>The value head is a 3-class classifier from the side-to-move perspective:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-text data-lang=text><span style=display:flex><span>value_logits = Linear(d, 3)  # [loss, draw, win]</span></span></code></pre></div><p>Labels are derived from the per-game result (<code>game_result_white ∈ {+1, 0, -1}</code>) and per-token <code>turn_id</code> (to flip perspective for black).</p><p>A scalar value is extracted as:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-text data-lang=text><span style=display:flex><span>V(s) = p(win) - p(loss) ∈ [-1, 1]</span></span></code></pre></div><h3 id=progress-weighting>Progress Weighting<a class=anchor href=#progress-weighting>#</a></h3><p>Value labels derived from final game results are noisy — early positions have a weak causal link to who ultimately wins. We downweight early plies and emphasize later ones:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-text data-lang=text><span style=display:flex><span>progress_weight = (ply_index / total_plies) ^ alpha</span></span></code></pre></div><p>Current config uses <code>value_weight_alpha = 0.9</code> (mild late-game emphasis). I still treat this as a tuning knob.</p><h3 id=combined-loss>Combined Loss<a class=anchor href=#combined-loss>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-text data-lang=text><span style=display:flex><span>total_loss = policy_loss + λ × value_loss</span></span></code></pre></div><p>Early notes suggested starting near <code>λ = 0.15</code> for safety.<br>Current checked-in config uses <code>value_loss_weight = 0.5</code>, and I am still tuning this.</p><pre class=mermaid>
flowchart TD
    A[HSTU hidden state] --&gt; B[Policy Head]
    A --&gt; C[Value Head]
    B --&gt; D[CE loss on next move]
    C --&gt; E[CE loss on WDL outcome]
    D --&gt; F[total_loss = policy_loss + λ × value_loss]
    E --&gt; F
</pre><h3 id=training-schedule>Training Schedule<a class=anchor href=#training-schedule>#</a></h3><ol><li><strong>Warm start</strong> (optional): freeze backbone for 1k–3k steps, train only heads.</li><li><strong>Joint training</strong>: unfreeze all, keep <code>value_loss_weight</code> low initially.</li><li><strong>Monitor</strong>: if policy metrics drop, reduce value weight.</li></ol><hr><h2 id=phase-3-using-value-at-inference>Phase 3: Using Value at Inference<a class=anchor href=#phase-3-using-value-at-inference>#</a></h2><p>Adding a value head to training only modestly improves playing strength. The real gain comes from using the value during <strong>move selection</strong>.</p><h3 id=mode-1-greedy-baseline>Mode 1: Greedy (Baseline)<a class=anchor href=#mode-1-greedy-baseline>#</a></h3><p>Pick the highest-logit legal move. Fast, deterministic, no value used.</p><h3 id=mode-2-sampled-decoding>Mode 2: Sampled Decoding<a class=anchor href=#mode-2-sampled-decoding>#</a></h3><p>Sample from top-k / top-p legal moves with temperature. Adds variety, occasionally finds surprising moves, but can also pick blunders.</p><h3 id=mode-3-value-rerank-1-ply-lookahead>Mode 3: Value Rerank (1-Ply Lookahead)<a class=anchor href=#mode-3-value-rerank-1-ply-lookahead>#</a></h3><p>Take top-K policy candidates, evaluate each resulting position with the value head, pick the best:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-text data-lang=text><span style=display:flex><span>score(move) = log π(move | s) - λ × V(next_state)</span></span></code></pre></div><p>The minus sign: after we move, it is the opponent&rsquo;s turn at <code>next_state</code>, so high opponent value is bad for us.</p><pre class=mermaid>
flowchart LR
    A[Current state s] --&gt; B[Policy: top-K legal moves]
    B --&gt; C[Apply each move → s&#39;]
    C --&gt; D[Value head on each s&#39;]
    D --&gt; E[Score = log π - λ V_opp]
    E --&gt; F[Pick best scoring move]
</pre><p>Default settings: <code>K = 8</code>, <code>λ = 0.35</code>.</p><h3 id=mode-4-depth-2-policy-pruned-minimax>Mode 4: Depth-2 Policy-Pruned Minimax<a class=anchor href=#mode-4-depth-2-policy-pruned-minimax>#</a></h3><p>One step deeper: after my move, simulate opponent reply, then choose move with best worst-case reply value.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-text data-lang=text><span style=display:flex><span>Q(a) = min_{b ∈ top-K} V(apply(apply(s, a), b))
</span></span><span style=display:flex><span>a*   = argmax_a Q(a)</span></span></code></pre></div><p>The branching factor is controlled by keeping only top-K policy candidates at each ply: K1 candidate moves for us, K2 opponent responses each.</p><pre class=mermaid>
flowchart TD
    A[Root state s] --&gt; B[Our top-K1 moves]
    B --&gt; C[For each candidate a → s&#39;]
    C --&gt; D[Opponent top-K2 moves]
    D --&gt; E[For each response b → s&#39;&#39;]
    E --&gt; F[Value V at s&#39;&#39;]
    F --&gt; G[Opponent picks b that minimizes V for us]
    G --&gt; H[We pick a with best worst-case V]
</pre><p>Why this helped me early: many losses were immediate tactical misses.<br>Depth-2 explicitly checks &ldquo;what is their best next reply?&rdquo;</p><p><strong>Batch optimization</strong>: instead of calling the transformer node-by-node, batch all K1 × K2 grandchild states into a single forward pass. This can be 10–100× faster on GPU.</p><hr><h2 id=what-went-wrong-and-why>What Went Wrong (And Why)<a class=anchor href=#what-went-wrong-and-why>#</a></h2><p>This was the most useful learning phase so far.</p><p>A policy model can look strong offline (<code>hr@10</code>, <code>top1</code>) and still collapse in actual play:</p><ul><li>imitation objective learns &ldquo;what humans played&rdquo;, not &ldquo;what maximizes winning chances&rdquo;</li><li>greedy decoding can overcommit to narrow policy modes</li><li>sampling can recover occasional wins, but not stable strength</li><li>win/loss signal is weakly coupled to policy CE unless value/search is explicitly used</li></ul><p>In short: pretraining gave me a good prior, but not reliable tactical behavior by itself.</p><hr><h2 id=ablation-matrix>Ablation Matrix<a class=anchor href=#ablation-matrix>#</a></h2><p>To measure what actually moves the needle on Stockfish win rate, this is the comparison matrix to run under identical settings:</p><table><thead><tr><th>Configuration</th><th>Description</th></tr></thead><tbody><tr><td>Policy-only + greedy</td><td>Baseline</td></tr><tr><td>Policy+value training, greedy decode</td><td>Does value training help representations?</td></tr><tr><td>Policy+value training, value-rerank</td><td>Does 1-ply value improve play?</td></tr><tr><td>Policy+value training, depth-2 search</td><td>Does minimax help further?</td></tr></tbody></table><p>All comparisons use the same Stockfish time controls and opening protocols.</p><hr><h2 id=current-limitations-and-known-issues>Current Limitations and Known Issues<a class=anchor href=#current-limitations-and-known-issues>#</a></h2><ul><li>No legal-move masking in the training loss yet. Policy is trained as full-vocab classification, then projected to legal moves during play/eval.</li><li>Training is single-process (no DDP launcher yet).</li><li><code>value_rerank</code> is one-ply only; <code>value_search_d2</code> is depth-2 and substantially slower than greedy.</li><li>Value labels are noisy for early plies — progress weighting helps but does not fully solve this.</li><li>Value head may learn player-strength bias (higher Elo games have more draws): we still need stronger value-slice/calibration diagnostics in the evaluator.</li></ul><hr><h2 id=planned-next-steps>Planned Next Steps<a class=anchor href=#planned-next-steps>#</a></h2><p><strong>Self-play RL (Phase 4)</strong></p><p>After pretraining and value metrics stabilize, next step is RL fine-tuning via self-play:</p><ul><li>Environment: <code>gym-chess</code> with parallel rollouts (~1000 workers via pufferlib)</li><li>Algorithm: PPO or KL-regularized PPO (GRPO-style)</li><li>Reward: +1 win, 0 draw, −1 loss; optional shaping from engine eval delta</li><li>League: self-play against current + past checkpoints; optional Stockfish/Leela matches</li></ul><p><strong>Beam Search</strong></p><p>Another path is beam search over likely continuations, with policy priors and value-scored leaves.</p><p><strong>Scaling</strong></p><p>Current production-ish config is ~12.6M parameters on a single RTX 4090. Interesting questions:</p><ul><li>How far can this config go before scaling width/depth?</li><li>If we scale toward the original 20–30M target, does Elo improve smoothly?</li><li>Does Elo scale smoothly with model size?</li><li>Does value search help more at smaller model sizes (where policy alone is weaker)?</li></ul><hr><h2 id=references>References<a class=anchor href=#references>#</a></h2><ul><li><a href=https://database.lichess.org/ target=_blank rel=noopener>Lichess Open Database</a></li><li><a href=https://python-chess.readthedocs.io/ target=_blank rel=noopener>python-chess</a></li><li><a href=https://github.com/google-deepmind/searchless_chess target=_blank rel=noopener>Searchless Chess (DeepMind)</a></li><li><a href=https://github.com/noamdwc/grpo_chess target=_blank rel=noopener>grpo_chess</a></li><li><a href=https://pytorch.org/docs/stable/nn.attention.flex_attention.html target=_blank rel=noopener>PyTorch FlexAttention</a></li><li><a href=https://mermaid.js.org/ target=_blank rel=noopener>Mermaid</a></li><li>Transcendence: Generative Models Can Outperform The Experts That Train Them</li><li>Amortized Planning with Large-Scale Transformers: A Case Study on Chess</li></ul><h2 id=related-posts>Related Posts<a class=anchor href=#related-posts>#</a></h2><ul><li><a href=./hstu-for-yambda.md>FlexAttention HSTU at 500M Events</a></li><li><a href=./ml_engineer_guidelines.md>Machine Learning Engineer Roadmap</a></li></ul><h2 id=closing>Closing<a class=anchor href=#closing>#</a></h2><p>My working bet is still the same: structured event modeling from recsys transfers to chess.</p><p>Value + search is the bridge from imitation to actual board strength.</p><p>Whether that is enough without heavy RL is still open.</p><p>The current working hypothesis is:</p><ul><li>better pretraining gives a better prior,</li><li>value + shallow search converts more of that prior into practical strength,</li><li>and RL is likely needed to unlock the next jump in actual board-level reasoning.</li></ul></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=/docs/machine-learning/ class="flex align-center"><img src=/icons/backward.svg class=book-icon alt=Backward>
<span>Machine Learning Toolkit</span>
</a></span><span><a href=/docs/posts/hstu-for-yambda/ class="flex align-center"><span>FlexAttention HSTU at 500M Events: RQ Tokens, QR Embeddings, and 1D Biases</span>
<img src=/icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><ul><li><a href=#build-log-snapshot>Build Log Snapshot</a></li><li><a href=#what-triggered-this-project>What Triggered This Project</a></li><li><a href=#why-chess-and-why-hstu>Why Chess and Why HSTU</a></li><li><a href=#original-project-brief-early-notes>Original Project Brief (Early Notes)</a></li><li><a href=#data-pipeline>Data Pipeline</a><ul><li><a href=#source>Source</a></li><li><a href=#temporal-splits>Temporal Splits</a></li><li><a href=#historical-run-profile-from-earlier-training-notes>Historical Run Profile (From Earlier Training Notes)</a></li><li><a href=#pgn-parsing-and-board-state>PGN Parsing and Board State</a></li><li><a href=#incremental-board-updates-benchmarked-not-yet-main-path>Incremental Board Updates (Benchmarked, Not Yet Main Path)</a></li><li><a href=#jagged-batching>Jagged Batching</a></li></ul></li><li><a href=#model-architecture>Model Architecture</a><ul><li><a href=#overview>Overview</a></li><li><a href=#embedding-layers>Embedding Layers</a></li><li><a href=#hstu-backbone>HSTU Backbone</a></li></ul></li><li><a href=#training-phase-1-supervised-policy-pretraining>Training Phase 1: Supervised Policy Pretraining</a><ul><li><a href=#objective>Objective</a></li><li><a href=#elo-weighted-loss>Elo-Weighted Loss</a></li><li><a href=#training-infrastructure>Training Infrastructure</a></li></ul></li><li><a href=#evaluation-metrics>Evaluation Metrics</a><ul><li><a href=#offline-metrics-phase-1>Offline Metrics (Phase 1)</a></li><li><a href=#slice-metrics-planned-expansion>Slice Metrics (Planned Expansion)</a></li><li><a href=#engine-evaluation-phase-2>Engine Evaluation (Phase 2)</a></li><li><a href=#current-snapshot-from-stored-artifacts>Current Snapshot from Stored Artifacts</a></li><li><a href=#early-milestone-notes-before-full-valuesearch-integration>Early Milestone Notes (Before Full Value/Search Integration)</a></li></ul></li><li><a href=#phase-2-adding-a-value-head>Phase 2: Adding a Value Head</a><ul><li><a href=#why-a-value-head>Why a Value Head</a></li><li><a href=#wdl-classification>WDL Classification</a></li><li><a href=#progress-weighting>Progress Weighting</a></li><li><a href=#combined-loss>Combined Loss</a></li><li><a href=#training-schedule>Training Schedule</a></li></ul></li><li><a href=#phase-3-using-value-at-inference>Phase 3: Using Value at Inference</a><ul><li><a href=#mode-1-greedy-baseline>Mode 1: Greedy (Baseline)</a></li><li><a href=#mode-2-sampled-decoding>Mode 2: Sampled Decoding</a></li><li><a href=#mode-3-value-rerank-1-ply-lookahead>Mode 3: Value Rerank (1-Ply Lookahead)</a></li><li><a href=#mode-4-depth-2-policy-pruned-minimax>Mode 4: Depth-2 Policy-Pruned Minimax</a></li></ul></li><li><a href=#what-went-wrong-and-why>What Went Wrong (And Why)</a></li><li><a href=#ablation-matrix>Ablation Matrix</a></li><li><a href=#current-limitations-and-known-issues>Current Limitations and Known Issues</a></li><li><a href=#planned-next-steps>Planned Next Steps</a></li><li><a href=#references>References</a></li><li><a href=#related-posts>Related Posts</a></li><li><a href=#closing>Closing</a></li></ul></li></ul></nav></div></aside></main></body></html>